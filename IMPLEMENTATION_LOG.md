# IMPLEMENTATION LOG: PopForecast

This document records the **exact steps that worked** (and the rationale behind key choices) so the project can be reproduced quickly on a new machine and resumed safely in a new chat.

---

## üìÖ CYCLE 01 ‚Äî INFRASTRUCTURE & MVP (EDA + evaluation protocol)

### 1. Strategic definition (artifacts created)

- **`WORKFLOW_SAPE.md`** ‚Äî workflow defined using SAPE (Output ‚Üí Process ‚Üí Input) and Fact‚ÄìDimension.
- **`ARCHITECTURE.md`** ‚Äî hexagonal architecture + tech stack (Python 3.10) + folder layout.

---

### 2. Environment setup (WSL / Ubuntu 22.04)

- **Python version:** Python **3.10.12** (managed by pyenv)
- **Environment isolation:** Poetry manages the virtual environment; pyenv provides only the Python interpreter.

```bash
pyenv install 3.10.12
pyenv local 3.10.12
```

- **Poetry installation (global):**

```bash
curl -sSL https://install.python-poetry.org | python3 -
echo 'export PATH="$HOME/.local/bin:$PATH"' >> ~/.bashrc
source ~/.bashrc
```

---

### 3. Dependency management (Poetry)

Poetry is used exclusively for dependency management and virtual‚Äëenvironment creation.  
Python itself is provided by pyenv, but **no pyenv virtualenvs are used** for this project.

#### 3.1 Poetry installation (global)

Poetry is installed globally (outside any virtual environment) using the official installer (see Section 2).

This ensures consistent behavior across machines and avoids conflicts with pyenv‚Äëmanaged environments.

#### 3.2 Project initialization

The project was initialized with Poetry:

```bash
poetry init --no-interaction
```

#### 3.3 Dependency installation

Cycle 1 requires versions compatible with Python 3.10.  
Pinned constraints reflect compatibility considerations (e.g., `pandas<3`, `scikit-learn<1.8`, `kaggle<1.8`).

```bash
poetry add "pandas<3.0.0" "scikit-learn<1.8" xgboost streamlit fastapi uvicorn pyarrow "kaggle<1.8"
```

Development dependencies:

```bash
poetry add --group dev jupyterlab jupyterlab-execute-time matplotlib ipykernel
```

#### 3.4 Package mode disabled

The project is notebook‚Äëfirst and not intended to be installed as a Python package in Cycle 1.  
To avoid Poetry attempting to install the project itself, `package-mode = false` is set in `pyproject.toml`:

```toml
[tool.poetry]
package-mode = false
```

#### 3.5 Environment creation

Poetry creates and manages its own virtual environment automatically.  
To ensure it uses the correct Python version from pyenv:

```bash
pyenv local 3.10.12
poetry env use $(pyenv which python)
poetry install
```

---

### 4. Version control (Git + GitHub)

* **Goal:** reproducible and shareable repository; datasets remain local.

* **Repository initialization:**

  ```bash
  git init
  git status
  ```

* **Ignore rules:**

  * `.gitignore` prevents committing datasets and machine-local artifacts.
  * Keep `data/raw/` and `data/processed/` via `.gitkeep`.

* **Remote setup (GitHub):**

  ```bash
  git remote add origin <REPO_URL>
  git branch -M main
  git push -u origin main
  ```

* **Notes:**

  * Do not commit secrets (Kaggle credentials, tokens, `.env`).

---

### 5. Data ingestion (Kaggle)

* **Goal:** download raw dataset locally (not committed).
* **Credentials:** Kaggle API token at `~/.kaggle/kaggle.json`.
* **Command (works):**

  ```bash
  poetry run python src/scripts/download_data.py --unzip
  ```
* **Output:**

  * Target directory: `data/raw/`
  * Extracted file: `spotify_tracks_metadata.csv`

---

### 6. EDA notebook (Cycle 1)

**Artifact:** `notebooks/EDA.ipynb`

Cycle 1 EDA is intentionally scoped to:

1. validate dataset structure,
2. surface lightweight data-quality issues that affect modeling, and
3. lock MVP decisions (task framing, split strategy, leakage rules).

The goal is **reproducibility and clarity**, not exhaustive auditing.

#### 6.1 Notebook structure adopted (storytelling layout)

0. Title & Purpose (Cycle 1 objective, risks, decisions)
1. Setup (imports, settings, paths)
2. Data Loading (raw data read + sanity checks)
3. Schema Overview (columns, dtypes, memory, cardinality)
4. Data Quality Checks (missingness, duplicates, release-year consistency)
5. Target Understanding (`song_popularity`: distribution, concentration, zero mass)
6. Feature Candidates ‚Äî quick scan (ranges, anomalies, weak bivariate signals)
7. Leakage & Non-usable Columns (MVP rules)
8. Baseline Evaluation Plan (task, splits, metrics; baseline protocol validation)
9. Decisions & Next Steps (compact list of closed decisions + action items)

#### 6.2 Cleaning workflow (EDA-first, production-later)

* Keep `data_raw` immutable (source of truth).
* Apply cleaning incrementally by overwriting `data_clean` inside the notebook.
* After decisions stabilize, consolidate into a **single production cleaning function/script**.

This preserves EDA speed while still producing a clean, reproducible production implementation later.

---

### 7. Key cleaning decisions implemented (Cycle 1)

#### 7.1 Duplicates by `spotify_id`

* `data_raw.duplicated().sum() == 0` ‚Üí no fully identical rows.
* `data_raw.duplicated(subset=["spotify_id"]).sum() > 0` ‚Üí collisions on Spotify IDs with divergent metadata.

**Decision (MVP):** for duplicated `spotify_id`, keep the record with the **maximum `song_popularity`**.
Rationale: the ID should represent the same track entity; popularity varies across updates, and max is a pragmatic, stable tie-breaker for Cycle 1.

#### 7.2 Release year consistency + diagnostic flag

Release-year metadata is imperfect (original releases vs remasters vs re-issues), and rare implausible years exist.

**Decision (MVP):**

* Keep `album_release_year` as the temporal signal for diagnostics.
* Introduce a boolean **diagnostic flag**:

  * `release_year_missing_or_suspect = True` for missing or suspicious years.
* Do not overfit cleaning rules to a few anomalies in Cycle 1; preserve traceability.

#### 7.3 Range diagnostics and anomaly counts (kept, flagged)

Cycle 1 does **not** block modeling on these edge cases, but they are recorded for later cycles:

* `tempo == 0` ‚Üí likely malformed/missing tempo encoded as zero.
* `time_signature == 0` ‚Üí likely invalid placeholder.
* very long tracks (`duration_ms > 20 min`) ‚Üí legitimate long-tail content.
* `loudness > 0` ‚Üí rare but plausible for some masters.

**Decision (MVP):** keep them for now; revisit only if they harm baseline stability.

#### 7.4 Type tightening (memory + clarity)

**Decision (MVP):** downcast to smaller dtypes where safe to reduce footprint and make intent explicit:

* `song_popularity`, `total_available_markets` ‚Üí small integers (e.g., `int16`)
* `key`, `mode`, `time_signature` ‚Üí small integers (e.g., `int8`)
* audio features ‚Üí `float32`
* `album_release_year` ‚Üí nullable integer (e.g., `Int16`)
* flags ‚Üí `bool`

---

### 8. Baseline evaluation plan (no heavy modeling)

This section formalizes how Cycle 1 baseline results are reported. The baseline is intentionally lightweight; priority is an evaluation protocol that is **reproducible** and **interpretable**.

#### 8.1 Task type

`song_popularity` is a bounded discrete score in **[0, 100]**.

**Decision (MVP):** treat as a **regression task**, predicting a numeric score directly (no arbitrary binning).

#### 8.2 Split strategy

Cycle 1 uses a **dual split strategy**, with different roles:

**A) Primary benchmark: random holdout (i.i.d. reference)**

* Purpose: stable baseline under i.i.d. assumptions.
* Implementation: shuffled holdout (train/test).
  *(A train/validation split is optional later for model selection, but not required to validate the baseline protocol.)*

**B) Secondary diagnostic: temporal best-effort**

* Purpose: stress-test generalization to ‚Äúnominally newer‚Äù releases.
* Temporal boundaries used:

  * Train: `album_release_year <= 2019`
  * Validation: `album_release_year == 2020`
  * Test: `album_release_year == 2021`

**Temporal hygiene rule (MVP):**

* Use integer years (no float quantiles).
* Exclude `release_year_missing_or_suspect == True` from the entire temporal diagnostic to avoid timeline noise.

#### 8.3 Metrics

All metrics are computed in ‚Äúpopularity points‚Äù.

* **MAE (primary):** robust, easy to interpret.
* **RMSE (secondary):** highlights large misses.
* **R¬≤ (context only):** reported for reference.

**Segment-aware diagnostics (because of zero mass):**

* MAE on `y == 0`
* MAE on `y > 0`

This prevents a baseline from looking strong merely by predicting near-zero for most tracks.

#### 8.4 Leakage prevention rule (MVP)

All preprocessing must be split-safe:

* Fit preprocessing **only on the training split**.
* Apply to validation/test via the same fitted pipeline.
* No statistics from validation/test may influence training.

#### 8.5 Baseline protocol validation inside the notebook

The notebook validates the evaluation protocol using a **constant median predictor** per split, producing:

* a consolidated metrics table (random vs temporal),
* a short interpretation of differences (notably the temporal shift in zero mass).

---

### 9. Environment portability and reproducibility notes

#### 9.1 Poetry / pyenv interaction (new machine)

On a clean machine, `poetry install` can fail when `.python-version` points to a Python version (or pyenv env name) not installed locally. Typical symptom: Poetry probes `python` via pyenv shims and exits non-zero.

**Resolution pattern (Cycle 1):**

* Install the required Python version under pyenv locally.
* Point Poetry to the correct interpreter:

  ```bash
  pyenv install 3.10.12          # if not already installed
  pyenv local 3.10.12
  poetry env use $(pyenv which python)
  poetry install
  ```

#### 9.2 Poetry ‚Äúproject could not be installed‚Äù error

If dependencies install successfully but Poetry fails installing the root project with:

> ‚ÄúNo file/folder found for package popforecast‚Äù

**Pragmatic Cycle 1 choice:** use Poetry for dependency management only.
Two valid options:

* `poetry install --no-root`, or
* set `package-mode = false` in `pyproject.toml` under `[tool.poetry]`

(We only need imports within notebooks/scripts in Cycle 1, not a distributable package.)

#### 9.3 Kaggle credentials portability

Kaggle downloads fail on a new machine if credentials exist only on the previous setup.

**Cycle 1 rule:** treat external credentials as machine-local configuration.
Ensure `~/.kaggle/kaggle.json` exists before re-running ingestion steps.

#### 9.4 Poetry 2.x activation model

Poetry 2.x no longer installs the `poetry shell` command by default.  
Cycle 1 standard:

```bash
poetry env activate
```

(Optionally, users may install `poetry-plugin-shell` if they prefer the legacy `poetry shell` workflow.)

#### 9.5 pyenv vs Poetry ‚Äî separation of concerns

Cycle 1 rule:

- **pyenv provides the correct Python version**  
- **Poetry creates and manages the virtual environment**  
- **Do not use `pyenv virtualenv` for this project**

This ensures consistent behavior across machines and avoids environment conflicts.

---

### 10. Git hygiene (Cycle 1)

* Track `notebooks/EDA.ipynb` as a first-class artifact (Cycle 1 audit trail).
* Do not commit `notebooks/.ipynb_checkpoints/` (keep it ignored).
* Keep `poetry.lock` in sync with `pyproject.toml`.

Recommended commit structure:

* one commit for environment/dependency changes (pyproject + lock),
* one commit for notebook milestones (EDA structure + major section completions).

---

### 11. Pipeline implementation (`src/core/preprocessing.py`)

* **Logic:** Implemented a `sklearn.Pipeline` wrapping custom functions.
* **Key Decisions:**
    * **Deduplication:** Strategy `keep="first"` after sorting by `popularity` (descending). Rationale: In case of ID conflict, the entry with higher signal (popularity) is the "true" one.
    * **Year Handling:** `1900` treated as NaN. Flag `release_year_missing_or_suspect` created.
    * **Type Safety:** Forced **Downcasting** (e.g., `Int16` for years to handle NaNs natively) and saving to Parquet to preserve schema.

### 12. Baseline & Protocol (`notebooks/02_baseline_train.ipynb`)

* **Models Tested:** Linear Regression (Naive) vs. Random Forest (Non-linear).
* **Protocol:** Validated on both **Random Split** (IID) and **Temporal Split** (Train < 2020, Val=2020, Test=2021).
* **Findings:**
    * **Non-linearity confirmed:** Random Forest significantly outperformed Linear Regression.
    * **Zero-Inflation:** Error analysis showed distinct behaviors for `popularity=0` vs `popularity>0`.
    * **Temporal Drift:** Confirmed that temporal split is harder (higher error), validating the need for robust validation.

### 13. Cycle 1 Closure & Decision

* **Status:** ‚úÖ COMPLETE.
* **Verdict:** The infrastructure is solid, data is clean, and there is a learnable signal (RF > Dummy).
* **Next Step (Cycle 2):** Focus on **Feature Engineering** to capture temporal context (Age, Seasonality) and Audio Interactions, as the baseline suggests simple features are not enough.




## üìÖ CYCLE 02 ‚Äî ADVANCED FEATURE ENGINEERING & ROBUST BASELINE

### 1. Feature Engineering Pipeline (Finalized)
* **New Blocks Implemented:** `audio_interactions`, `non_linear`, `market`, and `temporal`.
* **Leakage Prevention:** `year_meta` remains disabled under temporal evaluation due to target‚Äëproxy risk.
* **Recency Weighting:** `lambda = 0.05` selected after a sweep ‚Äî higher values improved 2020 but degraded 2021.
* **Tracks Explored:**
  - **numeric_only_15** (15 raw numeric columns + median fill + recency weights)
  - **engineered_38** (expanded feature space kept for exploration only)

---

### 2. Baseline Track Decision (Frozen for Cycle 3)
* **Official Track for Cycle 3:** `numeric_only_15`
* **Protocol Name:** `Baseline_Huber15_recency0p05_medfill`
* **Champion Reference (temporal split):**
  - **HuberRegressor:** MAE(2021) = **15.2127**
* **Engineered Track:** retained for research in Cycle 2 but **paused for Cycle 3** to avoid mixing input spaces.

---

### 3. The "Hurdle" Experiment (Architecture, Failure & Interpretation)
We tested the hypothesis that a **Two‚ÄëStage Hurdle Model (Classifier + Regressor)** would better handle zero‚Äëinflation and the 2021 concept drift.

**What actually happened:**

* **Collapse Issue:**  
  MAE‚Äëdriven threshold optimization pushed the classifier threshold extremely high (‚âà0.50‚Äì0.90), effectively **disabling zero prediction** (`pred_zero_pct ‚âà 0%`).  
  This nullified the classifier stage and amplified error propagation.

* **Benchmarking on Test Set 2021:**
  - ‚ùå **Hurdle Model:** MAE ~**16.14** (complex architecture, strong error propagation)
  - ‚úÖ **HuberRegressor:** MAE ~**15.21** (robust loss successfully ignored 2021 outliers)
  - ‚úÖ **RandomForest (small):** MAE ~**15.36** (outperformed Hurdle; regularization > complexity)

* **Status:**  
  The Hurdle model is kept **for reference only** and **does not continue into Cycle 3**.

---

### 4. Artifacts Frozen (Reproducibility & Auditability)

* **Deterministic Baseline Pack (Portable Anchor):**  
  `models/cycle_02/baseline_huber15_pack.npz`  
  ‚Üí single-source-of-truth for `Baseline_Huber15_recency0p05_medfill`  
  ‚Üí includes median‚Äëimputed X_*, split indices, y_*, and train weights  
  ‚Üí enables deterministic reproduction of MAE(2021)=15.2127 in Notebook 04, independent of upstream dataset regeneration.
 
* **Audit v3 (Derived from Pack):**  
  `models/cycle_02/baseline_huber15_audit_v3_from_pack.json`  
  ‚Üí hashes computed directly from the NPZ pack arrays using an explicit and uniform hash spec  
  ‚Üí authoritative forensic anchor for the baseline  
  ‚Üí replaces all prior audits for reproducibility purposes  
  ‚Üí ensures deterministic validation of the baseline by recomputing hashes from the pack in Notebook 04


* **Note:**  
  `baseline_huber15_audit_v2.json` remains in the repository as a historical dataset‚Äëlevel audit, but it is **not** used for baseline reproduction and is **not** part of the frozen artifacts for Cycle 3.

---

### 5. Next Steps (Strategic Pivot for Cycle 3)
* **Decision:**  
  The Hurdle architecture is **deprecated** ‚Äî no threshold will be frozen.

* **New Objective:**  
  Combine:
  - the **non‚Äëlinearity** of tree‚Äëbased models  
  - the **robustness** of Huber/MAE losses  

* **Direction for Cycle 3:**  
  Implement **Gradient Boosting (XGBoost)** with **robust objective functions** (`absoluteerror`, `pseudohubererror`) using the **same input space** as `Baseline_Huber15_recency0p05_medfill`.

* **Rule:**  
  Never report ‚Äú15.21‚Äù without the protocol qualifier.





## üìÖ CYCLE 03 ‚Äî ROBUST MODELING & CHAMPION SELECTION (XGBoost vs. Huber)

### 1. Goal & Strategy
* **Initial Objective:** Attempt to beat the frozen **Huber-15** baseline under the exact same protocol guardrails (Train ‚â§2019).
* **Diagnostic (Phase 1):** Early experiments proved that linear models (Huber) outperformed non-linear models (XGBoost) under the frozen split. This confirmed that the 2021 "Wall" (Concept Drift) was too severe; sticking to the frozen protocol would mean deploying an obsolete model.
* **Strategic Pivot (Phase 2):** We evolved to an **Operational Refit** strategy. We provided both the Huber baseline and the XGBoost challenger with the full 2020 signal to evaluate which architecture best captures new trends when given recent data.

```mermaid
%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#ffffff', 'primaryTextColor': '#000000', 'primaryBorderColor': '#333333', 'lineColor': '#333333', 'tertiaryColor': '#ffffff'}}}%%
flowchart LR
    A["Same protocol for everyone"] --> B["Phase 2 Comparison<br/>(Refit 2020 -> Test 2021)"]

    B --> HUB["Huber Fair Baseline<br/>(Linear Robustness)"]
    B --> XGB["High-Cap XGBoost<br/>(Depth 12 + Early Stopping)"]

    HUB --> F["Reason: Linear bounds<br/>‚Üí Fails to capture complex drift"]
    F --> HUBT["High Test MAE<br/>(15.63)"]

    XGB --> E["Reason: High capacity<br/>‚Üí Maps non-linear behavioral shifts"]
    E --> XGBT["Lowest Test MAE<br/>(14.39)"]

    HUBT --> G["Champion Rule:<br/>Generalization (Test 2021) is King"]
    XGBT --> G

    G --> WIN["Winner: High-Cap XGBoost<br/>(The Barrier is Broken)"]

    classDef win fill:#dff7df,stroke:#2e7d32,stroke-width:2px,color:#004d40;
    classDef lose fill:#ffebee,stroke:#c62828,stroke-width:1px,color:#b71c1c;
    classDef default fill:#ffffff,stroke:#333333,stroke-width:1px,color:#000000;
    class WIN,XGBT win;
    class HUBT lose;
```

### 2. Protocol Guardrails (Evolved)

The input space remained frozen to ensure fairness, but the temporal boundaries were updated for the operational scenario:

* **Input Space:** Frozen list of **15 numeric columns** (no new features).
* **Preprocessing:** Median imputation and Recency weighting (Œª=0.05).
* **Decision Split (Phase 2):** Train ‚â§2020 ‚Üí Test 2021.

### 3. Fair Baseline Establishment

To ensure an *apples-to-apples* comparison, we retrained the historical champion using the new 2020 data allowance.

* **Fair Baseline (Huber Refit):**
	* Test 2021 MAE (No-Clip): **15.6380**
	* Test 2021 MAE (Clipped 0-100): **15.6375**

### 4. Challenger Search (XGBoost Experiments)
We tested multiple configurations of XGBoost (`xgboost==3.1.3`), progressing from standard tuning under the frozen protocol to high-capacity architectures under the operational refit.

#### 4.1 Phase 1: Point Runs & Standard Tuning (Frozen Split)
* **Setup:** Tested multiple objectives (`reg:squarederror`, `reg:absoluteerror`, `reg:pseudohubererror`) using `RandomizedSearchCV` with `PredefinedSplit` (Val 2020).
* **Result:** The best tuned candidate (with regularization parameters `reg_alpha`, `reg_lambda`, `gamma`) achieved a Test 2021 MAE of **15.2541**.
* **Diagnostic:** Standard depth models failed to beat the Huber-15 baseline (15.2127). The non-linear patterns learned up to 2019 were insufficient to overcome the 2021 temporal drift.

#### 4.2 Phase 2: High-Capacity Architecture (The Breakthrough)
* **Setup:** Shifted to the Operational Refit strategy (Train ‚â§2020) and drastically increased structural complexity to bypass the need for a separate hurdle classifier.
* **Parameters:** Increased `max_depth=12` and lowered `learning_rate=0.01`.
* **Rationale:** The extreme depth forces the model to natively map complex audio interactions and isolate the massive zero-reach noise intrinsic to the popularity distribution.
* **Result:** Smashed the fair baseline (Huber Refit) with a Test 2021 MAE of **14.3968**.

#### 4.3 Calibration via Early Stopping
* **Setup:** To prevent the deep architecture from overfitting the new 2020 signal, dynamic calibration was required.
* **Execution:** Applied `early_stopping_rounds` monitoring the validation loss during the refit exploration.
* **Decision:** The process found the optimal capacity at `best_iteration = 1648`. The parameter `n_estimators` was statically locked at **1648** for the final champion model.

#### 4.4 Robustness Check (Occam's Razor)
* **Setup:** Evaluated whether the high complexity (`depth=12`) was strictly necessary or if a simpler refit model would suffice.
* **Execution:** Ran a simplified version of the Refit XGBoost using `max_depth=6`.
* **Result:** Performance heavily degraded, pushing the Test 2021 MAE up to **14.84** (+0.45 compared to the champion).
* **Decision:** Statistically confirms that Spotify's popularity landscape demands high structural depth; simpler trees fail to accurately distinguish true hits from noise.

### 5. Verdict & Outcome

To finalize the decision, we tracked the progression from the frozen protocol failures to the operational refit breakthrough.

#### 5.1 Leaderboard Progression
This summary highlights the best representative from each tested strategy:

| Phase / Strategy | Model Tag | Val 2020 (MAE) | Test 2021 (MAE) | Verdict |
| :--- | :--- | :--- | :--- | :--- |
| **Phase 1: Baseline** | `baseline_huber15` | 15.26 | 15.21 | **Stable, but stale** |
| **Phase 1: Tuned XGB** | `xgb_expanded_mae` | **14.12** | 15.25 | **Overfit** (Lost to 2021 drift) |
| **Phase 2: Fair Baseline** | `baseline_huber_refit` | *N/A (Train)* | 15.63 | **Rigid** (Failed to adapt to new data) |
| **Phase 2: High-Cap** | `xgb_high_cap_refit` | *N/A (Train)* | **14.39** | üëë **Barrier Broken** (New SOTA) |

#### 5.2 Final Verification (Symmetry Audit)
The champion was subjected to a final "trial by fire" to ensure its superiority wasn't merely an artifact of clipping the predictions to the [0-100] valid range.

| Evaluation Domain | Fair Baseline (Huber Refit) | Champion (High-Cap XGBoost) | Delta (Improvement) |
| :--- | :--- | :--- | :--- |
| **Clipped (0-100)** | 15.6375 | **14.3968** | üü¢ **-1.2407** |
| **No-Clip (Raw)** | 15.6380 | **14.3969** | üü¢ **-1.2411** |

#### 5.3 Key Observations & Final Decision
* **The Overfit Trap (Phase 1):** XGBoost variants initially learned the pre-drift 2020 patterns exceptionally well (Val MAE 14.12) but failed on the Test set, proving that standard tuning cannot overcome severe temporal drift.
* **The Structural Victory (Phase 2):** When both models were given the full 2020 signal, the linear architecture (Huber) degraded to 15.63, lacking the capacity to map new behavioral shifts. The High-Capacity XGBoost leveraged its depth to map these non-linearities natively.
* **Symmetry Integrity:** The Symmetry Audit proved the champion's dominance is structural. It isolated the "zero-reach noise" internally and demonstrated statistical prudence without relying on artificial bounds.
* **Decision:** **High-Capacity XGBoost** is the undisputed Cycle 3 Champion.

### 6. Artifacts Persisted
To maintain repository health, large binary models are ignored by Git. Reproducibility is guaranteed via the training notebook and metadata governance.

* **Champion Model:** `models/cycle_03/champion.json` (XGBoost Native Format - **Ignored via `.gitignore**` to prevent repo bloat).
* **Governance Metadata:** `models/cycle_03/run_metadata_cycle3.json`
* Upgraded to "Platinum Standard" including explicit `evaluation_context` to document the strategic pivot.
* Includes SHA256 hashes of the environment and historical baseline paths for full auditability.


## üìÖ CYCLE 04 ‚Äî ERROR ANALYSIS & INTERPRETABILITY (Data Enrichment Phase)

### 1. Goal & Strategic Pivot
* **Objective:** The high-capacity XGBoost (Cycle 3 Champion) hit a plateau (MAE ~14.39). Error analysis hypotheses suggest that acoustic features alone cannot explain cultural phenomena, artist authority, or "troll" metadata. To break the 14.0 MAE barrier, external context is required.
* **The Spotify API Attempt & Failure:** The initial strategy was to query the Spotify Web API for artist genres and track release dates. 
    * **Failure Point:** Spotify's API restricts bulk metadata extraction, imposes strict rate limits, and its "genres" are often too broad or sanitized. 
* **The Last.fm Adaptation:** Pivoted to the Last.fm API. 
    * **Rationale:** Last.fm provides crowd-sourced, highly granular cultural `tags` (capturing niches and trends) and a highly correlated `listeners` metric that serves as an excellent proxy for artist authority. It also does not require complex OAuth flows.

---

### 2. Infrastructure & Artifacts Created (Ingestion)

Cycle 04 required a stable ingestion layer to incorporate external cultural metadata (Last.fm) at scale. This section documents the infrastructure components created to support the enrichment process and ensure reproducibility across cycles.

#### 2.1 Last.fm Ingestion Pipeline

A dedicated ingestion workflow was implemented to retrieve artist-level metadata from the Last.fm API, focusing on two signals used later in modeling:

- `listeners` ‚Äî proxy for artist authority  
- `tags` ‚Äî crowd-sourced descriptors capturing genre niches and cultural context  

**Client module** `src/core/lastfm_client.py`  
- Provides `get_artist_info` (listeners + tags)  
- Provides `get_track_year` (album title lookup for suspect release years)  
- Includes retry logic, rate-limit handling, and defensive parsing

**Execution script** `src/scripts/enrich_with_lastfm.py`  
- **Input:** - `data/raw/spotify_tracks_metadata.csv` (names)  
  - `data/processed/spotify_tracks_modeling.parquet` (tracks with `release_year_missing_or_suspect == 1`)  
- **Process:** - Persistent caching every 50 records  
  - Rate-limited to ~5 requests/second  
  - Restart-safe execution  
- **Output:** - `data/interim/lastfm_enrichment_v1.json`

This pipeline ensures that large-scale metadata extraction can be performed without API bans or partial data loss.

---

#### 2.2 Technical Decision ‚Äî SHAP Versioning

> **Action:** Pinned `shap` to `>=0.42.0,<0.47.0` via Poetry.  
> **Rationale:** Maintain compatibility with Python 3.10 (Cycle 03 baseline) and avoid environment issues introduced in newer SHAP releases requiring Python ‚â• 3.11.  
> **Outcome:** Ensures full access to SHAP interpretability tools (summary, dependence, waterfall) without breaking legacy dependencies or destabilizing the environment.

This decision stabilizes the interpretability layer for Cycle 04 and preserves reproducibility across cycles.

### 3. Feature Engineering Integration (Retrocompatible)
The incoming JSON data contains extreme scales (listeners) and noise ("troll" tags). The feature pipeline was updated to parse this safely without breaking Cycle 02/03 reproducibility.

* **Core Logic Update:** `src/core/features.py`
  * Added `LastFMFeaturesTransformer` to the `src/core/features.py` module.
  * **Log-Normalization:** Applied `np.log1p` to the `listeners` metric to prevent scale dominance.
  * **Frequency Filtering:** Implemented logic to dynamically identify and One-Hot encode only the **Top 50** most frequent tags, effectively filtering out user-generated noise (e.g., "black metal" applied to pop artists).
  * **Configuration Gatekeeper:** Added a `lastfm: bool = False` flag to `FeatureEngineeringConfig`. This ensures all previous cycle pipelines run perfectly without requiring the Last.fm JSON.

* **Execution Script:** `src/scripts/build_lastfm_features.py`
  * Created to orchestrate the new pipeline. 
  * It toggles `lastfm=True`, runs `apply_feature_engineering(..., fit=True)`, and outputs the final master dataset to `data/processed/spotify_tracks_enriched.parquet`.

### 4. Interpretability-Driven Diagnosis (SHAP & The "Three Brains")

Instead of relying on global metrics, we used SHAP values to reverse-engineer how the XGBoost model processes the industry. We discovered that the model does not treat popularity as a continuous spectrum, but rigidly divides its logic into three cognitive regimes based on the `listeners_log` feature:

* **The Social Filter (Cold Start < 8.81):** In the underground, the model applies a heavily penalized rule-set driven by the sheer lack of audience.
* **The Acoustic Meritocracy (Tipping Point 8.81 to 13.09):** Here, audio features (loudness, energy, etc.) dominate the decision-making process.
* **The Authoritarian Wall (Mainstream Plateau > 13.09):** The model ignores acoustics and relies almost entirely on historical authority and market reach.

**The Deep Cut Problem & MAE Justification:** On a surface level, the enriched model appeared to regress, yielding a global MAE of 14.89 (compared to Cycle 03's baseline of 14.39). However, segmenting the error using our mathematically derived thresholds revealed a structural fracture. The model successfully broke the 14.39 barrier for the bottom **55.1% of the catalog (Cold Start, MAE 12.09)**. The global error inflation is entirely driven by the transition into higher-authority regimes: the **Tipping Point (MAE 17.63)** and the **Mainstream Plateau (MAE 19.95)**. In the Mainstream, a staggering 81.9% of errors are overpredictions. We empirically proved this is an *Irreducible Error* (Bayes Error) caused by a lack of intra-catalog metadata. A monolithic model cannot differentiate a mega-star's global hit from their obscure album filler.

### 5. K-Optimization & Topological Mapping

**Threshold Validation (From Heuristic to Data-Driven):** The initial visual thresholds (5.0 and 12.0) used in early exploration were subsequently validated by two independent data-driven methods on the SHAP values: a shallow Decision Tree (capturing piecewise rule changes in SHAP impact) and K-Means clustering (capturing latent sociological regimes). Both confirmed the existence of three distinct regimes, with the K-Means thresholds converging to **8.81 and 13.09**. These specific values were adopted for the final Cycle 05 gating architecture due to their topological stability and alignment with real-world cultural boundaries.

* **Model Logic (SHAP):** Forcing a k=3 clustering perfectly reconstructed the three macro-behaviors (Cold Start < 8.81, Tipping Point 8.81 - 13.09, Mainstream > 13.09).
* **Real-World Topology (Raw Data):** The Silhouette score explicitly peaked at **k=4**, empirically proving that the cultural landscape of the dataset has 4 distinct dimensions (Elite, Hybrid Frontier, Middle-Class, Underground).

**Architectural Flow (MoE):**
```mermaid
graph LR
    %% Core Flow
    Input[Track Data] --> Router{Gating Router<br>Listeners Log<br>n=3}
        
    %% Contextual Injections
    Topo[Topology Distances<br>k=4] ==> E1
    Topo ==> E2
    Topo ==> E3
    MB[MusicBrainz: Release Type] -.-> E3
    
    Router -- "log < 8.81" --> E1[Expert 1: Cold Start]
    Router -- "8.81 < log <  13.09" --> E2[Expert 2: Tipping Point]
    Router -- "log > 13.09" --> E3[Expert 3: Mainstream]

    %% Output
    E1 --> Out([Final Prediction])
    E2 --> Out
    E3 --> Out

    %% Styling to highlight Cycle 05 focus areas
    style Router fill:#f9f,stroke:#333,stroke-width:2px,color:#000
    style E3 fill:#bbf,stroke:#333,stroke-width:2px,color:#000
    style MB fill:#dfd,stroke:#333,stroke-dasharray: 5 5,color:#000
```

### 6. CYCLE 04 VERDICT & HANDOVER TO CYCLE 05

**Verdict:** Cycle 04 successfully broke the acoustic ceiling for the majority of the catalog (MAE 12.09 for the bottom 55%) and mapped the sociological physics of the streaming era. The Last.fm ingestion exposed the structural heterogeneity of the industry. Because the model fractures the moment an artist gains traction (MAE spiking to 17.63 and 19.95), we proved that the monolithic regression architecture is capacity-bounded and regime-blind. We cannot solve a multi-regime industry with a single-regime model, making the pivot to a Mixture of Experts (MoE) architecture mathematically necessary.

**The Cycle 05 Blueprint (Mixture of Experts):**
1.  **Segmented Gating:** Implement a routing classifier using the 3 data-driven regimes (Cold Start < 8.81, Tipping Point 8.81 to 13.09, Mainstream > 13.09).
2.  **Specialized Experts:** Train dedicated models for each regime.
3.  **Topological Features:** Use the k=4 clusters to create contextual distance metrics (e.g., `distance_to_elite_centroid`).

**Artifacts Frozen for Cycle 05 Handover:**
* `c4_global_challenger.json`: The baseline global model.
* `c4_topology_scaler.pkl`: The exact scaler used for the topological feature space.
* `c4_topology_kmeans_k4.pkl`: The trained k=4 KMeans model for dynamic feature engineering.
* `run_metadata_cycle4.json`: The governance platinum standard, containing the regime thresholds and feature contracts.