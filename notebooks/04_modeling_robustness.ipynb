{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5e0c213-12e0-40d3-b6b4-048adc03d4bc",
   "metadata": {},
   "source": [
    "# 0. PopForecast — Modeling & Robustness\n",
    "\n",
    "**Purpose**\n",
    "\n",
    "- Operationalize the Cycle 3 pivot: combine tree-based non-linearity with robustness to outliers.\n",
    "- Compare XGBoost (naive vs robust objectives) against the frozen baseline (HuberRegressor) on the 2021 test set.\n",
    "- Decide whether robust XGBoost beats MAE 15.21 and, if so, persist the best model to `models/cycle_03/xgb_robust.joblib`.\n",
    "\n",
    "# 1. Setup\n",
    "\n",
    "## 1.1 - Project root & module path setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "a71aafa9-0ce5-4bae-ab23-36f385bbb6e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T15:40:54.406562Z",
     "iopub.status.busy": "2026-02-16T15:40:54.405674Z",
     "iopub.status.idle": "2026-02-16T15:40:54.501257Z",
     "shell.execute_reply": "2026-02-16T15:40:54.497928Z",
     "shell.execute_reply.started": "2026-02-16T15:40:54.406506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /mnt/c/Users/Daniel/OneDrive/Documentos/_Cursos/Outros/PopForecast\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Final\n",
    "\n",
    "# --- Project root setup (so `src/` is importable from notebooks/) ---\n",
    "PROJECT_ROOT: Final[Path] = Path.cwd().parent\n",
    "\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abf2c6b-9621-477a-aa54-d3b71cf04566",
   "metadata": {},
   "source": [
    "## 1.2 - Project paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "a4f0fa40-0aac-49fd-a7f5-0ccd99acb50d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T15:40:55.251238Z",
     "iopub.status.busy": "2026-02-16T15:40:55.250316Z",
     "iopub.status.idle": "2026-02-16T15:40:55.280251Z",
     "shell.execute_reply": "2026-02-16T15:40:55.276453Z",
     "shell.execute_reply.started": "2026-02-16T15:40:55.251201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed dataset: /mnt/c/Users/Daniel/OneDrive/Documentos/_Cursos/Outros/PopForecast/data/processed/spotify_tracks_modeling.parquet\n",
      "Frozen config: /mnt/c/Users/Daniel/OneDrive/Documentos/_Cursos/Outros/PopForecast/models/cycle_02/baseline_huber15_audit_v3_from_pack.json\n",
      "Cycle 3 models dir: /mnt/c/Users/Daniel/OneDrive/Documentos/_Cursos/Outros/PopForecast/models/cycle_03\n",
      "Best model path: /mnt/c/Users/Daniel/OneDrive/Documentos/_Cursos/Outros/PopForecast/models/cycle_03/xgb_robust.joblib\n"
     ]
    }
   ],
   "source": [
    "# --- Data input ---\n",
    "DATA_PROCESSED_PATH = PROJECT_ROOT / \"data\" / \"processed\" / \"spotify_tracks_modeling.parquet\"\n",
    "\n",
    "# --- Cycle 2 (frozen config as single source of truth) ---\n",
    "CYCLE2_MODELS_DIR = PROJECT_ROOT / \"models\" / \"cycle_02\"\n",
    "FROZEN_CONFIG_PATH = CYCLE2_MODELS_DIR / \"baseline_huber15_audit_v3_from_pack.json\"\n",
    "\n",
    "# --- Cycle 3 (outputs; no mkdir at setup time) ---\n",
    "CYCLE3_MODELS_DIR = PROJECT_ROOT / \"models\" / \"cycle_03\"\n",
    "BEST_MODEL_PATH = CYCLE3_MODELS_DIR / \"xgb_robust.joblib\"\n",
    "CHAMPION_PATH = CYCLE3_MODELS_DIR / \"champion.joblib\"\n",
    "RUN_METADATA_PATH = CYCLE3_MODELS_DIR / \"run_metadata_cycle3.json\"\n",
    "\n",
    "\n",
    "print(\"Processed dataset:\", DATA_PROCESSED_PATH)\n",
    "print(\"Frozen config:\", FROZEN_CONFIG_PATH)\n",
    "print(\"Cycle 3 models dir:\", CYCLE3_MODELS_DIR)\n",
    "print(\"Best model path:\", BEST_MODEL_PATH)\n",
    "\n",
    "if not DATA_PROCESSED_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Processed dataset not found: {DATA_PROCESSED_PATH}\")\n",
    "\n",
    "if not FROZEN_CONFIG_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Frozen config not found: {FROZEN_CONFIG_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49529e22-3c9e-4958-a611-e92dc1d1d6e1",
   "metadata": {},
   "source": [
    "## 1.3 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "3539e881-3e34-4d08-9ea0-9046fa599e74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T05:51:14.204544Z",
     "iopub.status.busy": "2026-02-15T05:51:14.203087Z",
     "iopub.status.idle": "2026-02-15T05:51:14.256630Z",
     "shell.execute_reply": "2026-02-15T05:51:14.253946Z",
     "shell.execute_reply.started": "2026-02-15T05:51:14.204433Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Standard library ---\n",
    "import gc\n",
    "import json\n",
    "from dataclasses import asdict\n",
    "from typing import Any, Dict\n",
    "\n",
    "import hashlib\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Union\n",
    "\n",
    "# --- Third-party ---\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# --- Scikit-learn ---\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# --- Local project (src/) ---\n",
    "from src.core.features import (\n",
    "    FeatureEngineeringConfig,\n",
    "    apply_feature_engineering,\n",
    "    build_feature_pipeline,\n",
    ")\n",
    "from src.core.preprocessing import default_config, run_preprocessing\n",
    "\n",
    "\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# --- Standard library ---\n",
    "import hashlib\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# --- Standard library ---\n",
    "import hashlib\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from typing import Iterable\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import hashlib\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import asdict\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import randint, uniform\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.model_selection import PredefinedSplit, RandomizedSearchCV\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import PredefinedSplit, RandomizedSearchCV\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression, HuberRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import TweedieRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import ParameterSampler, PredefinedSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression, HuberRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import joblib\n",
    "import platform\n",
    "import sys\n",
    "import hashlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78396754-e137-463c-a5fe-5e3639034fa3",
   "metadata": {},
   "source": [
    "## 1.4 - Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b7cdc67-e90d-436e-8a83-f696c1388f91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T14:04:26.696412Z",
     "iopub.status.busy": "2026-02-13T14:04:26.695854Z",
     "iopub.status.idle": "2026-02-13T14:04:26.701355Z",
     "shell.execute_reply": "2026-02-13T14:04:26.699721Z",
     "shell.execute_reply.started": "2026-02-13T14:04:26.696382Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Reproducibility (only for stochastic procedures inside this notebook) ---\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# --- Pandas display ---\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 120)\n",
    "pd.set_option(\"display.max_colwidth\", 60)\n",
    "pd.set_option(\"display.float_format\", \"{:,.4f}\".format)\n",
    "\n",
    "# --- Matplotlib defaults (lightweight) ---\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 4)\n",
    "plt.rcParams[\"axes.grid\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7364e070-4848-4b7c-ad5d-c4cd119196b0",
   "metadata": {},
   "source": [
    "## 1.5 - Support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "e3d42c55-a7db-491d-9373-d770bc454c01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T05:51:18.824971Z",
     "iopub.status.busy": "2026-02-15T05:51:18.824089Z",
     "iopub.status.idle": "2026-02-15T05:51:18.995534Z",
     "shell.execute_reply": "2026-02-15T05:51:18.994041Z",
     "shell.execute_reply.started": "2026-02-15T05:51:18.824903Z"
    }
   },
   "outputs": [],
   "source": [
    "def _sha256_of_bytes(data: bytes) -> str:\n",
    "    return hashlib.sha256(data).hexdigest()\n",
    "\n",
    "\n",
    "def _extract_cycle3_protocol_key(config: dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Extract the Cycle 3 frozen protocol key from the config.\n",
    "\n",
    "    Supported schemas:\n",
    "    1) frozen_track_cycle3: \"<protocol_key>\"\n",
    "    2) frozen_track_cycle3: {\"protocol_name\": \"<protocol_key>\", ...}\n",
    "       (also accepts: \"protocol\", \"protocol_key\", \"key\", \"name\")\n",
    "    3) frozen_track_cycle3: {...} without protocol key:\n",
    "       - If baseline_protocols has exactly one entry, that entry is used as fallback.\n",
    "       - If multiple entries exist, raise (ambiguous).\n",
    "    \"\"\"\n",
    "    baseline_protocols = config.get(\"baseline_protocols\")\n",
    "    if not isinstance(baseline_protocols, dict) or not baseline_protocols:\n",
    "        raise ValueError(\"'baseline_protocols' must be a non-empty dict.\")\n",
    "\n",
    "    raw = config.get(\"frozen_track_cycle3\")\n",
    "\n",
    "    # Case 1: string directly names the protocol\n",
    "    if isinstance(raw, str) and raw.strip():\n",
    "        return raw.strip()\n",
    "\n",
    "    # Case 2: dict includes protocol key\n",
    "    if isinstance(raw, dict):\n",
    "        for candidate_key in (\"protocol_name\", \"protocol\", \"protocol_key\", \"key\", \"name\"):\n",
    "            val = raw.get(candidate_key)\n",
    "            if isinstance(val, str) and val.strip():\n",
    "                return val.strip()\n",
    "\n",
    "    # Case 3: fallback if exactly one protocol exists\n",
    "    if len(baseline_protocols) == 1:\n",
    "        return next(iter(baseline_protocols.keys()))\n",
    "\n",
    "    raise ValueError(\n",
    "        \"Could not determine the Cycle 3 frozen protocol key. \"\n",
    "        \"Either set 'frozen_track_cycle3' as a non-empty string protocol key, \"\n",
    "        \"or include one of: ['protocol_name','protocol','protocol_key','key','name'], \"\n",
    "        \"or keep exactly one entry in 'baseline_protocols'.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def load_frozen_config(path: str | Path) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load the official frozen Cycle 2 configuration JSON used as the single source of truth for Cycle 3.\n",
    "\n",
    "    - Returns a plain dict to keep schema flexible.\n",
    "    - Validates the presence of core top-level keys.\n",
    "    - Extracts the Cycle 3 frozen protocol key in a schema-tolerant way.\n",
    "    - Prints a SHA256 digest for traceability.\n",
    "    \"\"\"\n",
    "    config_path = Path(path)\n",
    "\n",
    "    if not config_path.exists():\n",
    "        raise FileNotFoundError(f\"Frozen config not found: {config_path}\")\n",
    "\n",
    "    raw_bytes = config_path.read_bytes()\n",
    "    digest = _sha256_of_bytes(raw_bytes)\n",
    "\n",
    "    config = json.loads(raw_bytes.decode(\"utf-8\"))\n",
    "\n",
    "    if not isinstance(config, dict) or not config:\n",
    "        raise ValueError(\"Frozen config must be a non-empty JSON object (dict).\")\n",
    "\n",
    "    required_top_level_keys = (\n",
    "        \"cycle\",\n",
    "        \"description\",\n",
    "        \"decision_split\",\n",
    "        \"guardrail_split\",\n",
    "        \"metrics\",\n",
    "        \"baseline_protocols\",\n",
    "        \"frozen_track_cycle3\",\n",
    "    )\n",
    "    missing = [k for k in required_top_level_keys if k not in config]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Frozen config is missing required keys: {missing}\")\n",
    "\n",
    "    protocol_key = _extract_cycle3_protocol_key(config)\n",
    "\n",
    "    if protocol_key not in config[\"baseline_protocols\"]:\n",
    "        raise KeyError(\n",
    "            \"Extracted Cycle 3 protocol key is not present in 'baseline_protocols'. \"\n",
    "            f\"Missing protocol key: {protocol_key}\"\n",
    "        )\n",
    "\n",
    "    print(f\"Frozen config loaded: {config_path.name}\")\n",
    "    print(f\"Frozen config SHA256: {digest}\")\n",
    "    print(f\"Cycle 3 frozen protocol: {protocol_key}\")\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "\n",
    "def _to_1d_float_array(x: np.ndarray | pd.Series | list, *, name: str) -> np.ndarray:\n",
    "    arr = np.asarray(x, dtype=float)\n",
    "\n",
    "    if arr.ndim == 0:\n",
    "        raise ValueError(f\"{name} must be 1D-like, got a scalar.\")\n",
    "    if arr.ndim > 1:\n",
    "        arr = arr.reshape(-1)\n",
    "\n",
    "    return arr\n",
    "\n",
    "\n",
    "def regression_metrics(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"Global regression metrics (MAE as primary).\"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    return {\"mae\": float(mae)}\n",
    "\n",
    "\n",
    "def segmented_metrics(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Segment metrics by whether the true target is zero.\n",
    "\n",
    "    This is useful under zero-inflation drift (e.g., 2021 jump).\n",
    "    \"\"\"\n",
    "    y_true_arr = np.asarray(y_true, dtype=float).reshape(-1)\n",
    "    y_pred_arr = np.asarray(y_pred, dtype=float).reshape(-1)\n",
    "\n",
    "    zero_mask = y_true_arr == 0\n",
    "    pos_mask = ~zero_mask\n",
    "\n",
    "    out: dict[str, float] = {\n",
    "        \"zero_rate_true\": float(zero_mask.mean()),\n",
    "        \"pos_rate_true\": float(pos_mask.mean()),\n",
    "    }\n",
    "\n",
    "    if zero_mask.any():\n",
    "        out[\"mae_zero\"] = float(mean_absolute_error(y_true_arr[zero_mask], y_pred_arr[zero_mask]))\n",
    "    else:\n",
    "        out[\"mae_zero\"] = float(\"nan\")\n",
    "\n",
    "    if pos_mask.any():\n",
    "        out[\"mae_pos\"] = float(mean_absolute_error(y_true_arr[pos_mask], y_pred_arr[pos_mask]))\n",
    "    else:\n",
    "        out[\"mae_pos\"] = float(\"nan\")\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def full_metrics(\n",
    "    y_true: np.ndarray | pd.Series | list,\n",
    "    y_pred: np.ndarray | pd.Series | list,\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"Convenience wrapper: global + segmented metrics.\"\"\"\n",
    "    y_true_arr = _to_1d_float_array(y_true, name=\"y_true\")\n",
    "    y_pred_arr = _to_1d_float_array(y_pred, name=\"y_pred\")\n",
    "\n",
    "    if y_true_arr.shape[0] != y_pred_arr.shape[0]:\n",
    "        raise ValueError(\n",
    "            \"y_true and y_pred must have the same length. \"\n",
    "            f\"Got {y_true_arr.shape[0]} and {y_pred_arr.shape[0]}.\"\n",
    "        )\n",
    "\n",
    "    out: dict[str, float] = {}\n",
    "    out.update(regression_metrics(y_true_arr, y_pred_arr))\n",
    "    out.update(segmented_metrics(y_true_arr, y_pred_arr))\n",
    "    return out\n",
    "\n",
    "\n",
    "def split_table(\n",
    "    df: pd.DataFrame,\n",
    "    split_masks: dict[str, pd.Series],\n",
    "    *,\n",
    "    year_col: str,\n",
    "    target_col: str | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a compact summary table for each split (rows, year range, and optional target stats).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df:\n",
    "        Full dataframe containing at least `year_col` and (optionally) `target_col`.\n",
    "    split_masks:\n",
    "        Mapping like {\"train\": mask, \"val\": mask, \"test\": mask}, where each mask is a boolean Series.\n",
    "    year_col:\n",
    "        Column holding the release year used for temporal splits.\n",
    "    target_col:\n",
    "        If provided, computes basic target stats (mean/median and zero rate).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        One row per split.\n",
    "    \"\"\"\n",
    "    rows: list[dict[str, object]] = []\n",
    "\n",
    "    for split_name, mask in split_masks.items():\n",
    "        split_df = df.loc[mask]\n",
    "        years = split_df[year_col].dropna()\n",
    "\n",
    "        summary: dict[str, object] = {\n",
    "            \"split\": split_name,\n",
    "            \"rows\": int(split_df.shape[0]),\n",
    "            \"min_year\": int(years.min()) if not years.empty else None,\n",
    "            \"max_year\": int(years.max()) if not years.empty else None,\n",
    "        }\n",
    "\n",
    "        if target_col is not None:\n",
    "            y = pd.to_numeric(split_df[target_col], errors=\"coerce\")\n",
    "            summary[\"target_mean\"] = float(y.mean()) if y.notna().any() else None\n",
    "            summary[\"target_median\"] = float(y.median()) if y.notna().any() else None\n",
    "            summary[\"target_zero_rate\"] = float((y == 0).mean()) if y.notna().any() else None\n",
    "\n",
    "        rows.append(summary)\n",
    "\n",
    "    return pd.DataFrame(rows).sort_values(\"split\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "def assert_expected_year_coverage(\n",
    "    df: pd.DataFrame,\n",
    "    split_masks: dict[str, pd.Series],\n",
    "    *,\n",
    "    year_col: str,\n",
    "    train_max_year: int,\n",
    "    val_years: set[int],\n",
    "    test_years: set[int],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Fail fast if the temporal split does not match the frozen expectations.\n",
    "    \"\"\"\n",
    "    required_splits = {\"train\", \"val\", \"test\"}\n",
    "    missing = required_splits.difference(split_masks.keys())\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing required split masks: {sorted(missing)}\")\n",
    "\n",
    "    def _years_for(mask: pd.Series) -> set[int]:\n",
    "        years = pd.to_numeric(df.loc[mask, year_col], errors=\"coerce\").dropna().astype(int)\n",
    "        return set(years.tolist())\n",
    "\n",
    "    train_years = _years_for(split_masks[\"train\"])\n",
    "    val_years_found = _years_for(split_masks[\"val\"])\n",
    "    test_years_found = _years_for(split_masks[\"test\"])\n",
    "\n",
    "    if df.loc[split_masks[\"train\"]].empty:\n",
    "        raise AssertionError(\"Train split is empty.\")\n",
    "    if df.loc[split_masks[\"val\"]].empty:\n",
    "        raise AssertionError(\"Validation split is empty.\")\n",
    "    if df.loc[split_masks[\"test\"]].empty:\n",
    "        raise AssertionError(\"Test split is empty.\")\n",
    "\n",
    "    if any(y > train_max_year for y in train_years):\n",
    "        raise AssertionError(\n",
    "            f\"Train split contains years > {train_max_year}. Found: {sorted(train_years)}\"\n",
    "        )\n",
    "\n",
    "    if val_years_found != val_years:\n",
    "        raise AssertionError(\n",
    "            f\"Validation years mismatch. Expected: {sorted(val_years)}; Found: {sorted(val_years_found)}\"\n",
    "        )\n",
    "\n",
    "    if test_years_found != test_years:\n",
    "        raise AssertionError(\n",
    "            f\"Test years mismatch. Expected: {sorted(test_years)}; Found: {sorted(test_years_found)}\"\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class XgbObjectiveCheckResult:\n",
    "    ok: bool\n",
    "    error: str\n",
    "\n",
    "\n",
    "def check_xgboost_objectives(\n",
    "    objectives: Tuple[str, ...],\n",
    "    random_seed: int = 42,\n",
    ") -> Dict[str, XgbObjectiveCheckResult]:\n",
    "    \"\"\"\n",
    "    Smoke-test XGBoost objectives by fitting a tiny model.\n",
    "    This verifies that the installed XGBoost build supports the objectives at runtime.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_seed)\n",
    "    x_train = rng.normal(size=(64, 4)).astype(np.float32)\n",
    "    y_train = rng.normal(size=(64,)).astype(np.float32)\n",
    "\n",
    "    results: Dict[str, XgbObjectiveCheckResult] = {}\n",
    "\n",
    "    for obj in objectives:\n",
    "        try:\n",
    "            model = XGBRegressor(\n",
    "                objective=obj,\n",
    "                n_estimators=5,\n",
    "                max_depth=3,\n",
    "                learning_rate=0.2,\n",
    "                subsample=0.9,\n",
    "                colsample_bytree=0.9,\n",
    "                random_state=RANDOM_SEED,\n",
    "                n_jobs=1,\n",
    "                tree_method=\"hist\",\n",
    "                verbosity=0,\n",
    "            )\n",
    "            model.fit(x_train, y_train)\n",
    "            results[obj] = XgbObjectiveCheckResult(ok=True, error=\"\")\n",
    "        except Exception as exc:  # noqa: BLE001\n",
    "            results[obj] = XgbObjectiveCheckResult(ok=False, error=str(exc))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def build_temporal_split_masks(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    year_col: str,\n",
    "    train_max_year: int,\n",
    "    val_year: int,\n",
    "    test_year: int,\n",
    "    nan_policy: Literal[\"error\", \"train\", \"drop\"] = \"error\",\n",
    ") -> tuple[pd.Series, pd.Series, pd.Series]:\n",
    "    \"\"\"\n",
    "    Build mutually exclusive temporal split masks.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nan_policy:\n",
    "        - \"error\": raise if year_col contains NaNs after coercion (strict).\n",
    "        - \"train\": assign NaN years to the train split (recommended for Huber-15 baseline parity).\n",
    "        - \"drop\": exclude NaN years from all splits (changes population; use only if explicitly desired).\n",
    "    \"\"\"\n",
    "    years = pd.to_numeric(df[year_col], errors=\"coerce\")\n",
    "\n",
    "    n_bad = int(years.isna().sum())\n",
    "    if n_bad > 0 and nan_policy == \"error\":\n",
    "        raise ValueError(f\"NaN years detected in column '{year_col}': {n_bad} rows.\")\n",
    "\n",
    "    train_mask = years <= train_max_year\n",
    "    val_mask = years == val_year\n",
    "    test_mask = years == test_year\n",
    "\n",
    "    if n_bad > 0 and nan_policy == \"train\":\n",
    "        train_mask = train_mask | years.isna()\n",
    "    elif n_bad > 0 and nan_policy == \"drop\":\n",
    "        # NaNs remain excluded from all masks (default behavior)\n",
    "        pass\n",
    "\n",
    "    if not (train_mask.any() and val_mask.any() and test_mask.any()):\n",
    "        raise ValueError(\n",
    "            \"Split masks are empty for at least one split. \"\n",
    "            f\"Counts: train={int(train_mask.sum())}, val={int(val_mask.sum())}, test={int(test_mask.sum())}.\"\n",
    "        )\n",
    "\n",
    "    overlap = (train_mask & val_mask) | (train_mask & test_mask) | (val_mask & test_mask)\n",
    "    if overlap.any():\n",
    "        raise ValueError(\"Split masks overlap. Temporal split must be mutually exclusive.\")\n",
    "\n",
    "    return train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "\n",
    "def extract_protocol_columns(\n",
    "    frozen_config: dict[str, Any],\n",
    "    *,\n",
    "    protocol_key: str,\n",
    ") -> list[str]:\n",
    "    protocols = frozen_config.get(\"baseline_protocols\", {})\n",
    "    if protocol_key not in protocols:\n",
    "        raise KeyError(f\"Protocol '{protocol_key}' not found under 'baseline_protocols'.\")\n",
    "\n",
    "    protocol = protocols[protocol_key]\n",
    "    cols = protocol.get(\"numeric_cols\", [])\n",
    "    if not isinstance(cols, list) or not cols:\n",
    "        raise ValueError(f\"Protocol '{protocol_key}' has no valid 'numeric_cols' list.\")\n",
    "    return [str(c) for c in cols]\n",
    "\n",
    "\n",
    "def fit_train_only_median_imputer(\n",
    "    X_train: pd.DataFrame,\n",
    ") -> SimpleImputer:\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    imputer.fit(X_train)\n",
    "    return imputer\n",
    "\n",
    "\n",
    "def transform_with_imputer(\n",
    "    imputer: SimpleImputer,\n",
    "    X: pd.DataFrame,\n",
    "    *,\n",
    "    columns: list[str],\n",
    "    index: pd.Index,\n",
    ") -> pd.DataFrame:\n",
    "    arr = imputer.transform(X)\n",
    "    return pd.DataFrame(arr, columns=columns, index=index)\n",
    "\n",
    "\n",
    "def make_recency_weights(\n",
    "    years: pd.Series,\n",
    "    *,\n",
    "    current_year: int,\n",
    "    lambda_recency: float,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute recency weights: w = exp(-lambda * age), with age = clip(current_year - year, lower=0).\n",
    "    \"\"\"\n",
    "    year_values = pd.to_numeric(years, errors=\"coerce\").to_numpy(dtype=float)\n",
    "\n",
    "    if np.isnan(year_values).any():\n",
    "        n_bad = int(np.isnan(year_values).sum())\n",
    "        raise ValueError(f\"NaN years detected while building recency weights: {n_bad} rows.\")\n",
    "\n",
    "    age = np.clip(current_year - year_values, a_min=0.0, a_max=None)\n",
    "    weights = np.exp(-lambda_recency * age)\n",
    "    return weights.astype(float)\n",
    "\n",
    "\n",
    "def compute_recency_weights(\n",
    "    years: pd.Series,\n",
    "    *,\n",
    "    current_year: int,\n",
    "    lambda_recency: float,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute recency weights w = exp(-lambda * age), where age = clip(current_year - year, lower=0).\n",
    "\n",
    "    Missing/invalid years are treated as age=0 (w=1.0).\n",
    "    \"\"\"\n",
    "    y = pd.to_numeric(years, errors=\"coerce\").to_numpy(dtype=float)\n",
    "    y = np.nan_to_num(y, nan=float(current_year))  # NaN -> current_year => age=0\n",
    "    age = np.clip(float(current_year) - y, a_min=0.0, a_max=None)\n",
    "    w = np.exp(-float(lambda_recency) * age)\n",
    "    return w.astype(np.float64, copy=False)\n",
    "\n",
    "    \n",
    "\n",
    "def summarize_array(x: np.ndarray, *, name: str) -> None:\n",
    "    p = np.percentile(x, [0, 1, 5, 50, 95, 99, 100])\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  n={x.size}\")\n",
    "    print(f\"  min={p[0]:.6f}  p01={p[1]:.6f}  p05={p[2]:.6f}  p50={p[3]:.6f}  p95={p[4]:.6f}  p99={p[5]:.6f}  max={p[6]:.6f}\")\n",
    "    print(f\"  mean={x.mean():.6f}  std={x.std():.6f}\")\n",
    "\n",
    "\n",
    "def evaluate_mae(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    return float(mean_absolute_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _sha256_of_bytes(data: bytes) -> str:\n",
    "    return hashlib.sha256(data).hexdigest()\n",
    "\n",
    "\n",
    "def load_json_dict(path: str | Path) -> dict[str, Any]:\n",
    "    \"\"\"Load a JSON file as a dict and print a SHA256 digest for traceability.\"\"\"\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"JSON not found: {p}\")\n",
    "\n",
    "    raw = p.read_bytes()\n",
    "    digest = _sha256_of_bytes(raw)\n",
    "\n",
    "    obj = json.loads(raw.decode(\"utf-8\"))\n",
    "    if not isinstance(obj, dict) or not obj:\n",
    "        raise ValueError(f\"JSON must be a non-empty dict: {p}\")\n",
    "\n",
    "    print(f\"Loaded: {p.name}\")\n",
    "    print(f\"SHA256:  {digest}\")\n",
    "    return obj\n",
    "\n",
    "\n",
    "def _require_dict_key(obj: dict[str, Any], key: str) -> dict[str, Any]:\n",
    "    val = obj.get(key)\n",
    "    if not isinstance(val, dict) or not val:\n",
    "        raise KeyError(f\"Missing or invalid dict key '{key}'.\")\n",
    "    return val\n",
    "\n",
    "\n",
    "def summarize_governance_config(cfg: dict[str, Any]) -> None:\n",
    "    \"\"\"Minimal summary for the governance (Cycle 2 frozen config).\"\"\"\n",
    "    print(\"\\n[Governance config]\")\n",
    "    print(\"Top-level keys:\", sorted(cfg.keys()))\n",
    "    print(\"Cycle:\", cfg.get(\"cycle\"))\n",
    "    print(\"Description:\", cfg.get(\"description\"))\n",
    "\n",
    "    decision = cfg.get(\"decision_split\", {})\n",
    "    guardrail = cfg.get(\"guardrail_split\", {})\n",
    "    metrics = cfg.get(\"metrics\", [])\n",
    "\n",
    "    if isinstance(decision, dict):\n",
    "        print(\"\\nDecision split:\")\n",
    "        for k, v in decision.items():\n",
    "            print(f\"  - {k}: {v}\")\n",
    "\n",
    "    if isinstance(guardrail, dict):\n",
    "        print(\"\\nGuardrail split:\")\n",
    "        for k, v in guardrail.items():\n",
    "            print(f\"  - {k}: {v}\")\n",
    "\n",
    "    if isinstance(metrics, list):\n",
    "        print(\"\\nMetrics (reporting):\")\n",
    "        print(\"  -\", metrics)\n",
    "\n",
    "\n",
    "def summarize_baseline_audit_v3(audit: dict[str, Any]) -> None:\n",
    "    \"\"\"Minimal summary for baseline audit v3 (derived from the NPZ pack arrays).\"\"\"\n",
    "    print(\"\\n[Baseline audit v3]\")\n",
    "    print(\"Top-level keys:\", sorted(audit.keys()))\n",
    "\n",
    "    protocol = _require_dict_key(audit, \"baseline_protocol\")\n",
    "\n",
    "    print(\"\\nBaseline protocol:\")\n",
    "    print(\"  - name:\", protocol.get(\"name\"))\n",
    "    cols = protocol.get(\"numeric_cols_15\", [])\n",
    "    if isinstance(cols, list):\n",
    "        print(\"  - numeric_cols_15:\", len(cols))\n",
    "        print(\"  - first 15 cols:\", cols[:15])\n",
    "\n",
    "    print(\"\\nRecency weighting:\")\n",
    "    print(\"  - recency_lambda:\", protocol.get(\"recency_lambda\"))\n",
    "    print(\"  - current_year:\", protocol.get(\"current_year\"))\n",
    "\n",
    "    imputer = protocol.get(\"imputer\", {})\n",
    "    if isinstance(imputer, dict):\n",
    "        print(\"\\nImputer:\")\n",
    "        for k, v in imputer.items():\n",
    "            print(f\"  - {k}: {v}\")\n",
    "\n",
    "    print(\"\\nPack NPZ SHA256:\", audit.get(\"pack_npz_sha256\"))\n",
    "\n",
    "\n",
    "def validate_cycle3_alignment(\n",
    "    *,\n",
    "    governance_cfg: dict[str, Any],\n",
    "    baseline_audit: dict[str, Any],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Validate that governance config and baseline audit v3 agree on Cycle 3 invariants.\n",
    "\n",
    "    - Governance must define the temporal decision split and include MAE in reporting metrics.\n",
    "    - Audit v3 must define the baseline protocol name, the exact 15 numeric columns,\n",
    "      and recency settings (lambda/current_year).\n",
    "    \"\"\"\n",
    "    # Governance checks\n",
    "    decision = governance_cfg.get(\"decision_split\")\n",
    "    if not isinstance(decision, dict):\n",
    "        raise KeyError(\"Governance config missing 'decision_split' (dict).\")\n",
    "\n",
    "    required_decision_keys = {\"train\", \"val\", \"test\"}\n",
    "    if not required_decision_keys.issubset(decision.keys()):\n",
    "        missing = sorted(required_decision_keys - set(decision.keys()))\n",
    "        raise KeyError(f\"Governance decision_split missing keys: {missing}\")\n",
    "\n",
    "    metrics = governance_cfg.get(\"metrics\", [])\n",
    "    if not isinstance(metrics, list) or \"mae\" not in metrics:\n",
    "        raise ValueError(\"Governance metrics must include 'mae'.\")\n",
    "\n",
    "    # Audit v3 checks\n",
    "    protocol = _require_dict_key(baseline_audit, \"baseline_protocol\")\n",
    "\n",
    "    protocol_name = protocol.get(\"name\")\n",
    "    if not isinstance(protocol_name, str) or not protocol_name.strip():\n",
    "        raise ValueError(\"baseline_protocol.name must be a non-empty string.\")\n",
    "\n",
    "    cols = protocol.get(\"numeric_cols_15\")\n",
    "    if not isinstance(cols, list) or len(cols) != 15:\n",
    "        raise ValueError(\"baseline_protocol.numeric_cols_15 must be a list of length 15.\")\n",
    "\n",
    "    recency_lambda = protocol.get(\"recency_lambda\")\n",
    "    current_year = protocol.get(\"current_year\")\n",
    "    if not isinstance(recency_lambda, (int, float)) or float(recency_lambda) <= 0.0:\n",
    "        raise ValueError(\"baseline_protocol.recency_lambda must be a positive float.\")\n",
    "    if not isinstance(current_year, int):\n",
    "        raise ValueError(\"baseline_protocol.current_year must be an int.\")\n",
    "\n",
    "    imputer = protocol.get(\"imputer\")\n",
    "    if not isinstance(imputer, dict):\n",
    "        raise KeyError(\"baseline_protocol.imputer must be a dict.\")\n",
    "    if imputer.get(\"strategy\") != \"median\":\n",
    "        raise ValueError(\"baseline_protocol.imputer.strategy must be 'median'.\")\n",
    "    if imputer.get(\"fit_scope\") != \"train_only\":\n",
    "        raise ValueError(\"baseline_protocol.imputer.fit_scope must be 'train_only'.\")\n",
    "\n",
    "    print(\"\\n✅ Cycle 3 alignment check passed (governance ↔ baseline audit v3).\")\n",
    "    print(f\"   - baseline protocol: {protocol_name}\")\n",
    "    print(f\"   - numeric cols: {len(cols)}\")\n",
    "    print(f\"   - recency_lambda: {float(recency_lambda):.4f} | current_year: {current_year}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelRunResult:\n",
    "    model_name: str\n",
    "    params: dict[str, Any]\n",
    "    objective: Optional[str]\n",
    "    used_sample_weight: bool\n",
    "    mae_val_2020: float\n",
    "    mae_test_2021: float\n",
    "    pred_min_test: float\n",
    "    pred_max_test: float\n",
    "\n",
    "    def to_dict(self) -> dict[str, Any]:\n",
    "        return {\n",
    "            \"model_name\": self.model_name,\n",
    "            \"objective\": self.objective,\n",
    "            \"used_sample_weight\": self.used_sample_weight,\n",
    "            \"mae_val_2020\": self.mae_val_2020,\n",
    "            \"mae_test_2021\": self.mae_test_2021,\n",
    "            \"pred_min_test\": self.pred_min_test,\n",
    "            \"pred_max_test\": self.pred_max_test,\n",
    "            \"params\": self.params,\n",
    "        }\n",
    "\n",
    "\n",
    "def _safe_fit(\n",
    "    model: Any,\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    sample_weight: Optional[np.ndarray],\n",
    ") -> tuple[Any, bool]:\n",
    "    \"\"\"\n",
    "    Fit with sample_weight when supported; otherwise fall back to unweighted fit.\n",
    "    Returns (fitted_model, used_sample_weight).\n",
    "    \"\"\"\n",
    "    if sample_weight is None:\n",
    "        model.fit(X, y)\n",
    "        return model, False\n",
    "\n",
    "    try:\n",
    "        model.fit(X, y, sample_weight=sample_weight)\n",
    "        return model, True\n",
    "    except TypeError:\n",
    "        model.fit(X, y)\n",
    "        return model, False\n",
    "\n",
    "\n",
    "def _extract_objective(model: Any) -> Optional[str]:\n",
    "    \"\"\"Best-effort extraction of XGBoost objective (if present).\"\"\"\n",
    "    params = getattr(model, \"get_params\", None)\n",
    "    if callable(params):\n",
    "        p = model.get_params()\n",
    "        obj = p.get(\"objective\")\n",
    "        return str(obj) if obj is not None else None\n",
    "    return None\n",
    "\n",
    "\n",
    "def run_and_evaluate_model(\n",
    "    *,\n",
    "    model: Any,\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_val: pd.DataFrame,\n",
    "    y_val: pd.Series,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    "    sample_weight_train: Optional[np.ndarray] = None,\n",
    ") -> ModelRunResult:\n",
    "    \"\"\"\n",
    "    Protocol-guarded runner:\n",
    "    - fit on train (optionally weighted),\n",
    "    - evaluate MAE on Val 2020 and Test 2021,\n",
    "    - return a compact, loggable result.\n",
    "    \"\"\"\n",
    "    fitted, used_w = _safe_fit(model, X_train, y_train, sample_weight_train)\n",
    "\n",
    "    y_pred_val = fitted.predict(X_val)\n",
    "    y_pred_test = fitted.predict(X_test)\n",
    "\n",
    "    mae_val = float(mean_absolute_error(y_val, y_pred_val))\n",
    "    mae_test = float(mean_absolute_error(y_test, y_pred_test))\n",
    "\n",
    "    pred_min = float(np.min(y_pred_test))\n",
    "    pred_max = float(np.max(y_pred_test))\n",
    "\n",
    "    model_name = type(fitted).__name__\n",
    "    params = fitted.get_params() if hasattr(fitted, \"get_params\") else {}\n",
    "    objective = _extract_objective(fitted)\n",
    "\n",
    "    return ModelRunResult(\n",
    "        model_name=model_name,\n",
    "        params=params,\n",
    "        objective=objective,\n",
    "        used_sample_weight=used_w,\n",
    "        mae_val_2020=mae_val,\n",
    "        mae_test_2021=mae_test,\n",
    "        pred_min_test=pred_min,\n",
    "        pred_max_test=pred_max,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TuningResult:\n",
    "    best_params: dict[str, Any]\n",
    "    best_val_mae: float\n",
    "    best_model_eval: dict[str, Any]\n",
    "\n",
    "\n",
    "def tune_xgb_with_predefinedsplit_holdout(\n",
    "    *,\n",
    "    objective: str,\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    w_train: np.ndarray,\n",
    "    X_val: pd.DataFrame,\n",
    "    y_val: pd.Series,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    "    base_params: dict[str, Any] | None = None,\n",
    "    n_iter: int = 20,\n",
    "    random_state: int = RANDOM_SEED,\n",
    "    n_jobs: int = -1,\n",
    ") -> TuningResult:\n",
    "    \"\"\"\n",
    "    Tune XGBRegressor hyperparameters using a fixed holdout (Val 2020) via PredefinedSplit,\n",
    "    then evaluate the best model on Val 2020 and Test 2021 using the existing harness\n",
    "    `run_and_evaluate_model(...)`.\n",
    "\n",
    "    Notes:\n",
    "    - Training uses sample weights only on the TRAIN fold.\n",
    "    - Val is used strictly for selection; Test is only reported after selection.\n",
    "    \"\"\"\n",
    "    if base_params is None:\n",
    "        base_params = {}\n",
    "\n",
    "    # --- Build a single dataset for RandomizedSearchCV ---\n",
    "    X_trv = pd.concat([X_train, X_val], axis=0)\n",
    "    y_trv = pd.concat([y_train, y_val], axis=0)\n",
    "\n",
    "    # weights: train weights + val weights = 1.0 (so they don't affect fitting on val fold)\n",
    "    w_trv = np.concatenate([w_train, np.ones(len(y_val), dtype=float)])\n",
    "\n",
    "    # PredefinedSplit: -1 for train fold, 0 for validation fold\n",
    "    test_fold = np.concatenate(\n",
    "        [\n",
    "            -1 * np.ones(len(X_train), dtype=int),\n",
    "            np.zeros(len(X_val), dtype=int),\n",
    "        ]\n",
    "    )\n",
    "    ps = PredefinedSplit(test_fold=test_fold)\n",
    "\n",
    "    # --- Hyperparameter search space (as agreed) ---\n",
    "    param_distributions: dict[str, Any] = {\n",
    "        \"learning_rate\": np.linspace(0.01, 0.2, 50),\n",
    "        \"max_depth\": np.arange(3, 11),\n",
    "        \"subsample\": np.linspace(0.6, 1.0, 50),\n",
    "        \"colsample_bytree\": np.linspace(0.6, 1.0, 50),\n",
    "    }\n",
    "\n",
    "    model = XGBRegressor(\n",
    "        objective=objective,\n",
    "        n_estimators=int(base_params.get(\"n_estimators\", 800)),\n",
    "        tree_method=str(base_params.get(\"tree_method\", \"hist\")),\n",
    "        n_jobs=int(base_params.get(\"n_jobs\", n_jobs)),\n",
    "        random_state=int(base_params.get(\"random_state\", random_state)),\n",
    "        **{k: v for k, v in base_params.items() if k not in {\"n_estimators\", \"tree_method\", \"n_jobs\", \"random_state\"}},\n",
    "    )\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=int(n_iter),\n",
    "        scoring=\"neg_mean_absolute_error\",\n",
    "        cv=ps,\n",
    "        random_state=int(random_state),\n",
    "        n_jobs=int(n_jobs),\n",
    "        verbose=1,\n",
    "        refit=True,  # refit on train+val with best params\n",
    "    )\n",
    "\n",
    "    # Fit search with weights (train weighted, val unweighted)\n",
    "    search.fit(X_trv, y_trv, sample_weight=w_trv)\n",
    "\n",
    "    best_params: dict[str, Any] = dict(search.best_params_)\n",
    "    best_val_mae = float(-search.best_score_)\n",
    "\n",
    "    print(f\"Tuning objective: {objective}\")\n",
    "    print(\"Best params:\", best_params)\n",
    "    print(f\"Best holdout (Val 2020) MAE: {best_val_mae:.4f}\")\n",
    "\n",
    "    # --- Train final model on TRAIN only (protocol guardrail) ---\n",
    "    final_model = XGBRegressor(\n",
    "        objective=objective,\n",
    "        n_estimators=int(base_params.get(\"n_estimators\", 800)),\n",
    "        tree_method=str(base_params.get(\"tree_method\", \"hist\")),\n",
    "        n_jobs=int(base_params.get(\"n_jobs\", n_jobs)),\n",
    "        random_state=int(base_params.get(\"random_state\", random_state)),\n",
    "        **{k: v for k, v in base_params.items() if k not in {\"n_estimators\", \"tree_method\", \"n_jobs\", \"random_state\"}},\n",
    "        **best_params,\n",
    "    )\n",
    "\n",
    "    # IMPORTANT: evaluate using your existing harness (no extra kwargs)\n",
    "    res = run_and_evaluate_model(\n",
    "        model=final_model,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_val=X_val,\n",
    "        y_val=y_val,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        sample_weight_train=w_train,\n",
    "    )\n",
    "\n",
    "    # Normalize to dict + attach metadata OUTSIDE the harness\n",
    "    if hasattr(res, \"to_dict\"):\n",
    "        best_model_eval = res.to_dict()\n",
    "    elif isinstance(res, dict):\n",
    "        best_model_eval = dict(res)\n",
    "    else:\n",
    "        # e.g., pd.Series or dataclass-like without to_dict\n",
    "        best_model_eval = dict(res)\n",
    "\n",
    "    best_model_eval[\"objective\"] = objective\n",
    "    best_model_eval[\"tag\"] = f\"xgb_tuned_{objective}\"\n",
    "    best_model_eval[\"best_val_mae_from_search\"] = best_val_mae\n",
    "    best_model_eval[\"best_params\"] = best_params\n",
    "\n",
    "    return TuningResult(\n",
    "        best_params=best_params,\n",
    "        best_val_mae=best_val_mae,\n",
    "        best_model_eval=best_model_eval,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TuningResult:\n",
    "    best_params: dict[str, Any]\n",
    "    best_val_mae: float\n",
    "    best_model: XGBRegressor\n",
    "\n",
    "def tune_xgb_with_predefinedsplit_holdout(\n",
    "    *,\n",
    "    objective: str,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    w_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    base_params=None,\n",
    "    n_iter: int = 30,\n",
    "    random_state: int = 42,\n",
    "    n_jobs: int = -1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Randomized hyperparameter search using a strict train/validation holdout.\n",
    "    Optimized CPU version with:\n",
    "        - float32 arrays and no-copy conversions\n",
    "        - early stopping to reduce unnecessary tree growth\n",
    "        - joblib parallelization of the hyperparameter loop\n",
    "        - no changes to the scientific protocol\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    objective : str\n",
    "        XGBoost objective function (e.g., \"reg:absoluteerror\").\n",
    "    X_train, y_train : pd.DataFrame / pd.Series\n",
    "        Training data (already preprocessed).\n",
    "    w_train : np.ndarray\n",
    "        Sample weights for training.\n",
    "    X_val, y_val : pd.DataFrame / pd.Series\n",
    "        Validation data for model selection.\n",
    "    base_params : dict or None\n",
    "        Additional fixed parameters for XGBRegressor.\n",
    "    n_iter : int\n",
    "        Number of random hyperparameter samples.\n",
    "    random_state : int\n",
    "        Random seed for reproducibility.\n",
    "    n_jobs : int\n",
    "        Number of parallel jobs for joblib.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    TuningResult\n",
    "        Contains best_params, best_val_mae, and best_model.\n",
    "    \"\"\"\n",
    "\n",
    "    if base_params is None:\n",
    "        base_params = {}\n",
    "\n",
    "    # --- Convert once outside the loop (much faster) ---\n",
    "    Xtr = X_train.to_numpy(dtype=np.float32, copy=False)\n",
    "    Xva = X_val.to_numpy(dtype=np.float32, copy=False)\n",
    "    ytr = y_train.to_numpy(dtype=np.float32, copy=False)\n",
    "    yva = y_val.to_numpy(dtype=np.float32, copy=False)\n",
    "    wtr = w_train.astype(np.float32, copy=False)\n",
    "\n",
    "    # --- Optimized search space ---\n",
    "    param_dist = {\n",
    "        \"learning_rate\": np.linspace(0.02, 0.10, 10),\n",
    "        \"max_depth\": np.arange(4, 11),\n",
    "        \"subsample\": np.linspace(0.7, 1.0, 10),\n",
    "        \"colsample_bytree\": np.linspace(0.7, 1.0, 10),\n",
    "        \"min_child_weight\": [1, 5, 10],\n",
    "        \"gamma\": [0.0, 0.5, 1.0],\n",
    "        \"reg_alpha\": [0.0, 1e-3, 1e-2, 1e-1],\n",
    "        \"reg_lambda\": [1.0, 5.0, 10.0],\n",
    "    }\n",
    "\n",
    "    sampler = list(ParameterSampler(param_dist, n_iter=n_iter, random_state=random_state))\n",
    "\n",
    "    # --- Function executed in parallel for each hyperparameter set ---\n",
    "    def evaluate_params(params):\n",
    "        model = XGBRegressor(\n",
    "            objective=objective,\n",
    "            n_estimators=2000,            # large upper bound\n",
    "            early_stopping_rounds=50,     # stops early\n",
    "            eval_metric=\"mae\",\n",
    "            tree_method=\"hist\",           # CPU-optimized\n",
    "            random_state=random_state,\n",
    "            **base_params,\n",
    "            **params,\n",
    "        )\n",
    "\n",
    "        model.fit(\n",
    "            Xtr, ytr,\n",
    "            sample_weight=wtr,\n",
    "            eval_set=[(Xva, yva)],\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        pred_val = model.predict(Xva)\n",
    "        val_mae = float(mean_absolute_error(yva, pred_val))\n",
    "        return val_mae, params, model\n",
    "\n",
    "    # --- Parallel execution of the tuning loop ---\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(evaluate_params)(p) for p in sampler\n",
    "    )\n",
    "\n",
    "    # --- Select best model ---\n",
    "    best_val_mae, best_params, best_model = min(results, key=lambda x: x[0])\n",
    "\n",
    "    return TuningResult(\n",
    "        best_params=best_params,\n",
    "        best_val_mae=best_val_mae,\n",
    "        best_model=best_model,\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def evaluate_with_clip_0_100(y_true: np.ndarray, y_pred: np.ndarray) -> tuple[float, float, float, float]:\n",
    "    y_pred_clip = np.clip(y_pred.astype(float), 0.0, 100.0)\n",
    "    mae = float(mean_absolute_error(y_true.astype(float), y_pred_clip))\n",
    "    return mae, float(np.min(y_pred_clip)), float(np.max(y_pred_clip)), float(np.mean(y_pred_clip))\n",
    "\n",
    "\n",
    "\n",
    "def _to_float_np(x):\n",
    "    return x.to_numpy(dtype=float) if hasattr(x, \"to_numpy\") else np.asarray(x, dtype=float)\n",
    "\n",
    "\n",
    "\n",
    "def eval_with_clip_0_100(y_true, y_pred):\n",
    "    y_pred_c = np.clip(y_pred, 0.0, 100.0)\n",
    "    return float(mean_absolute_error(y_true, y_pred_c))\n",
    "\n",
    "\n",
    "def mae_clip(y_true, y_pred):\n",
    "    return mean_absolute_error(y_true, np.clip(y_pred, 0.0, 100.0))\n",
    "\n",
    "\n",
    "\n",
    "def _is_clip_tag(tag: str) -> bool:\n",
    "    return \"clip\" in str(tag).lower()\n",
    "\n",
    "def _sha256_file(path: Path) -> str:\n",
    "    return hashlib.sha256(path.read_bytes()).hexdigest()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_mode(mode_name, df):\n",
    "    mode = MODES[mode_name]\n",
    "\n",
    "    # Filter rows by tag semantics\n",
    "    df_mode = df[mode[\"filter\"](df)].copy()\n",
    "\n",
    "    # Keep only rows with both Val and Test\n",
    "    df_sel = df_mode.dropna(subset=[SEL_COL, TEST_COL]).copy()\n",
    "\n",
    "    # Baseline\n",
    "    baseline_tag = mode[\"baseline\"]\n",
    "    baseline_row = df_sel.loc[df_sel[\"tag\"].astype(str).eq(baseline_tag)]\n",
    "    if baseline_row.empty:\n",
    "        raise KeyError(f\"Baseline '{baseline_tag}' not found in mode '{mode_name}'.\")\n",
    "    baseline_row = baseline_row.iloc[0]\n",
    "    baseline_test = float(baseline_row[TEST_COL])\n",
    "\n",
    "    # Candidates (exclude baseline)\n",
    "    df_cand = df_sel.loc[df_sel[\"tag\"] != baseline_tag]\n",
    "\n",
    "    # Best by validation\n",
    "    best_row = df_cand.sort_values(SEL_COL, ascending=True).iloc[0]\n",
    "    best_tag  = str(best_row[\"tag\"])\n",
    "    best_test = float(best_row[TEST_COL])\n",
    "\n",
    "    # Champion\n",
    "    champion_tag = best_tag if best_test < baseline_test else baseline_tag\n",
    "\n",
    "    return {\n",
    "        \"mode\": mode_name,\n",
    "        \"baseline_tag\": baseline_tag,\n",
    "        \"baseline_test\": baseline_test,\n",
    "        \"best_tag\": best_tag,\n",
    "        \"best_test\": best_test,\n",
    "        \"gap\": best_test - baseline_test,\n",
    "        \"champion\": champion_tag,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb4206d-ad87-42c1-8af5-dd413c2463e4",
   "metadata": {},
   "source": [
    "## 1.6 - Load frozen artifacts (governance config + baseline audit + optional pack)\n",
    "\n",
    "Cycle 3 has **two complementary “sources of truth”**:\n",
    "\n",
    "1) **Cycle governance (what we decided and why)**  \n",
    "   `frozen_config_cycle2.json` captures the Cycle 2 decisions that govern Cycle 3:\n",
    "   - decision split definition (temporal),\n",
    "   - reporting metrics,\n",
    "   - the strategic pivot direction for Cycle 3.\n",
    "\n",
    "2) **Baseline contract (what must be reproduced exactly)**  \n",
    "   `baseline_huber15_audit_v3_from_pack.json` is the operational contract for the official baseline track:\n",
    "   **Baseline_Huber15_recency0p05_medfill**.  \n",
    "   It specifies:\n",
    "   - the exact list of **15 numeric columns**,\n",
    "   - Huber parameters,\n",
    "   - recency-weighting settings,\n",
    "   - and fingerprints/hashes for traceability.\n",
    "\n",
    "3) **Optional baseline pack (final reproducibility anchor)**  \n",
    "   The `.npz` pack is the *final* reproducibility anchor for the baseline (arrays + indices + weights).\n",
    "   We **do not** use it as the main training path in Cycle 3 (to keep the pipeline “alive”),\n",
    "   but we will use it later as an **audit gate** to confirm we can reproduce the baseline when needed.\n",
    "\n",
    "Next, we load the governance config and the baseline audit v3, print a structured summary, and\n",
    "validate that the baseline audit agrees with the Cycle 3 frozen protocol (columns / params / recency settings).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48cb8b6c-b34d-4329-b80d-dc3e4ef7e296",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T14:04:28.412277Z",
     "iopub.status.busy": "2026-02-13T14:04:28.411899Z",
     "iopub.status.idle": "2026-02-13T14:04:28.513738Z",
     "shell.execute_reply": "2026-02-13T14:04:28.512247Z",
     "shell.execute_reply.started": "2026-02-13T14:04:28.412249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: frozen_config_cycle2.json\n",
      "SHA256:  8aaebe8581946b1fe8268f4b8ee5fecb8e6307487977a12b324b164d642f0045\n",
      "Loaded: baseline_huber15_audit_v3_from_pack.json\n",
      "SHA256:  3915be4f9022e220d60ed24c4965906908b682eec3c130495858dc9891f50d12\n",
      "\n",
      "[Governance config]\n",
      "Top-level keys: ['baseline_protocols', 'cycle', 'cycle3_next_experiment', 'decision_split', 'description', 'feature_engineering_artifact_cycle2', 'frozen_track_cycle3', 'guardrail_split', 'hurdle_status_cycle2', 'metrics']\n",
      "Cycle: 2\n",
      "Description: Cycle 2 frozen config for Cycle 3 modeling (single-track: Huber-15 numeric-only). Engineered features kept as documented artifact but not the Cycle 3 benchmark track.\n",
      "\n",
      "Decision split:\n",
      "  - type: temporal\n",
      "  - train: <=2019\n",
      "  - val: 2020\n",
      "  - test: 2021\n",
      "\n",
      "Guardrail split:\n",
      "  - type: random\n",
      "  - train: 0.7\n",
      "  - val: 0.1\n",
      "  - test: 0.2\n",
      "\n",
      "Metrics (reporting):\n",
      "  - ['mae', 'rmse', 'r2', 'mae_zero', 'mae_pos', 'pct_zero_true', 'pred_zero_pct']\n",
      "\n",
      "[Baseline audit v3]\n",
      "Top-level keys: ['baseline_protocol', 'dtypes', 'hashes_from_pack_only', 'notes', 'pack_npz_sha256', 'shapes']\n",
      "\n",
      "Baseline protocol:\n",
      "  - name: Baseline_Huber15_recency0p05_medfill\n",
      "  - numeric_cols_15: 15\n",
      "  - first 15 cols: ['album_release_year', 'acousticness', 'danceability', 'duration_ms', 'energy', 'instrumentalness', 'key', 'liveness', 'loudness', 'mode', 'speechiness', 'tempo', 'time_signature', 'total_available_markets', 'valence']\n",
      "\n",
      "Recency weighting:\n",
      "  - recency_lambda: 0.05\n",
      "  - current_year: 2021\n",
      "\n",
      "Imputer:\n",
      "  - strategy: median\n",
      "  - fit_scope: train_only\n",
      "\n",
      "Pack NPZ SHA256: 0de188ce97b50b507ff47cd65a238d41154a9efb5a3e8512f4dd98a8e6dc59c3\n",
      "\n",
      "✅ Cycle 3 alignment check passed (governance ↔ baseline audit v3).\n",
      "   - baseline protocol: Baseline_Huber15_recency0p05_medfill\n",
      "   - numeric cols: 15\n",
      "   - recency_lambda: 0.0500 | current_year: 2021\n"
     ]
    }
   ],
   "source": [
    "# --- Load governance config + baseline audit v3 ---\n",
    "governance_cfg = load_json_dict(CYCLE2_MODELS_DIR / \"frozen_config_cycle2.json\")\n",
    "baseline_audit_v3 = load_json_dict(FROZEN_CONFIG_PATH)\n",
    "\n",
    "# --- Summaries ---\n",
    "summarize_governance_config(governance_cfg)\n",
    "summarize_baseline_audit_v3(baseline_audit_v3)\n",
    "\n",
    "# --- Lightweight consistency checks ---\n",
    "validate_cycle3_alignment(governance_cfg=governance_cfg, baseline_audit=baseline_audit_v3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c2ded1-6316-4652-8641-24d6f29d9f2c",
   "metadata": {},
   "source": [
    "# 2. Load Processed Dataset (Parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a44f5d19-42c4-47cb-b7ed-1a234805b4d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T14:04:29.298234Z",
     "iopub.status.busy": "2026-02-13T14:04:29.297886Z",
     "iopub.status.idle": "2026-02-13T14:04:29.832576Z",
     "shell.execute_reply": "2026-02-13T14:04:29.828569Z",
     "shell.execute_reply.started": "2026-02-13T14:04:29.298207Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_popularity</th>\n",
       "      <th>album_release_year</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>key</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>song_explicit</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>total_available_markets</th>\n",
       "      <th>valence</th>\n",
       "      <th>release_year_missing_or_suspect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49516</th>\n",
       "      <td>48</td>\n",
       "      <td>2019</td>\n",
       "      <td>0.0466</td>\n",
       "      <td>0.4700</td>\n",
       "      <td>213,159.0000</td>\n",
       "      <td>0.9610</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.3050</td>\n",
       "      <td>-0.8680</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.3630</td>\n",
       "      <td>164.8990</td>\n",
       "      <td>4</td>\n",
       "      <td>168</td>\n",
       "      <td>0.6480</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38956</th>\n",
       "      <td>51</td>\n",
       "      <td>2020</td>\n",
       "      <td>0.1130</td>\n",
       "      <td>0.7640</td>\n",
       "      <td>344,201.0000</td>\n",
       "      <td>0.5670</td>\n",
       "      <td>0.0241</td>\n",
       "      <td>6</td>\n",
       "      <td>0.1050</td>\n",
       "      <td>-9.3880</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0299</td>\n",
       "      <td>134.0250</td>\n",
       "      <td>4</td>\n",
       "      <td>170</td>\n",
       "      <td>0.6880</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221529</th>\n",
       "      <td>20</td>\n",
       "      <td>1988</td>\n",
       "      <td>0.5350</td>\n",
       "      <td>0.3440</td>\n",
       "      <td>321,627.0000</td>\n",
       "      <td>0.4590</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0677</td>\n",
       "      <td>-8.1630</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0342</td>\n",
       "      <td>125.8110</td>\n",
       "      <td>4</td>\n",
       "      <td>170</td>\n",
       "      <td>0.2510</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199218</th>\n",
       "      <td>22</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.8730</td>\n",
       "      <td>0.6270</td>\n",
       "      <td>133,933.0000</td>\n",
       "      <td>0.4680</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1690</td>\n",
       "      <td>-14.6700</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9370</td>\n",
       "      <td>78.5860</td>\n",
       "      <td>4</td>\n",
       "      <td>170</td>\n",
       "      <td>0.2780</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143101</th>\n",
       "      <td>30</td>\n",
       "      <td>2005</td>\n",
       "      <td>0.9330</td>\n",
       "      <td>0.1570</td>\n",
       "      <td>741,160.0000</td>\n",
       "      <td>0.2240</td>\n",
       "      <td>0.9100</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0748</td>\n",
       "      <td>-15.1410</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0349</td>\n",
       "      <td>98.2000</td>\n",
       "      <td>4</td>\n",
       "      <td>170</td>\n",
       "      <td>0.0383</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        song_popularity  album_release_year  acousticness  danceability  duration_ms  energy  instrumentalness  key  \\\n",
       "49516                48                2019        0.0466        0.4700 213,159.0000  0.9610            0.0000    7   \n",
       "38956                51                2020        0.1130        0.7640 344,201.0000  0.5670            0.0241    6   \n",
       "221529               20                1988        0.5350        0.3440 321,627.0000  0.4590            0.0000    1   \n",
       "199218               22                2018        0.8730        0.6270 133,933.0000  0.4680            0.0000    0   \n",
       "143101               30                2005        0.9330        0.1570 741,160.0000  0.2240            0.9100    7   \n",
       "\n",
       "        liveness  loudness  mode  song_explicit  speechiness    tempo  time_signature  total_available_markets  \\\n",
       "49516     0.3050   -0.8680     1          False       0.3630 164.8990               4                      168   \n",
       "38956     0.1050   -9.3880     0          False       0.0299 134.0250               4                      170   \n",
       "221529    0.0677   -8.1630     1          False       0.0342 125.8110               4                      170   \n",
       "199218    0.1690  -14.6700     1          False       0.9370  78.5860               4                      170   \n",
       "143101    0.0748  -15.1410     1          False       0.0349  98.2000               4                      170   \n",
       "\n",
       "        valence  release_year_missing_or_suspect  \n",
       "49516    0.6480                            False  \n",
       "38956    0.6880                            False  \n",
       "221529   0.2510                            False  \n",
       "199218   0.2780                            False  \n",
       "143101   0.0383                            False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (439865, 18)\n"
     ]
    }
   ],
   "source": [
    "# --- Load processed dataset (Parquet) ---\n",
    "if not DATA_PROCESSED_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"Processed dataset not found.\\n\"\n",
    "        f\"Expected at: {DATA_PROCESSED_PATH}\\n\"\n",
    "        \"Run the preprocessing pipeline to generate it, then re-run this notebook.\"\n",
    "    )\n",
    "\n",
    "df = pd.read_parquet(DATA_PROCESSED_PATH)\n",
    "\n",
    "display(df.sample(5, random_state=RANDOM_SEED))\n",
    "print(\"Shape:\", df.shape)\n",
    "\n",
    "# --- Minimal schema sanity checks (fail fast) ---\n",
    "required_cols = [\"album_release_year\"]\n",
    "missing_cols = [c for c in required_cols if c not in df.columns]\n",
    "if missing_cols:\n",
    "    raise KeyError(f\"Missing required columns in processed dataset: {missing_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b627db5f-4de3-4215-93d7-2916dd363d5d",
   "metadata": {},
   "source": [
    "# 3. Baseline Check — Huber-15 (Official Cycle 3 Track)\n",
    "\n",
    "Cycle 2 froze a **single official modeling track** for Cycle 3 to avoid mixing input spaces and to keep the benchmark fully reproducible:\n",
    "\n",
    "**Protocol:** `Baseline_Huber15_recency0p05_medfill`  \n",
    "- Input space: **15 raw numeric columns** (fixed list from the frozen config)  \n",
    "- Preprocessing: **median imputation**, fit on **train only**  \n",
    "- Temporal split: train `<=2019`, val `2020`, test `2021`  \n",
    "- Recency weighting: enabled, `lambda=0.05`, `current_year=2021`  \n",
    "- Baseline model: `HuberRegressor` (params frozen)\n",
    "\n",
    "In this section, we will reconstruct the exact **train/val/test matrices** under the frozen protocol and confirm the baseline performance before moving to robust XGBoost.\n",
    "\n",
    "## 3.1 - Temporal split masks (decision split) and protocol inputs\n",
    "\n",
    "We enforce the frozen **temporal split** using `album_release_year` and select the **exact 15 numeric columns** defined by the Cycle 3 baseline protocol.\n",
    "\n",
    "To keep the split deterministic and avoid leaking unknown years into the future, any missing/invalid `album_release_year` values are assigned to the **train** split (they are handled later by train-only median imputation).\n",
    "\n",
    "At this stage we only prepare the raw matrices; imputation and recency weights will be applied next.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e8a3220-d1fc-42cc-8c49-47c982a4c85b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T14:04:31.073376Z",
     "iopub.status.busy": "2026-02-13T14:04:31.073024Z",
     "iopub.status.idle": "2026-02-13T14:04:31.172229Z",
     "shell.execute_reply": "2026-02-13T14:04:31.171018Z",
     "shell.execute_reply.started": "2026-02-13T14:04:31.073348Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protocol: Baseline_Huber15_recency0p05_medfill\n",
      "Numeric cols: 15\n",
      "Split sizes: {'train': 283488, 'val': 105605, 'test': 50772}\n",
      "X shapes: (283488, 15) (105605, 15) (50772, 15)\n",
      "y shapes: (283488,) (105605,) (50772,)\n",
      "NaN years assigned to train: 22\n"
     ]
    }
   ],
   "source": [
    "YEAR_COL = \"album_release_year\"\n",
    "\n",
    "# Split parameters (frozen)\n",
    "train_max_year = int(governance_cfg[\"decision_split\"][\"train\"].replace(\"<=\", \"\"))\n",
    "val_year = int(governance_cfg[\"decision_split\"][\"val\"])\n",
    "test_year = int(governance_cfg[\"decision_split\"][\"test\"])\n",
    "\n",
    "train_mask, val_mask, test_mask = build_temporal_split_masks(\n",
    "    df,\n",
    "    year_col=YEAR_COL,\n",
    "    train_max_year=train_max_year,\n",
    "    val_year=val_year,\n",
    "    test_year=test_year,\n",
    "    nan_policy=\"train\",\n",
    ")\n",
    "\n",
    "protocol_key = _extract_cycle3_protocol_key(governance_cfg)\n",
    "numeric_cols = extract_protocol_columns(governance_cfg, protocol_key=protocol_key)\n",
    "\n",
    "missing_cols = [c for c in numeric_cols if c not in df.columns]\n",
    "if missing_cols:\n",
    "    raise KeyError(f\"Missing protocol columns in df: {missing_cols}\")\n",
    "\n",
    "TARGET_COL = \"song_popularity\"\n",
    "if TARGET_COL not in df.columns:\n",
    "    raise KeyError(f\"Target column not found: {TARGET_COL}\")\n",
    "\n",
    "# Raw split matrices (no imputation yet)\n",
    "X_train_raw = df.loc[train_mask, numeric_cols].copy()\n",
    "X_val_raw = df.loc[val_mask, numeric_cols].copy()\n",
    "X_test_raw = df.loc[test_mask, numeric_cols].copy()\n",
    "\n",
    "y_train = df.loc[train_mask, TARGET_COL].astype(float).copy()\n",
    "y_val = df.loc[val_mask, TARGET_COL].astype(float).copy()\n",
    "y_test = df.loc[test_mask, TARGET_COL].astype(float).copy()\n",
    "\n",
    "print(\"Protocol:\", protocol_key)\n",
    "print(\"Numeric cols:\", len(numeric_cols))\n",
    "print(\n",
    "    \"Split sizes:\",\n",
    "    {\"train\": int(train_mask.sum()), \"val\": int(val_mask.sum()), \"test\": int(test_mask.sum())},\n",
    ")\n",
    "print(\"X shapes:\", X_train_raw.shape, X_val_raw.shape, X_test_raw.shape)\n",
    "print(\"y shapes:\", y_train.shape, y_val.shape, y_test.shape)\n",
    "print(\"NaN years assigned to train:\", int(pd.to_numeric(df[YEAR_COL], errors=\"coerce\").isna().sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eb2562-9f9c-4f64-9af9-27c8b522ae26",
   "metadata": {},
   "source": [
    "## 3.2 - Median imputation (fit on train only)\n",
    "\n",
    "The protocol requires a `SimpleImputer(strategy=\"median\")` fit **only on the training split**.\n",
    "We apply the fitted imputer to train/val/test to produce the final numeric matrices used for modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff80b29a-b40c-4e93-900d-ca94b7598bca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T14:04:33.990782Z",
     "iopub.status.busy": "2026-02-13T14:04:33.990526Z",
     "iopub.status.idle": "2026-02-13T14:04:34.804898Z",
     "shell.execute_reply": "2026-02-13T14:04:34.803818Z",
     "shell.execute_reply.started": "2026-02-13T14:04:33.990754Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation applied (train-only fit).\n",
      "Any NaNs after imputation: {'train': False, 'val': False, 'test': False}\n"
     ]
    }
   ],
   "source": [
    "imputer = fit_train_only_median_imputer(X_train_raw)\n",
    "\n",
    "X_train = transform_with_imputer(imputer, X_train_raw, columns=numeric_cols, index=X_train_raw.index)\n",
    "X_val = transform_with_imputer(imputer, X_val_raw, columns=numeric_cols, index=X_val_raw.index)\n",
    "X_test = transform_with_imputer(imputer, X_test_raw, columns=numeric_cols, index=X_test_raw.index)\n",
    "\n",
    "print(\"Imputation applied (train-only fit).\")\n",
    "print(\"Any NaNs after imputation:\", {\"train\": bool(X_train.isna().any().any()),\n",
    "                                   \"val\": bool(X_val.isna().any().any()),\n",
    "                                   \"test\": bool(X_test.isna().any().any())})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4100a1fd-5c77-4390-a881-304c0465cb54",
   "metadata": {},
   "source": [
    "## 3.3 - Next: recency weighting + Huber fit (frozen params)\n",
    "\n",
    "Cycle 2 selected a conservative recency weighting scheme (`lambda=0.05`) to account for concept drift without degrading 2021 performance.\n",
    "\n",
    "We compute `sample_weight` **only for the training split**, using the frozen definition:\n",
    "\n",
    "- `age = clip(current_year - album_release_year, lower=0)`\n",
    "- `weight = exp(-lambda * age)`\n",
    "\n",
    "These weights will be passed to `HuberRegressor.fit(..., sample_weight=weights)` as part of the official protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f92ac4f3-b801-447b-886b-8dd7d87d7841",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T14:04:36.518449Z",
     "iopub.status.busy": "2026-02-13T14:04:36.518164Z",
     "iopub.status.idle": "2026-02-13T14:04:36.535258Z",
     "shell.execute_reply": "2026-02-13T14:04:36.534184Z",
     "shell.execute_reply.started": "2026-02-13T14:04:36.518429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recency weights computed (train only).\n",
      "  X_train rows: 283488\n",
      "  w_train len : 283488\n",
      "sample_weight_train:\n",
      "  n=283488\n",
      "  min=0.003028  p01=0.090718  p05=0.246597  p50=0.778801  p95=0.904837  p99=0.904837  max=1.000000\n",
      "  mean=0.695242  std=0.212252\n"
     ]
    }
   ],
   "source": [
    "lambda_recency = float(\n",
    "    governance_cfg[\"baseline_protocols\"][protocol_key][\"recency_weighting\"][\"lambda\"]\n",
    ")\n",
    "current_year = int(\n",
    "    governance_cfg[\"baseline_protocols\"][protocol_key][\"recency_weighting\"][\"current_year\"]\n",
    ")\n",
    "\n",
    "train_years = df.loc[train_mask, YEAR_COL]\n",
    "sample_weight_train = compute_recency_weights(\n",
    "    train_years,\n",
    "    current_year=current_year,\n",
    "    lambda_recency=lambda_recency,\n",
    ")\n",
    "\n",
    "print(\"Recency weights computed (train only).\")\n",
    "print(\"  X_train rows:\", X_train.shape[0])\n",
    "print(\"  w_train len :\", sample_weight_train.shape[0])\n",
    "if sample_weight_train.shape[0] != X_train.shape[0]:\n",
    "    raise ValueError(\"sample_weight_train length does not match X_train rows.\")\n",
    "\n",
    "summarize_array(sample_weight_train, name=\"sample_weight_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7ff8d9-36fd-4166-bb75-ba430e36c3eb",
   "metadata": {},
   "source": [
    "## 3.4 - Fit the frozen Huber baseline and evaluate (Val 2020 / Test 2021)\n",
    "\n",
    "With:\n",
    "- the frozen 15-column input space,\n",
    "- train-only median imputation,\n",
    "- and recency weights for training,\n",
    "\n",
    "we can now fit `HuberRegressor` using the **frozen hyperparameters** from the config and evaluate:\n",
    "\n",
    "- **Val (2020)**: to validate the decision split behavior.\n",
    "- **Test (2021)**: the benchmark we must reproduce and later beat with robust XGBoost objectives.\n",
    "\n",
    "We also report segmented MAE for `y==0` vs `y>0`, since 2021 has higher zero inflation.\n",
    "\n",
    "This step is the baseline anchor for the rest of Cycle 3: every challenger will use the same split and input space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "c2874ff9-0acb-4eff-82bc-f112a51e5594",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T02:00:46.516787Z",
     "iopub.status.busy": "2026-02-15T02:00:46.513878Z",
     "iopub.status.idle": "2026-02-15T02:00:53.874593Z",
     "shell.execute_reply": "2026-02-15T02:00:53.870107Z",
     "shell.execute_reply.started": "2026-02-15T02:00:46.516745Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Huber baseline — Val 2020 MAE: 15.2613\n",
      "Huber baseline — Test 2021 MAE: 15.2127\n",
      "Pred range (Test): [-15.6361, 31.3571]\n"
     ]
    }
   ],
   "source": [
    "huber_params = governance_cfg[\"baseline_protocols\"][protocol_key][\"model\"][\"params\"]\n",
    "huber = HuberRegressor(**huber_params)\n",
    "\n",
    "huber.fit(\n",
    "    X_train.to_numpy(dtype=float),\n",
    "    y_train.to_numpy(dtype=float),\n",
    "    sample_weight=sample_weight_train,\n",
    ")\n",
    "\n",
    "y_pred_val = huber.predict(X_val.to_numpy(dtype=float))\n",
    "y_pred_test = huber.predict(X_test.to_numpy(dtype=float))\n",
    "\n",
    "mae_val = evaluate_mae(y_val.to_numpy(dtype=float), y_pred_val)\n",
    "mae_test = evaluate_mae(y_test.to_numpy(dtype=float), y_pred_test)\n",
    "\n",
    "print(\"Huber baseline — Val 2020 MAE:\", f\"{mae_val:.4f}\")\n",
    "print(\"Huber baseline — Test 2021 MAE:\", f\"{mae_test:.4f}\")\n",
    "print(\n",
    "    \"Pred range (Test):\",\n",
    "    f\"[{float(np.min(y_pred_test)):.4f}, {float(np.max(y_pred_test)):.4f}]\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2573bd73-6426-4693-b648-8a3304f20cf1",
   "metadata": {},
   "source": [
    "# 4. The Challenger — Robust XGBoost (same frozen protocol as the baseline) \n",
    "\n",
    "With the baseline fully reconciled under the frozen Cycle 3 protocol (**Baseline_Huber15_recency0p05_medfill**: 15 numeric columns + train‑only median imputation + train‑only recency weights), we now execute the strategic pivot of Cycle 3: \n",
    "\n",
    "> Combine the **non‑linearity** of gradient‑boosted trees with **robust loss functions**\n",
    "> to reduce sensitivity to outliers and improve generalization under the 2021 regime shift.\n",
    "\n",
    "This section is intentionally structured as: \n",
    "1. **protocol guardrails** (what must remain invariant),\n",
    "2. a **single evaluation harness** (to prevent accidental drift),\n",
    "3. **point‑runs** (fast signal),\n",
    "4. a **short interpretation** that determines the tuning direction.\n",
    "\n",
    "---\n",
    "\n",
    "## 4.1 - Common Training/Evaluation Harness + Protocol Guardrails\n",
    "\n",
    "Before training any XGBoost model, we define a small, reusable harness that:\n",
    "\n",
    "* takes a model instance,\n",
    "* fits it on `(X_train, y_train)` with `sample_weight_train`,\n",
    "* evaluates MAE on **Val 2020** and **Test 2021**,\n",
    "* and returns a compact result dict for logging.\n",
    "\n",
    "This avoids repeated code and prevents accidental deviations (e.g., forgetting weights, changing matrices, or mixing input spaces).\n",
    "\n",
    "To ensure a fair comparison, **every XGBoost candidate** evaluated through this harness must reuse *exactly* the same frozen components as the baseline:\n",
    "\n",
    "* **Decision split:**  \n",
    "  train ≤ 2019, val = 2020, test = 2021  \n",
    "  (with invalid/missing years assigned to **train**, as defined in the frozen protocol)\n",
    "\n",
    "* **Input space:**  \n",
    "  the frozen list of **15 raw numeric columns**\n",
    "\n",
    "* **Preprocessing:**  \n",
    "  `SimpleImputer(strategy=\"median\")` fit on **train only**, applied to val/test\n",
    "\n",
    "* **Training weights:**  \n",
    "  recency weights (λ = 0.05) applied **only** on train\n",
    "\n",
    "If any of these components change, we are no longer comparing models — **we are comparing protocols**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f55b0705-02e4-40d7-b568-5dec0d0e4d9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T15:10:22.536111Z",
     "iopub.status.busy": "2026-02-13T15:10:22.535753Z",
     "iopub.status.idle": "2026-02-13T15:10:28.411802Z",
     "shell.execute_reply": "2026-02-13T15:10:28.411208Z",
     "shell.execute_reply.started": "2026-02-13T15:10:22.536082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"model_name\": \"HuberRegressor\",\n",
      "    \"objective\": null,\n",
      "    \"used_sample_weight\": true,\n",
      "    \"mae_val_2020\": 15.261278957800723,\n",
      "    \"mae_test_2021\": 15.212667070481631,\n",
      "    \"pred_min_test\": -15.636086723517685,\n",
      "    \"pred_max_test\": 31.357097696169706,\n",
      "    \"params\": {\n",
      "        \"alpha\": 0.0001,\n",
      "        \"epsilon\": 1.35,\n",
      "        \"fit_intercept\": true,\n",
      "        \"max_iter\": 100,\n",
      "        \"tol\": 1e-05,\n",
      "        \"warm_start\": false\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "protocol_key = _extract_cycle3_protocol_key(governance_cfg)\n",
    "frozen_huber_params = governance_cfg[\"baseline_protocols\"][protocol_key][\"model\"][\"params\"]\n",
    "\n",
    "huber = HuberRegressor(**frozen_huber_params)\n",
    "\n",
    "res_huber = run_and_evaluate_model(\n",
    "    model=huber,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    sample_weight_train=sample_weight_train,\n",
    ")\n",
    "\n",
    "print(json.dumps(res_huber.to_dict(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c257773-ed25-4609-be47-133c5ca56b34",
   "metadata": {},
   "source": [
    "## 4.2 - Point-runs (fast signal): three objectives\n",
    "\n",
    "We start with three minimal XGBoost runs (“point-runs”) to obtain an initial signal under the frozen protocol.\n",
    "At this stage we are not trying to win — we are trying to understand how each loss behaves in this dataset.\n",
    "\n",
    "### 4.2.1 Experiment 1 (control / naive): `objective=\"reg:squarederror\"`\n",
    "\n",
    "We begin with the default squared-error objective as a control condition.  \n",
    "Because squared error is sensitive to outliers and heavy-tailed residuals, we expect it to underperform in this setting.  \n",
    "Still, it provides a useful baseline against which more robust objectives can be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "6587eb9b-2b70-4d99-bb1d-c675630fb0e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T01:47:05.958450Z",
     "iopub.status.busy": "2026-02-15T01:47:05.957522Z",
     "iopub.status.idle": "2026-02-15T01:48:14.329285Z",
     "shell.execute_reply": "2026-02-15T01:48:14.327560Z",
     "shell.execute_reply.started": "2026-02-15T01:47:05.958383Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>objective</th>\n",
       "      <th>used_sample_weight</th>\n",
       "      <th>mae_val_2020</th>\n",
       "      <th>mae_test_2021</th>\n",
       "      <th>pred_min_test</th>\n",
       "      <th>pred_max_test</th>\n",
       "      <th>enable_categorical</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>random_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>True</td>\n",
       "      <td>14.4042</td>\n",
       "      <td>15.5807</td>\n",
       "      <td>-14.4951</td>\n",
       "      <td>64.6988</td>\n",
       "      <td>False</td>\n",
       "      <td>500</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     model_name         objective  used_sample_weight  mae_val_2020  mae_test_2021  pred_min_test  pred_max_test  \\\n",
       "0  XGBRegressor  reg:squarederror                True       14.4042        15.5807       -14.4951        64.6988   \n",
       "\n",
       "   enable_categorical  n_estimators  random_state  \n",
       "0               False           500            42  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_sq = XGBRegressor(objective=\"reg:squarederror\", random_state=RANDOM_SEED, n_estimators=500) \n",
    "res_sq = run_and_evaluate_model( \n",
    "    model=xgb_sq,\n",
    "    X_train=X_train, \n",
    "    y_train=y_train, \n",
    "    X_val=X_val, \n",
    "    y_val=y_val, \n",
    "    X_test=X_test, \n",
    "    y_test=y_test, \n",
    "    sample_weight_train=sample_weight_train, \n",
    ")\n",
    "res_sq = res_sq.to_dict()\n",
    "flat = {\n",
    "    **{k: v for k, v in res_sq.items() if k != \"params\"},\n",
    "    **{k: v for k, v in res_sq[\"params\"].items() if pd.notna(v)}\n",
    "}\n",
    "pd.DataFrame([flat])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fae6aa8-270f-4854-a493-9fc8d6a96a6f",
   "metadata": {},
   "source": [
    "### 4.2.2 Experiment 2 (robust): `objective=\"reg:absoluteerror\"`\n",
    "\n",
    "We then switch to MAE-based boosting, which is inherently more robust to outliers and heavy-tailed residuals.  \n",
    "Given this robustness, we expect improved performance relative to the squared-error control, particularly on the Test 2021 MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "89a4d702-b53e-475b-ba5f-1a96dee8d6b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T02:09:25.308489Z",
     "iopub.status.busy": "2026-02-15T02:09:25.307966Z",
     "iopub.status.idle": "2026-02-15T02:10:25.760456Z",
     "shell.execute_reply": "2026-02-15T02:10:25.759367Z",
     "shell.execute_reply.started": "2026-02-15T02:09:25.308446Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>objective</th>\n",
       "      <th>used_sample_weight</th>\n",
       "      <th>mae_val_2020</th>\n",
       "      <th>mae_test_2021</th>\n",
       "      <th>pred_min_test</th>\n",
       "      <th>pred_max_test</th>\n",
       "      <th>enable_categorical</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>random_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:absoluteerror</td>\n",
       "      <td>True</td>\n",
       "      <td>14.2693</td>\n",
       "      <td>15.7063</td>\n",
       "      <td>-13.5263</td>\n",
       "      <td>63.2540</td>\n",
       "      <td>False</td>\n",
       "      <td>500</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     model_name          objective  used_sample_weight  mae_val_2020  mae_test_2021  pred_min_test  pred_max_test  \\\n",
       "0  XGBRegressor  reg:absoluteerror                True       14.2693        15.7063       -13.5263        63.2540   \n",
       "\n",
       "   enable_categorical  n_estimators  random_state  \n",
       "0               False           500            42  "
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_abs = XGBRegressor(objective=\"reg:absoluteerror\", random_state=RANDOM_SEED, n_estimators=500) \n",
    "res_abs = run_and_evaluate_model( \n",
    "    model=xgb_abs,\n",
    "    X_train=X_train, \n",
    "    y_train=y_train, \n",
    "    X_val=X_val, \n",
    "    y_val=y_val, \n",
    "    X_test=X_test, \n",
    "    y_test=y_test, \n",
    "    sample_weight_train=sample_weight_train, \n",
    ")\n",
    "res_abs = res_abs.to_dict()\n",
    "flat = {\n",
    "    **{k: v for k, v in res_abs.items() if k != \"params\"},\n",
    "    **{k: v for k, v in res_abs[\"params\"].items() if pd.notna(v)}\n",
    "}\n",
    "pd.DataFrame([flat])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77f6d2e-758a-4343-9f5b-be32190e3bfa",
   "metadata": {},
   "source": [
    "### 4.2.3 - Experiment 3 (robust): `objective=\"reg:pseudohubererror\"`\n",
    "\n",
    "Finally, we test the pseudo-Huber objective, which behaves like L2 near zero and transitions toward L1 for large residuals.  \n",
    "This hybrid structure offers a compromise between stability and robustness, potentially bridging the performance gap between squared-error and absolute-error losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8f6648cd-3754-46a5-9c78-1d6f1b96bcb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T18:04:34.596599Z",
     "iopub.status.busy": "2026-02-13T18:04:34.596453Z",
     "iopub.status.idle": "2026-02-13T18:04:36.876066Z",
     "shell.execute_reply": "2026-02-13T18:04:36.875090Z",
     "shell.execute_reply.started": "2026-02-13T18:04:34.596587Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>objective</th>\n",
       "      <th>used_sample_weight</th>\n",
       "      <th>mae_val_2020</th>\n",
       "      <th>mae_test_2021</th>\n",
       "      <th>pred_min_test</th>\n",
       "      <th>pred_max_test</th>\n",
       "      <th>enable_categorical</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>random_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:pseudohubererror</td>\n",
       "      <td>True</td>\n",
       "      <td>15.0095</td>\n",
       "      <td>15.8873</td>\n",
       "      <td>-295.9467</td>\n",
       "      <td>260.7536</td>\n",
       "      <td>False</td>\n",
       "      <td>500</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     model_name             objective  used_sample_weight  mae_val_2020  mae_test_2021  pred_min_test  pred_max_test  \\\n",
       "0  XGBRegressor  reg:pseudohubererror                True       15.0095        15.8873      -295.9467       260.7536   \n",
       "\n",
       "   enable_categorical  n_estimators  random_state  \n",
       "0               False           500            42  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_ph = XGBRegressor(objective=\"reg:pseudohubererror\", random_state=RANDOM_SEED, n_estimators=500) \n",
    "res_ph = run_and_evaluate_model( \n",
    "    model=xgb_ph,\n",
    "    X_train=X_train, \n",
    "    y_train=y_train, \n",
    "    X_val=X_val, \n",
    "    y_val=y_val, \n",
    "    X_test=X_test, \n",
    "    y_test=y_test, \n",
    "    sample_weight_train=sample_weight_train, \n",
    ")\n",
    "res_ph = res_ph.to_dict()\n",
    "flat = {\n",
    "    **{k: v for k, v in res_ph.items() if k != \"params\"},\n",
    "    **{k: v for k, v in res_ph[\"params\"].items() if pd.notna(v)}\n",
    "}\n",
    "pd.DataFrame([flat])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ab3831-6415-48d8-87f8-97f7a7f0741b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.3 - Interpretation of point-runs\n",
    "\n",
    "We compare these results to the frozen baseline:\n",
    "\n",
    "* **Baseline (Huber-15 protocol):** Test 2021 MAE = **15.2127**\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "1. **None of the point-runs surpasses the baseline on Test 2021.**  \n",
    "   Although all objectives achieve stronger Val 2020 MAE than the baseline, this advantage does not translate to 2021.  \n",
    "   The squared-error run (15.58), absolute-error run (15.71), and pseudo-Huber run (15.89) all underperform relative to 15.21.\n",
    "\n",
    "2. **Val improves while Test degrades**, reinforcing the presence of **temporal/regime generalization difficulty**.  \n",
    "   This aligns with the known 2021 distribution shift (increased zero-inflation and heavier tails), which penalizes objectives that overfit to 2020 structure.\n",
    "\n",
    "3. **Pseudo-Huber is unstable under default settings.**  \n",
    "   Its prediction range is extreme (≈ −296 to +261), far beyond the other objectives (≈ −14 to +65).  \n",
    "   This indicates insufficient regularization or an unsuitable parameterization for this dataset; it should not be used without additional constraints.\n",
    "\n",
    "Operational conclusion:  \n",
    "We proceed to a controlled tuning phase focused on **generalization under the frozen protocol**, using MAE-driven selection and applying stronger regularization to stabilize robust objectives.\n",
    "\n",
    "---\n",
    "\n",
    "## 4.4 - Tuning with `RandomizedSearchCV` under a Pure Holdout Protocol (MAE)\n",
    "\n",
    "The point-runs established that default XGBoost configurations do not surpass the frozen Huber-15 baseline on **Test 2021**.  \n",
    "To investigate whether improved regularization can reduce the generalization gap, we now perform a controlled hyperparameter search.\n",
    "\n",
    "Crucially, we adopt **Pure holdout**:  \n",
    "**Train (≤2019)** is used for fitting all candidates, **Val 2020** is used exclusively for model selection, and **Test 2021** remains a strictly untouched holdout for final evaluation.  \n",
    "This preserves the temporal structure of the problem and prevents any feedback loop with the test year.\n",
    "\n",
    "### Guardrails (unchanged)\n",
    "\n",
    "* **Temporal split:** train ≤ 2019, val = 2020, test = 2021  \n",
    "* **Input space:** 15 numeric columns (Huber-15 protocol)  \n",
    "* **Imputation:** median, fit on train-only  \n",
    "* **Weights:** recency weights (λ = 0.05), applied to train-only  \n",
    "* **Selection metric:** `neg_mean_absolute_error` (i.e., minimize MAE on Val 2020)\n",
    "\n",
    "### Tuning strategy (and rationale)\n",
    "\n",
    "We tune only the hyperparameters that directly control model capacity and regularization:\n",
    "\n",
    "* `learning_rate`  \n",
    "* `max_depth`  \n",
    "* `subsample`  \n",
    "* `colsample_bytree`\n",
    "\n",
    "To maintain the integrity of the temporal split, we avoid internal cross-validation folds.  \n",
    "Instead, we construct a **single predefined split** (train vs. val) and run:\n",
    "\n",
    "* `RandomizedSearchCV(n_iter=20, scoring=\"neg_mean_absolute_error\", refit=False)`  \n",
    "* with a **PredefinedSplit** ensuring that Val 2020 is the sole selection reference.\n",
    "\n",
    "This design prevents year mixing, avoids diluting the drift signal, and keeps Test 2021 fully isolated.\n",
    "\n",
    "### Final model \n",
    "\n",
    "After identifying the best hyperparameters, we **refit the final model on Train (≤2019) only**, not on train+val.  \n",
    "Val 2020 may be reported as a post-hoc sanity check, but it does not participate in training the final estimator.\n",
    "\n",
    "### Expected outputs\n",
    "\n",
    "* Best hyperparameter configuration (`best_params_`)  \n",
    "* Val 2020 MAE of the selected configuration (`best_score_`)  \n",
    "* Final refit on Train (≤2019)  \n",
    "* Evaluation on Test 2021 (MAE + prediction range)  \n",
    "* Comparison against the baseline: success if `MAE_test_2021 < 15.2127`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "020bd1f3-c8e8-4509-84ad-92f49066d956",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T18:30:02.550767Z",
     "iopub.status.busy": "2026-02-13T18:30:02.550352Z",
     "iopub.status.idle": "2026-02-13T18:33:24.633115Z",
     "shell.execute_reply": "2026-02-13T18:33:24.628827Z",
     "shell.execute_reply.started": "2026-02-13T18:30:02.550737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 20 candidates, totalling 20 fits\n",
      "Tuning objective: reg:squarederror\n",
      "Best params: {'subsample': np.float64(0.6653061224489796), 'max_depth': np.int64(10), 'learning_rate': np.float64(0.025510204081632654), 'colsample_bytree': np.float64(0.6489795918367347)}\n",
      "Best holdout (Val 2020) MAE: 14.1789\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model_name</th>\n",
       "      <td>XGBRegressor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>objective</th>\n",
       "      <td>reg:squarederror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>used_sample_weight</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mae_val_2020</th>\n",
       "      <td>14.1789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mae_test_2021</th>\n",
       "      <td>15.4564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_min_test</th>\n",
       "      <td>-3.2927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_max_test</th>\n",
       "      <td>54.7270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag</th>\n",
       "      <td>xgb_tuned_reg:squarederror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best_val_mae_from_search</th>\n",
       "      <td>14.1789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>colsample_bytree</th>\n",
       "      <td>0.6490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enable_categorical</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning_rate</th>\n",
       "      <td>0.0255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_depth</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_estimators</th>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_jobs</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random_state</th>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subsample</th>\n",
       "      <td>0.6653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tree_method</th>\n",
       "      <td>hist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "model_name                              XGBRegressor\n",
       "objective                           reg:squarederror\n",
       "used_sample_weight                              True\n",
       "mae_val_2020                                 14.1789\n",
       "mae_test_2021                                15.4564\n",
       "pred_min_test                                -3.2927\n",
       "pred_max_test                                54.7270\n",
       "tag                       xgb_tuned_reg:squarederror\n",
       "best_val_mae_from_search                     14.1789\n",
       "colsample_bytree                              0.6490\n",
       "enable_categorical                             False\n",
       "learning_rate                                 0.0255\n",
       "max_depth                                         10\n",
       "n_estimators                                     800\n",
       "n_jobs                                            -1\n",
       "random_state                                      42\n",
       "subsample                                     0.6653\n",
       "tree_method                                     hist"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Tuning - squarederror\n",
    "res_tuned_sq = tune_xgb_with_predefinedsplit_holdout(\n",
    "    objective=\"reg:squarederror\",\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    w_train=sample_weight_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    n_iter=20,\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n",
    "flat = {\n",
    "    **{k: v for k, v in res_tuned_sq.best_model_eval.items()\n",
    "       if k not in (\"best_params\", \"params\")},\n",
    "    **{k: v for k, v in res_tuned_sq.best_model_eval[\"params\"].items()\n",
    "       if pd.notna(v)}\n",
    "}\n",
    "pd.DataFrame([flat]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0785c19b-8607-4211-a138-2ec0627197e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T18:34:57.486241Z",
     "iopub.status.busy": "2026-02-13T18:34:57.484568Z",
     "iopub.status.idle": "2026-02-13T18:39:14.430207Z",
     "shell.execute_reply": "2026-02-13T18:39:14.428640Z",
     "shell.execute_reply.started": "2026-02-13T18:34:57.486171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 20 candidates, totalling 20 fits\n",
      "Tuning objective: reg:absoluteerror\n",
      "Best params: {'subsample': np.float64(0.6653061224489796), 'max_depth': np.int64(10), 'learning_rate': np.float64(0.025510204081632654), 'colsample_bytree': np.float64(0.6489795918367347)}\n",
      "Best holdout (Val 2020) MAE: 14.1279\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model_name</th>\n",
       "      <td>XGBRegressor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>objective</th>\n",
       "      <td>reg:absoluteerror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>used_sample_weight</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mae_val_2020</th>\n",
       "      <td>14.1279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mae_test_2021</th>\n",
       "      <td>15.5150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_min_test</th>\n",
       "      <td>-3.2848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_max_test</th>\n",
       "      <td>55.2573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag</th>\n",
       "      <td>xgb_tuned_reg:absoluteerror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best_val_mae_from_search</th>\n",
       "      <td>14.1279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>colsample_bytree</th>\n",
       "      <td>0.6490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enable_categorical</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning_rate</th>\n",
       "      <td>0.0255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_depth</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_estimators</th>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_jobs</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random_state</th>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subsample</th>\n",
       "      <td>0.6653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tree_method</th>\n",
       "      <td>hist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0\n",
       "model_name                               XGBRegressor\n",
       "objective                           reg:absoluteerror\n",
       "used_sample_weight                               True\n",
       "mae_val_2020                                  14.1279\n",
       "mae_test_2021                                 15.5150\n",
       "pred_min_test                                 -3.2848\n",
       "pred_max_test                                 55.2573\n",
       "tag                       xgb_tuned_reg:absoluteerror\n",
       "best_val_mae_from_search                      14.1279\n",
       "colsample_bytree                               0.6490\n",
       "enable_categorical                              False\n",
       "learning_rate                                  0.0255\n",
       "max_depth                                          10\n",
       "n_estimators                                      800\n",
       "n_jobs                                             -1\n",
       "random_state                                       42\n",
       "subsample                                      0.6653\n",
       "tree_method                                      hist"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) Tuning - absoluteerror\n",
    "res_tuned_abs = tune_xgb_with_predefinedsplit_holdout(\n",
    "    objective=\"reg:absoluteerror\",\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    w_train=sample_weight_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    ")\n",
    "flat = {\n",
    "    **{k: v for k, v in res_tuned_abs.best_model_eval.items()\n",
    "       if k not in (\"best_params\", \"params\")},\n",
    "    **{k: v for k, v in res_tuned_abs.best_model_eval[\"params\"].items()\n",
    "       if pd.notna(v)}\n",
    "}\n",
    "pd.DataFrame([flat]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "94b83849-e0ea-45f0-af63-872b0832c4e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T18:53:53.190234Z",
     "iopub.status.busy": "2026-02-13T18:53:53.189896Z",
     "iopub.status.idle": "2026-02-13T18:57:01.290380Z",
     "shell.execute_reply": "2026-02-13T18:57:01.288351Z",
     "shell.execute_reply.started": "2026-02-13T18:53:53.190207Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 20 candidates, totalling 20 fits\n",
      "Tuning objective: reg:pseudohubererror\n",
      "Best params: {'subsample': np.float64(0.6653061224489796), 'max_depth': np.int64(10), 'learning_rate': np.float64(0.025510204081632654), 'colsample_bytree': np.float64(0.6489795918367347)}\n",
      "Best holdout (Val 2020) MAE: 14.1467\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model_name</th>\n",
       "      <td>XGBRegressor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>objective</th>\n",
       "      <td>reg:pseudohubererror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>used_sample_weight</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mae_val_2020</th>\n",
       "      <td>14.1467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mae_test_2021</th>\n",
       "      <td>15.3044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_min_test</th>\n",
       "      <td>-3.2895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_max_test</th>\n",
       "      <td>58.3352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag</th>\n",
       "      <td>xgb_tuned_reg:pseudohubererror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best_val_mae_from_search</th>\n",
       "      <td>14.1467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>colsample_bytree</th>\n",
       "      <td>0.6490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enable_categorical</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning_rate</th>\n",
       "      <td>0.0255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_depth</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_estimators</th>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_jobs</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random_state</th>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subsample</th>\n",
       "      <td>0.6653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tree_method</th>\n",
       "      <td>hist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0\n",
       "model_name                                  XGBRegressor\n",
       "objective                           reg:pseudohubererror\n",
       "used_sample_weight                                  True\n",
       "mae_val_2020                                     14.1467\n",
       "mae_test_2021                                    15.3044\n",
       "pred_min_test                                    -3.2895\n",
       "pred_max_test                                    58.3352\n",
       "tag                       xgb_tuned_reg:pseudohubererror\n",
       "best_val_mae_from_search                         14.1467\n",
       "colsample_bytree                                  0.6490\n",
       "enable_categorical                                 False\n",
       "learning_rate                                     0.0255\n",
       "max_depth                                             10\n",
       "n_estimators                                         800\n",
       "n_jobs                                                -1\n",
       "random_state                                          42\n",
       "subsample                                         0.6653\n",
       "tree_method                                         hist"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) Tuning - pseudohubererror\n",
    "res_tuned_ph  = tune_xgb_with_predefinedsplit_holdout(\n",
    "    objective=\"reg:pseudohubererror\",\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    w_train=sample_weight_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    ")\n",
    "flat = {\n",
    "    **{k: v for k, v in res_tuned_ph.best_model_eval.items()\n",
    "       if k not in (\"best_params\", \"params\")},\n",
    "    **{k: v for k, v in res_tuned_ph.best_model_eval[\"params\"].items()\n",
    "       if pd.notna(v)}\n",
    "}\n",
    "pd.DataFrame([flat]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416c023a-19e7-4655-9955-e97c2e44c43d",
   "metadata": {},
   "source": [
    "## 4.5 — Final comparison (Val 2020 vs Test 2021) and Cycle 3 decision\n",
    "\n",
    "We now consolidate all candidates trained under the exact same frozen protocol\n",
    "(**Huber-15 numeric-only + train-only median imputation + train-only recency weights**).\n",
    "\n",
    "**Selection rule (methodological):**\n",
    "- **Val 2020** is used for tuning/selection (no peeking into Test).\n",
    "- **Test 2021** is the final holdout used only once to decide whether the challenger truly beats the baseline.\n",
    "\n",
    "**Cycle 3 success criterion:**\n",
    "The best challenger must achieve **MAE(Test 2021) < 15.2127** (Huber-15 baseline).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "8bbdfa65-03c0-44b2-b1f0-087000b23ba9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T03:46:07.678528Z",
     "iopub.status.busy": "2026-02-15T03:46:07.677788Z",
     "iopub.status.idle": "2026-02-15T03:46:07.713676Z",
     "shell.execute_reply": "2026-02-15T03:46:07.712660Z",
     "shell.execute_reply.started": "2026-02-15T03:46:07.678474Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>objective</th>\n",
       "      <th>tag</th>\n",
       "      <th>used_sample_weight</th>\n",
       "      <th>mae_val_2020</th>\n",
       "      <th>mae_test_2021</th>\n",
       "      <th>pred_min_test</th>\n",
       "      <th>pred_max_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>None</td>\n",
       "      <td>baseline_huber15</td>\n",
       "      <td>True</td>\n",
       "      <td>15.2613</td>\n",
       "      <td>15.2127</td>\n",
       "      <td>-15.6361</td>\n",
       "      <td>31.3571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:pseudohubererror</td>\n",
       "      <td>xgb_tuned_reg:pseudohubererror</td>\n",
       "      <td>True</td>\n",
       "      <td>14.1467</td>\n",
       "      <td>15.3044</td>\n",
       "      <td>-3.2895</td>\n",
       "      <td>58.3352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>xgb_tuned_reg:squarederror</td>\n",
       "      <td>True</td>\n",
       "      <td>14.1789</td>\n",
       "      <td>15.4564</td>\n",
       "      <td>-3.2927</td>\n",
       "      <td>54.7270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:absoluteerror</td>\n",
       "      <td>xgb_tuned_reg:absoluteerror</td>\n",
       "      <td>True</td>\n",
       "      <td>14.1279</td>\n",
       "      <td>15.5150</td>\n",
       "      <td>-3.2848</td>\n",
       "      <td>55.2573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>xgb_point_reg:squarederror</td>\n",
       "      <td>True</td>\n",
       "      <td>14.4042</td>\n",
       "      <td>15.5807</td>\n",
       "      <td>-14.4951</td>\n",
       "      <td>64.6988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:absoluteerror</td>\n",
       "      <td>xgb_point_reg:absoluteerror</td>\n",
       "      <td>True</td>\n",
       "      <td>14.2693</td>\n",
       "      <td>15.7063</td>\n",
       "      <td>-13.5263</td>\n",
       "      <td>63.2540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:pseudohubererror</td>\n",
       "      <td>xgb_point_reg:pseudohubererror</td>\n",
       "      <td>True</td>\n",
       "      <td>15.0095</td>\n",
       "      <td>15.8873</td>\n",
       "      <td>-295.9467</td>\n",
       "      <td>260.7536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model_name             objective                             tag  used_sample_weight  mae_val_2020  \\\n",
       "0  HuberRegressor                  None                baseline_huber15                True       15.2613   \n",
       "1    XGBRegressor  reg:pseudohubererror  xgb_tuned_reg:pseudohubererror                True       14.1467   \n",
       "2    XGBRegressor      reg:squarederror      xgb_tuned_reg:squarederror                True       14.1789   \n",
       "3    XGBRegressor     reg:absoluteerror     xgb_tuned_reg:absoluteerror                True       14.1279   \n",
       "4    XGBRegressor      reg:squarederror      xgb_point_reg:squarederror                True       14.4042   \n",
       "5    XGBRegressor     reg:absoluteerror     xgb_point_reg:absoluteerror                True       14.2693   \n",
       "6    XGBRegressor  reg:pseudohubererror  xgb_point_reg:pseudohubererror                True       15.0095   \n",
       "\n",
       "   mae_test_2021  pred_min_test  pred_max_test  \n",
       "0        15.2127       -15.6361        31.3571  \n",
       "1        15.3044        -3.2895        58.3352  \n",
       "2        15.4564        -3.2927        54.7270  \n",
       "3        15.5150        -3.2848        55.2573  \n",
       "4        15.5807       -14.4951        64.6988  \n",
       "5        15.7063       -13.5263        63.2540  \n",
       "6        15.8873      -295.9467       260.7536  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (Huber-15) MAE Test 2021: 15.2127\n",
      "Best candidate MAE Test 2021:      15.3044\n",
      "Success criterion met: False (baseline NOT beaten)\n",
      "Closest candidate was: xgb_tuned_reg:pseudohubererror with MAE=15.3044 (gap=0.0917)\n"
     ]
    }
   ],
   "source": [
    "rows = [\n",
    "    dict(res_huber.to_dict(), tag=\"baseline_huber15\"),\n",
    "    dict(res_sq,  tag=\"xgb_point_reg:squarederror\"),\n",
    "    dict(res_abs, tag=\"xgb_point_reg:absoluteerror\"),\n",
    "    dict(res_ph,  tag=\"xgb_point_reg:pseudohubererror\"),\n",
    "    dict(res_tuned_sq.best_model_eval,  tag=\"xgb_tuned_reg:squarederror\"),\n",
    "    dict(res_tuned_abs.best_model_eval, tag=\"xgb_tuned_reg:absoluteerror\"),\n",
    "    dict(res_tuned_ph.best_model_eval,  tag=\"xgb_tuned_reg:pseudohubererror\"),\n",
    "]\n",
    "\n",
    "df_results = pd.DataFrame(rows)\n",
    "\n",
    "cols_order = [\n",
    "    \"model_name\", \"objective\", \"tag\", \"used_sample_weight\",\n",
    "    \"mae_val_2020\", \"mae_test_2021\",\n",
    "    \"pred_min_test\", \"pred_max_test\",\n",
    "]\n",
    "df_results = df_results[[c for c in cols_order if c in df_results.columns]]\n",
    "df_results = df_results.sort_values(\"mae_test_2021\").reset_index(drop=True)\n",
    "\n",
    "display(df_results)\n",
    "\n",
    "# --- Baseline / candidates split (anchor by tag) ---\n",
    "df_base = df_results[df_results[\"tag\"].eq(\"baseline_huber15\")]\n",
    "if df_base.shape[0] != 1:\n",
    "    raise ValueError(f\"Expected exactly 1 baseline row. Found {df_base.shape[0]}.\")\n",
    "\n",
    "baseline_mae = float(df_base[\"mae_test_2021\"].iloc[0])\n",
    "\n",
    "df_cand = df_results[~df_results[\"tag\"].eq(\"baseline_huber15\")].copy()\n",
    "df_cand[\"gap_to_baseline\"] = df_cand[\"mae_test_2021\"] - baseline_mae\n",
    "\n",
    "best_cand_row = df_cand.loc[df_cand[\"mae_test_2021\"].idxmin()]\n",
    "best_cand_mae = float(best_cand_row[\"mae_test_2021\"])\n",
    "\n",
    "print(f\"Baseline (Huber-15) MAE Test 2021: {baseline_mae:.4f}\")\n",
    "print(f\"Best candidate MAE Test 2021:      {best_cand_mae:.4f}\")\n",
    "\n",
    "if best_cand_mae < baseline_mae:\n",
    "    print(\"Success criterion met: True (baseline beaten)\")\n",
    "else:\n",
    "    closest_row = df_cand.iloc[df_cand[\"gap_to_baseline\"].abs().argmin()]\n",
    "    print(\"Success criterion met: False (baseline NOT beaten)\")\n",
    "    print(\n",
    "        f\"Closest candidate was: {closest_row['tag']} \"\n",
    "        f\"with MAE={closest_row['mae_test_2021']:.4f} \"\n",
    "        f\"(gap={closest_row['gap_to_baseline']:.4f})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7094e36-932f-4da6-b499-a5463f28feb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T19:42:20.787770Z",
     "iopub.status.busy": "2026-02-13T19:42:20.787434Z",
     "iopub.status.idle": "2026-02-13T19:42:20.796749Z",
     "shell.execute_reply": "2026-02-13T19:42:20.794697Z",
     "shell.execute_reply.started": "2026-02-13T19:42:20.787743Z"
    }
   },
   "source": [
    "### Decision\n",
    "\n",
    "Under the frozen Cycle 3 protocol, none of the XGBoost candidates (including tuned robust objectives)\n",
    "improved upon the **Huber-15 baseline** on the **Test 2021** holdout.\n",
    "\n",
    "Therefore, the Cycle 3 champion remains:\n",
    "\n",
    "- **Baseline_Huber15_recency0p05_medfill** (MAE Test 2021 = 15.2127)\n",
    "\n",
    "The XGBoost models are kept as documented experiments, but they are **not** promoted as the Cycle 3 best model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c43205-11c0-4a6a-bf8c-8c852ef86b24",
   "metadata": {},
   "source": [
    "## 4.6 - Final Robustness Sweep: Confirming the Baseline’s Dominance\n",
    "\n",
    "This section documents the full experimental trajectory that led to the final model selection.  \n",
    "Each step includes:\n",
    "\n",
    "- the **code used**,  \n",
    "- the **motivation behind the experiment**,  \n",
    "- and the **result that justified moving to the next stage**.\n",
    "\n",
    "This creates a transparent and reproducible narrative showing why the Huber‑15 baseline ultimately remained the strongest model under the 2020→2021 drift.\n",
    "\n",
    "---\n",
    "\n",
    "### **4.6.1 Establishing the Baseline (Huber‑15)**\n",
    "\n",
    "We begin with a robust linear baseline using the Huber loss with parameter (ε = 15).\n",
    "\n",
    "This model is intentionally simple, convex, and resistant to outliers — a natural anchor for evaluating drift robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "33b5a23a-0eea-4bad-956e-85982664e0f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T03:02:54.240862Z",
     "iopub.status.busy": "2026-02-15T03:02:54.240323Z",
     "iopub.status.idle": "2026-02-15T03:02:54.289083Z",
     "shell.execute_reply": "2026-02-15T03:02:54.284530Z",
     "shell.execute_reply.started": "2026-02-15T03:02:54.240820Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE Test 2021 (no clip): 15.2127\n",
      "MAE Test 2021 (clip 0-100): 15.2000\n",
      "Delta (clip - no clip): -0.0127\n"
     ]
    }
   ],
   "source": [
    "y_pred_test_clip = np.clip(y_pred_test, 0.0, 100.0)\n",
    "\n",
    "mae_test_no_clip = mean_absolute_error(y_test.to_numpy(float), y_pred_test)\n",
    "mae_test_clip = mean_absolute_error(y_test.to_numpy(float), y_pred_test_clip)\n",
    "\n",
    "print(f\"MAE Test 2021 (no clip): {mae_test_no_clip:.4f}\")\n",
    "print(f\"MAE Test 2021 (clip 0-100): {mae_test_clip:.4f}\")\n",
    "print(f\"Delta (clip - no clip): {mae_test_clip - mae_test_no_clip:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "7736f1f6-b585-449e-8b73-6ca3150b7a8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T11:15:20.140549Z",
     "iopub.status.busy": "2026-02-15T11:15:20.140049Z",
     "iopub.status.idle": "2026-02-15T11:15:20.157706Z",
     "shell.execute_reply": "2026-02-15T11:15:20.151364Z",
     "shell.execute_reply.started": "2026-02-15T11:15:20.140491Z"
    }
   },
   "outputs": [],
   "source": [
    "row_huber_clip = {\n",
    "    \"model_name\": \"HuberRegressor\",\n",
    "    \"objective\": None,\n",
    "    \"tag\": \"baseline_huber15_clip\",\n",
    "    \"used_sample_weight\": True,\n",
    "    \"mae_val_2020\": res_huber.to_dict().get(\"mae_val_2020\"),\n",
    "    \"mae_test_2021\": mae_test_clip,\n",
    "    \"pred_min_test\": float(y_pred_test_clip.min()),\n",
    "    \"pred_max_test\": float(y_pred_test_clip.max()),\n",
    "}\n",
    "df_results = pd.concat([df_results, pd.DataFrame([row_huber_clip])], ignore_index=True)\n",
    "df_results = df_results.drop_duplicates(subset=[\"tag\"], keep=\"first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00305b4-4125-4a4c-bdcb-ce4a3b3ba0a0",
   "metadata": {},
   "source": [
    "This value becomes the **target to beat** for all subsequent models.\n",
    "\n",
    "---\n",
    "\n",
    "### **4.6.2 Exploring Alternative Model Families**\n",
    "\n",
    "Before moving to boosted trees, we evaluated whether other model classes could naturally outperform the baseline under drift.\n",
    "\n",
    "#### **1. Hurdle Model (Two‑Stage Zero Inflation)**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "c51d5f0b-2a09-4f13-9ea3-f2a0b3b460fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T03:09:57.244887Z",
     "iopub.status.busy": "2026-02-15T03:09:57.244516Z",
     "iopub.status.idle": "2026-02-15T03:10:05.518355Z",
     "shell.execute_reply": "2026-02-15T03:10:05.513649Z",
     "shell.execute_reply.started": "2026-02-15T03:09:57.244865Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/.cache/pypoetry/virtualenvs/popforecast-dRaaJELz-py3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 200 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=200).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hurdle(min) MAE Test 2021: 17.6846\n",
      "True zero%: 23.87 | Pred zero%: 34.33 | thr=0.5\n"
     ]
    }
   ],
   "source": [
    "thr = 0.5\n",
    "\n",
    "# ---- Stage 1: classifier (train) ----\n",
    "y_train_bin = (y_train.to_numpy(float) > 0).astype(int)\n",
    "y_val_bin   = (y_val.to_numpy(float) > 0).astype(int)\n",
    "y_test_bin  = (y_test.to_numpy(float) > 0).astype(int)\n",
    "\n",
    "clf = LogisticRegression(max_iter=200, class_weight=\"balanced\", n_jobs=-1)\n",
    "clf.fit(X_train.to_numpy(float), y_train_bin, sample_weight=sample_weight_train)\n",
    "\n",
    "p_test_pos = clf.predict_proba(X_test.to_numpy(float))[:, 1]\n",
    "\n",
    "# ---- Stage 2: regressor on positives only (train) ----\n",
    "pos_mask_train = y_train.to_numpy(float) > 0\n",
    "reg = HuberRegressor(**huber_params)\n",
    "reg.fit(\n",
    "    X_train.loc[pos_mask_train].to_numpy(float),\n",
    "    y_train.loc[pos_mask_train].to_numpy(float),\n",
    "    sample_weight=sample_weight_train[pos_mask_train],\n",
    ")\n",
    "\n",
    "y_pred_test_reg = reg.predict(X_test.to_numpy(float))\n",
    "\n",
    "# ---- Combine ----\n",
    "y_pred_test_hurdle = np.where(p_test_pos >= thr, y_pred_test_reg, 0.0)\n",
    "y_pred_test_hurdle = np.clip(y_pred_test_hurdle, 0.0, 100.0)\n",
    "\n",
    "mae_test_hurdle = mean_absolute_error(y_test.to_numpy(float), y_pred_test_hurdle)\n",
    "\n",
    "pred_zero_pct = float((y_pred_test_hurdle == 0.0).mean() * 100)\n",
    "true_zero_pct = float((y_test.to_numpy(float) == 0.0).mean() * 100)\n",
    "\n",
    "print(f\"Hurdle(min) MAE Test 2021: {mae_test_hurdle:.4f}\")\n",
    "print(f\"True zero%: {true_zero_pct:.2f} | Pred zero%: {pred_zero_pct:.2f} | thr={thr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "df122ec4-69d6-4556-9f80-e50147dc4d2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T11:15:36.877994Z",
     "iopub.status.busy": "2026-02-15T11:15:36.871341Z",
     "iopub.status.idle": "2026-02-15T11:15:36.920314Z",
     "shell.execute_reply": "2026-02-15T11:15:36.914032Z",
     "shell.execute_reply.started": "2026-02-15T11:15:36.877704Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Criar linha para o hurdle ---\n",
    "row_hurdle = {\n",
    "    \"model_name\": \"Hurdle(LogReg + Huber)\",\n",
    "    \"objective\": \"hurdle\",\n",
    "    \"tag\": \"hurdle_logreg_huber_clip\",\n",
    "    \"used_sample_weight\": True,\n",
    "    \"mae_val_2020\": res_huber.to_dict().get(\"mae_val_2020\"),\n",
    "    \"mae_test_2021\": mae_test_hurdle,\n",
    "    \"pred_min_test\": float(y_pred_test_hurdle.min()),\n",
    "    \"pred_max_test\": float(y_pred_test_hurdle.max()),\n",
    "}\n",
    "\n",
    "# --- Adicionar ao df_results ---\n",
    "df_results = pd.concat([df_results, pd.DataFrame([row_hurdle])], ignore_index=True)\n",
    "\n",
    "# --- Remover duplicadas por tag ---\n",
    "df_results = df_results.drop_duplicates(subset=[\"tag\"], keep=\"first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f08d0e-2632-472c-b30b-9064b762f6f6",
   "metadata": {},
   "source": [
    "The model over‑predicts zeros and performs substantially worse than the baseline.\n",
    "\n",
    "#### **2. Tweedie Regressors (GLM Family)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e429e796-c5da-405d-94fd-caa755ebcb64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T20:46:59.258712Z",
     "iopub.status.busy": "2026-02-13T20:46:59.258196Z",
     "iopub.status.idle": "2026-02-13T20:47:06.065086Z",
     "shell.execute_reply": "2026-02-13T20:47:06.064483Z",
     "shell.execute_reply.started": "2026-02-13T20:46:59.258678Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>power</th>\n",
       "      <th>mae_val_2020_clip</th>\n",
       "      <th>mae_test_2021_clip</th>\n",
       "      <th>pred_min_test</th>\n",
       "      <th>pred_max_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TweedieRegressor</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>15.2135</td>\n",
       "      <td>15.5129</td>\n",
       "      <td>3.7809</td>\n",
       "      <td>41.5472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TweedieRegressor</td>\n",
       "      <td>1.2000</td>\n",
       "      <td>15.2108</td>\n",
       "      <td>15.5369</td>\n",
       "      <td>4.2808</td>\n",
       "      <td>41.7994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TweedieRegressor</td>\n",
       "      <td>1.5000</td>\n",
       "      <td>15.2073</td>\n",
       "      <td>15.5736</td>\n",
       "      <td>5.1983</td>\n",
       "      <td>42.2427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TweedieRegressor</td>\n",
       "      <td>1.8000</td>\n",
       "      <td>15.2044</td>\n",
       "      <td>15.6097</td>\n",
       "      <td>6.1842</td>\n",
       "      <td>42.7838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model  power  mae_val_2020_clip  mae_test_2021_clip  pred_min_test  pred_max_test\n",
       "0  TweedieRegressor 1.0000            15.2135             15.5129         3.7809        41.5472\n",
       "1  TweedieRegressor 1.2000            15.2108             15.5369         4.2808        41.7994\n",
       "2  TweedieRegressor 1.5000            15.2073             15.5736         5.1983        42.2427\n",
       "3  TweedieRegressor 1.8000            15.2044             15.6097         6.1842        42.7838"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1) Scale (fit on train only)\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train.to_numpy(float))\n",
    "X_val_s = scaler.transform(X_val.to_numpy(float))\n",
    "X_test_s = scaler.transform(X_test.to_numpy(float))\n",
    "\n",
    "# 2) Try a small sweep on power (keep it minimal)\n",
    "powers = [1.0, 1.2, 1.5, 1.8]   # minimal, interpretable sweep\n",
    "rows = []\n",
    "\n",
    "for p in powers:\n",
    "    model = TweedieRegressor(\n",
    "        power=p,\n",
    "        alpha=0.0,          # start simple; add regularization only if it converges\n",
    "        link=\"log\",\n",
    "        max_iter=2000,\n",
    "        tol=1e-6,\n",
    "    )\n",
    "\n",
    "    # TweedieRegressor supports sample_weight\n",
    "    model.fit(X_train_s, y_train.to_numpy(float), sample_weight=sample_weight_train)\n",
    "\n",
    "    pred_val = model.predict(X_val_s)\n",
    "    pred_test = model.predict(X_test_s)\n",
    "\n",
    "    # Clip to domain (0–100) to match your “defensible” reporting choice\n",
    "    pred_val_c = np.clip(pred_val, 0.0, 100.0)\n",
    "    pred_test_c = np.clip(pred_test, 0.0, 100.0)\n",
    "\n",
    "    mae_val = float(mean_absolute_error(y_val.to_numpy(float), pred_val_c))\n",
    "    mae_test = float(mean_absolute_error(y_test.to_numpy(float), pred_test_c))\n",
    "\n",
    "    rows.append(\n",
    "        {\n",
    "            \"model\": \"TweedieRegressor\",\n",
    "            \"power\": p,\n",
    "            \"mae_val_2020_clip\": mae_val,\n",
    "            \"mae_test_2021_clip\": mae_test,\n",
    "            \"pred_min_test\": float(np.min(pred_test)),\n",
    "            \"pred_max_test\": float(np.max(pred_test)),\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_tweedie = pd.DataFrame(rows).sort_values(\"mae_test_2021_clip\").reset_index(drop=True)\n",
    "display(df_tweedie)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7789b006-7483-4fa7-ae96-55c9b1e76f04",
   "metadata": {},
   "source": [
    "None approached the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "51d78a0d-9cc3-41e1-a280-29297ca5e35d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T11:15:45.904063Z",
     "iopub.status.busy": "2026-02-15T11:15:45.897291Z",
     "iopub.status.idle": "2026-02-15T11:15:45.956166Z",
     "shell.execute_reply": "2026-02-15T11:15:45.949795Z",
     "shell.execute_reply.started": "2026-02-15T11:15:45.903942Z"
    }
   },
   "outputs": [],
   "source": [
    "rows_tweedie_for_df = []\n",
    "\n",
    "for _, r in df_tweedie.iterrows():\n",
    "    p = r[\"power\"]\n",
    "    rows_tweedie_for_df.append({\n",
    "        \"model_name\": \"TweedieRegressor\",\n",
    "        \"objective\": f\"tweedie_power_{p}\",\n",
    "        \"tag\": f\"tweedie_power_{p}\",\n",
    "        \"used_sample_weight\": True,\n",
    "        \"mae_val_2020\": r[\"mae_val_2020_clip\"],\n",
    "        \"mae_test_2021\": r[\"mae_test_2021_clip\"],\n",
    "        \"pred_min_test\": r[\"pred_min_test\"],\n",
    "        \"pred_max_test\": r[\"pred_max_test\"],\n",
    "    })\n",
    "\n",
    "df_results = pd.concat([df_results, pd.DataFrame(rows_tweedie_for_df)], ignore_index=True)\n",
    "\n",
    "# garantir que não duplica\n",
    "df_results = df_results.drop_duplicates(subset=[\"tag\"], keep=\"first\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c793e381-68e0-4887-98d4-6759144d4aef",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **4.6.3 First Wave of XGBoost Experiments (Point‑Runs)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "57508eb9-2c91-4186-8edc-a08b1655801c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T04:29:58.138101Z",
     "iopub.status.busy": "2026-02-15T04:29:58.137083Z",
     "iopub.status.idle": "2026-02-15T04:30:55.948943Z",
     "shell.execute_reply": "2026-02-15T04:30:55.947617Z",
     "shell.execute_reply.started": "2026-02-15T04:29:58.138033Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>objective</th>\n",
       "      <th>mae_val_2020_clip</th>\n",
       "      <th>mae_test_2021_clip</th>\n",
       "      <th>pred_min_test</th>\n",
       "      <th>pred_max_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xgb_tuned_pseudohuber</td>\n",
       "      <td>reg:pseudohubererror</td>\n",
       "      <td>14.1455</td>\n",
       "      <td>15.3036</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>58.3352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xgb_tuned_squarederror</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>14.1788</td>\n",
       "      <td>15.4563</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>54.7270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xgb_tuned_absoluteerror</td>\n",
       "      <td>reg:absoluteerror</td>\n",
       "      <td>14.1274</td>\n",
       "      <td>15.5146</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>55.2573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xgb_point_squarederror</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>14.3968</td>\n",
       "      <td>15.5734</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>64.6988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xgb_point_absoluteerror</td>\n",
       "      <td>reg:absoluteerror</td>\n",
       "      <td>14.2578</td>\n",
       "      <td>15.6953</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>63.2540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xgb_point_pseudohuber</td>\n",
       "      <td>reg:pseudohubererror</td>\n",
       "      <td>14.6141</td>\n",
       "      <td>15.8018</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       tag             objective  mae_val_2020_clip  mae_test_2021_clip  pred_min_test  pred_max_test\n",
       "0    xgb_tuned_pseudohuber  reg:pseudohubererror            14.1455             15.3036         0.0000        58.3352\n",
       "1   xgb_tuned_squarederror      reg:squarederror            14.1788             15.4563         0.0000        54.7270\n",
       "2  xgb_tuned_absoluteerror     reg:absoluteerror            14.1274             15.5146         0.0000        55.2573\n",
       "3   xgb_point_squarederror      reg:squarederror            14.3968             15.5734         0.0000        64.6988\n",
       "4  xgb_point_absoluteerror     reg:absoluteerror            14.2578             15.6953         0.0000        63.2540\n",
       "5    xgb_point_pseudohuber  reg:pseudohubererror            14.6141             15.8018         0.0000       100.0000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = {\n",
    "    \"xgb_point_squarederror\": xgb_sq,\n",
    "    \"xgb_point_absoluteerror\": xgb_abs,\n",
    "    \"xgb_point_pseudohuber\": xgb_ph,\n",
    "}\n",
    "\n",
    "# reconstrói tuned models a partir de best_params\n",
    "models[\"xgb_tuned_squarederror\"] = XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    n_estimators=800,\n",
    "    tree_method=\"hist\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    **res_tuned_sq.best_params,\n",
    ")\n",
    "models[\"xgb_tuned_absoluteerror\"] = XGBRegressor(\n",
    "    objective=\"reg:absoluteerror\",\n",
    "    n_estimators=800,\n",
    "    tree_method=\"hist\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    **res_tuned_abs.best_params,\n",
    ")\n",
    "models[\"xgb_tuned_pseudohuber\"] = XGBRegressor(\n",
    "    objective=\"reg:pseudohubererror\",\n",
    "    n_estimators=800,\n",
    "    tree_method=\"hist\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    **res_tuned_ph.best_params,\n",
    ")\n",
    "\n",
    "# importante: fazer fit antes de predict\n",
    "for tag, model in models.items():\n",
    "    if tag.startswith(\"xgb_tuned_\"):\n",
    "        model.fit(\n",
    "            X_train.to_numpy(float),\n",
    "            y_train.to_numpy(float),\n",
    "            sample_weight=sample_weight_train,\n",
    "        )\n",
    "\n",
    "rows = []\n",
    "for tag, model in models.items():\n",
    "    pred_val = model.predict(X_val.to_numpy(float))\n",
    "    pred_test = model.predict(X_test.to_numpy(float))\n",
    "    pred_val_c = np.clip(pred_val, 0.0, 100.0)\n",
    "    pred_test_c = np.clip(pred_test, 0.0, 100.0)\n",
    "\n",
    "    rows.append({\n",
    "        \"tag\": tag,\n",
    "        \"objective\": getattr(model, \"objective\", None),\n",
    "        \"mae_val_2020_clip\": eval_with_clip_0_100(y_val.to_numpy(float), pred_val),\n",
    "        \"mae_test_2021_clip\": eval_with_clip_0_100(y_test.to_numpy(float), pred_test),\n",
    "        \"pred_min_test\": float(pred_test_c.min()),   # <-- CORRIGIDO\n",
    "        \"pred_max_test\": float(pred_test_c.max()),   # <-- CORRIGIDO\n",
    "    })\n",
    "\n",
    "df_xgb_clip = pd.DataFrame(rows).sort_values(\"mae_test_2021_clip\").reset_index(drop=True)\n",
    "display(df_xgb_clip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "b048bd8e-0177-45af-a3d1-ab8a1d0b2950",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T11:16:38.873055Z",
     "iopub.status.busy": "2026-02-15T11:16:38.872612Z",
     "iopub.status.idle": "2026-02-15T11:16:38.946676Z",
     "shell.execute_reply": "2026-02-15T11:16:38.941941Z",
     "shell.execute_reply.started": "2026-02-15T11:16:38.873019Z"
    }
   },
   "outputs": [],
   "source": [
    "df_xgb_clip_for_merge = df_xgb_clip.rename(columns={\n",
    "    \"mae_val_2020_clip\": \"mae_val_2020\",\n",
    "    \"mae_test_2021_clip\": \"mae_test_2021\",\n",
    "})\n",
    "\n",
    "df_xgb_clip_for_merge[\"model_name\"] = \"XGBRegressor\"\n",
    "df_xgb_clip_for_merge[\"used_sample_weight\"] = True\n",
    "df_xgb_clip_for_merge[\"tag\"] = df_xgb_clip_for_merge[\"tag\"] + \"_clip\"\n",
    "cols = [\n",
    "    \"model_name\",\n",
    "    \"objective\",\n",
    "    \"tag\",\n",
    "    \"used_sample_weight\",\n",
    "    \"mae_val_2020\",\n",
    "    \"mae_test_2021\",\n",
    "    \"pred_min_test\",\n",
    "    \"pred_max_test\",\n",
    "]\n",
    "\n",
    "df_xgb_clip_for_merge = df_xgb_clip_for_merge[cols]\n",
    "df_results = pd.concat([df_results, df_xgb_clip_for_merge], ignore_index=True)\n",
    "df_results = df_results.sort_values(\"mae_test_2021\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a8f54c-4104-456d-ba20-afd65bb7ac7e",
   "metadata": {},
   "source": [
    "We next evaluated XGBoost with fixed hyperparameters (“point‑runs”) to test whether non‑linear models could naturally outperform the baseline. These models were stable but clearly inferior to the baseline.\n",
    "\n",
    "We then performed a RandomizedSearchCV over a moderate hyperparameter space. For the first time, a model approached the baseline — but still did not surpass it.\n",
    "\n",
    "---\n",
    "\n",
    "### **4.6.4 Expanded Hyperparameter Search (Aggressive Tuning)**\n",
    "\n",
    "To ensure that the search space was not limiting performance, we expanded the tuning space substantially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c7bad01a-f6a5-4a47-a86f-8eaec2ae672e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T21:03:13.459188Z",
     "iopub.status.busy": "2026-02-13T21:03:13.458633Z",
     "iopub.status.idle": "2026-02-13T21:27:41.559353Z",
     "shell.execute_reply": "2026-02-13T21:27:41.557904Z",
     "shell.execute_reply.started": "2026-02-13T21:03:13.459144Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>objective</th>\n",
       "      <th>best_val_mae_from_search</th>\n",
       "      <th>mae_val_2020_clip</th>\n",
       "      <th>mae_test_2021_clip</th>\n",
       "      <th>pred_min_test_clip</th>\n",
       "      <th>pred_max_test_clip</th>\n",
       "      <th>subsample</th>\n",
       "      <th>reg_lambda</th>\n",
       "      <th>reg_alpha</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>gamma</th>\n",
       "      <th>colsample_bytree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xgb_tuned_expanded_reg:absoluteerror</td>\n",
       "      <td>reg:absoluteerror</td>\n",
       "      <td>14.1302</td>\n",
       "      <td>14.1280</td>\n",
       "      <td>15.2541</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>55.0586</td>\n",
       "      <td>0.8667</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0450</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xgb_tuned_expanded_reg:pseudohubererror</td>\n",
       "      <td>reg:pseudohubererror</td>\n",
       "      <td>14.1440</td>\n",
       "      <td>14.1431</td>\n",
       "      <td>15.3699</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>54.9421</td>\n",
       "      <td>0.6833</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0325</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.7833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xgb_tuned_expanded_reg:squarederror</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>14.1689</td>\n",
       "      <td>14.1685</td>\n",
       "      <td>15.4683</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>52.6271</td>\n",
       "      <td>0.6833</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0325</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.7833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       tag             objective  best_val_mae_from_search  mae_val_2020_clip  \\\n",
       "0     xgb_tuned_expanded_reg:absoluteerror     reg:absoluteerror                   14.1302            14.1280   \n",
       "1  xgb_tuned_expanded_reg:pseudohubererror  reg:pseudohubererror                   14.1440            14.1431   \n",
       "2      xgb_tuned_expanded_reg:squarederror      reg:squarederror                   14.1689            14.1685   \n",
       "\n",
       "   mae_test_2021_clip  pred_min_test_clip  pred_max_test_clip  subsample  reg_lambda  reg_alpha  min_child_weight  \\\n",
       "0             15.2541              0.0000             55.0586     0.8667      1.0000     0.1000                20   \n",
       "1             15.3699              0.0000             54.9421     0.6833      5.0000     0.1000                 2   \n",
       "2             15.4683              0.0000             52.6271     0.6833      5.0000     0.1000                 2   \n",
       "\n",
       "   max_depth  learning_rate  gamma  colsample_bytree  \n",
       "0         10         0.0450 1.0000            0.8833  \n",
       "1          8         0.0325 0.5000            0.7833  \n",
       "2          8         0.0325 0.5000            0.7833  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Expanded XGB tuning space (still: PredefinedSplit holdout train/val; MAE; same protocol) ---\n",
    "\n",
    "# --- How to use (repeat for the objectives you care about) ---\n",
    "# Assumes you already have: X_train, y_train, X_val, y_val, X_test, y_test, sample_weight_train\n",
    "# from the Baseline_Huber15_recency0p05_medfill protocol.\n",
    "\n",
    "objectives_to_try = [\n",
    "    \"reg:squarederror\",\n",
    "    \"reg:absoluteerror\",\n",
    "    \"reg:pseudohubererror\",\n",
    "]\n",
    "\n",
    "rows = []\n",
    "\n",
    "for obj in objectives_to_try:\n",
    "    res = tune_xgb_with_predefinedsplit_holdout(\n",
    "        objective=obj,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        w_train=sample_weight_train,\n",
    "        X_val=X_val,\n",
    "        y_val=y_val,\n",
    "        n_iter=30,          # bump to 30 since we expanded the space\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    # Evaluate best model on val/test with clip 0-100 (same as your recent reporting)\n",
    "    Xva = X_val.to_numpy(dtype=float)\n",
    "    Xte = X_test.to_numpy(dtype=float)\n",
    "    yva = y_val.to_numpy(dtype=float)\n",
    "    yte = y_test.to_numpy(dtype=float)\n",
    "\n",
    "    pred_val = res.best_model.predict(Xva)\n",
    "    pred_test = res.best_model.predict(Xte)\n",
    "\n",
    "    mae_val_clip, _, _, _ = evaluate_with_clip_0_100(yva, pred_val)\n",
    "    mae_test_clip, pmin, pmax, _ = evaluate_with_clip_0_100(yte, pred_test)\n",
    "\n",
    "    rows.append(\n",
    "        {\n",
    "            \"tag\": f\"xgb_tuned_expanded_{obj}\",\n",
    "            \"objective\": obj,\n",
    "            \"best_val_mae_from_search\": res.best_val_mae,   # raw (no clip) val MAE used for selection\n",
    "            \"mae_val_2020_clip\": mae_val_clip,\n",
    "            \"mae_test_2021_clip\": mae_test_clip,\n",
    "            \"pred_min_test_clip\": pmin,\n",
    "            \"pred_max_test_clip\": pmax,\n",
    "            **res.best_params,\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_xgb_tuned_expanded = pd.DataFrame(rows).sort_values(\"mae_test_2021_clip\").reset_index(drop=True)\n",
    "display(df_xgb_tuned_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53f6501-bde1-47ea-964e-0dd20c916965",
   "metadata": {},
   "source": [
    "**Representative result:**\n",
    "\n",
    "```\n",
    "Best params: {subsample=0.8667, reg_lambda=1.0, reg_alpha=0.1,\n",
    "              min_child_weight=20, max_depth=10, learning_rate=0.045,\n",
    "              gamma=1.0, colsample_bytree=0.8833}\n",
    "\n",
    "Val 2020 MAE (clip): 14.1280\n",
    "Test 2021 MAE (clip): 15.2541\n",
    "```\n",
    "\n",
    "This was the **closest any model came** to beating the baseline — only **0.04** above it.\n",
    "\n",
    "But still: **not better**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "8ee1b679-209f-46cd-838b-1f5127a4d7d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T11:16:54.995981Z",
     "iopub.status.busy": "2026-02-15T11:16:54.994859Z",
     "iopub.status.idle": "2026-02-15T11:16:55.039342Z",
     "shell.execute_reply": "2026-02-15T11:16:55.035151Z",
     "shell.execute_reply.started": "2026-02-15T11:16:54.995901Z"
    }
   },
   "outputs": [],
   "source": [
    "df_expanded = df_xgb_tuned_expanded.rename(columns={\n",
    "    \"mae_val_2020_clip\": \"mae_val_2020\",\n",
    "    \"mae_test_2021_clip\": \"mae_test_2021\",\n",
    "    \"pred_min_test_clip\": \"pred_min_test\",\n",
    "    \"pred_max_test_clip\": \"pred_max_test\",\n",
    "})\n",
    "df_expanded[\"model_name\"] = \"XGBRegressor\"\n",
    "df_expanded[\"used_sample_weight\"] = True\n",
    "df_expanded[\"tag\"] = df_expanded[\"tag\"].astype(str)\n",
    "cols = [\n",
    "    \"model_name\",\n",
    "    \"objective\",\n",
    "    \"tag\",\n",
    "    \"used_sample_weight\",\n",
    "    \"mae_val_2020\",\n",
    "    \"mae_test_2021\",\n",
    "    \"pred_min_test\",\n",
    "    \"pred_max_test\",\n",
    "]\n",
    "\n",
    "df_expanded = df_expanded[cols]\n",
    "df_results = pd.concat([df_results, df_expanded], ignore_index=True)\n",
    "df_results = df_results.sort_values(\"mae_test_2021\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c734e0b3-62b4-4564-93c7-0462f634bf73",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **4.6.5 Additional Tuning Attempts (n_iter = 30)**\n",
    "\n",
    "To ensure that the expanded search space had been sufficiently explored, we conducted a series of additional tuning attempts using **30 random samples** from a broader hyperparameter distribution. These experiments included:\n",
    "\n",
    "- a full 30‑iteration search using the predefined split (val + test),  \n",
    "- a separate 30‑iteration manual search evaluated **only on Val 2020**,  \n",
    "- several refits using the best configurations found,  \n",
    "- and targeted variations (simplified models, deeper models, and early‑stopping refits).\n",
    "\n",
    "Across all attempts, **none of the models surpassed the Huber‑15 baseline**.\n",
    "\n",
    "\n",
    "#### **1. Full 100‑iteration expanded search (val + test)**  \n",
    "**Objective:** `reg:absoluteerror`  \n",
    "**n_iter:** 30  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "7019a40b-9844-43d4-97fe-97b63d4959e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T05:51:25.461511Z",
     "iopub.status.busy": "2026-02-15T05:51:25.461188Z",
     "iopub.status.idle": "2026-02-15T05:57:54.658400Z",
     "shell.execute_reply": "2026-02-15T05:57:54.654836Z",
     "shell.execute_reply.started": "2026-02-15T05:51:25.461485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params (abs, expanded, n_iter=100): {'subsample': np.float64(0.9), 'reg_lambda': 5.0, 'reg_alpha': 0.001, 'min_child_weight': 5, 'max_depth': np.int64(9), 'learning_rate': np.float64(0.028888888888888888), 'gamma': 1.0, 'colsample_bytree': np.float64(0.7)}\n",
      "Val 2020 MAE (clip): 14.1227\n",
      "Test 2021 MAE (clip): 15.3649\n",
      "Pred range test (clip): [0.0000, 54.3557]\n"
     ]
    }
   ],
   "source": [
    "res_abs_expanded_100 = tune_xgb_with_predefinedsplit_holdout(\n",
    "    objective=\"reg:absoluteerror\",\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    w_train=sample_weight_train,\n",
    "    X_val=X_val, y_val=y_val,\n",
    "    n_iter=30,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "pred_val = res_abs_expanded_100.best_model.predict(X_val.to_numpy(float))\n",
    "pred_test = res_abs_expanded_100.best_model.predict(X_test.to_numpy(float))\n",
    "\n",
    "mae_val_clip, _, _, _ = evaluate_with_clip_0_100(y_val.to_numpy(float), pred_val)\n",
    "mae_test_clip, pmin, pmax, _ = evaluate_with_clip_0_100(y_test.to_numpy(float), pred_test)\n",
    "\n",
    "print(\"Best params (abs, expanded, n_iter=100):\", res_abs_expanded_100.best_params)\n",
    "print(\"Val 2020 MAE (clip):\", f\"{mae_val_clip:.4f}\")\n",
    "print(\"Test 2021 MAE (clip):\", f\"{mae_test_clip:.4f}\")\n",
    "print(\"Pred range test (clip):\", f\"[{pmin:.4f}, {pmax:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "b5212421-f8dd-43fd-91ed-eaaebf8924ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T11:17:07.565744Z",
     "iopub.status.busy": "2026-02-15T11:17:07.565226Z",
     "iopub.status.idle": "2026-02-15T11:17:07.576171Z",
     "shell.execute_reply": "2026-02-15T11:17:07.574635Z",
     "shell.execute_reply.started": "2026-02-15T11:17:07.565710Z"
    }
   },
   "outputs": [],
   "source": [
    "row_abs_expanded_30 = {\n",
    "    \"model_name\": \"XGBRegressor\",\n",
    "    \"objective\": \"reg:absoluteerror\",\n",
    "    \"tag\": \"xgb_tuned_expanded_abs_n30_clip\",\n",
    "    \"used_sample_weight\": True,\n",
    "    \"mae_val_2020\": mae_val_clip,\n",
    "    \"mae_test_2021\": mae_test_clip,\n",
    "    \"pred_min_test\": float(pmin),\n",
    "    \"pred_max_test\": float(pmax),\n",
    "}\n",
    "df_results = pd.concat([df_results, pd.DataFrame([row_abs_expanded_30])], ignore_index=True)\n",
    "df_results = df_results.sort_values(\"mae_test_2021\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172bca8d-6b35-4178-81a9-d9e094bb182a",
   "metadata": {},
   "source": [
    "This configuration did **not** improve upon the best result from the earlier expanded search (15.2541), and remained above the baseline (15.2000).\n",
    "\n",
    "#### **2. Manual 30‑iteration search (validation‑only)**  \n",
    "\n",
    "To complement the predefined‑split search, we also ran a manual 30‑iteration exploration over a wider hyperparameter space, evaluating each configuration only on the 2020 validation set. This experiment was not intended to produce a final model, but rather to probe whether the expanded space contained regions with systematically lower validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "0075c8b9-373b-4254-8f69-71debe0ecf8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T06:52:07.221461Z",
     "iopub.status.busy": "2026-02-15T06:52:07.220353Z",
     "iopub.status.idle": "2026-02-15T07:17:30.625744Z",
     "shell.execute_reply": "2026-02-15T07:17:30.624717Z",
     "shell.execute_reply.started": "2026-02-15T06:52:07.221391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best (by Val 2020 MAE with clip):\n",
      "  val_mae_clip: 14.1195\n",
      "  params: {'subsample': np.float64(0.8333333333333333), 'reg_lambda': 0.5, 'reg_alpha': 0.01, 'min_child_weight': 10, 'max_depth': 10, 'learning_rate': np.float64(0.03), 'gamma': 0.25, 'colsample_bytree': np.float64(0.9333333333333333)}\n",
      "  pred range val (clip): [0.0000, 62.7752]\n"
     ]
    }
   ],
   "source": [
    "# Assumes you already have:\n",
    "# - X_train, y_train, sample_weight_train\n",
    "# - X_val, y_val\n",
    "# - evaluate_with_clip_0_100(y_true_np, y_pred_np)\n",
    "\n",
    "# Convert once (faster)\n",
    "Xtr = _to_float_np(X_train)\n",
    "ytr = _to_float_np(y_train)\n",
    "wtr = _to_float_np(sample_weight_train)\n",
    "\n",
    "Xva = _to_float_np(X_val)\n",
    "yva = _to_float_np(y_val)\n",
    "\n",
    "objective = \"reg:absoluteerror\"\n",
    "random_state = 42\n",
    "n_iter = 30   \n",
    "\n",
    "base_params = dict(\n",
    "    objective=objective,\n",
    "    n_estimators=2000,          # large upper bound; early stopping will cut it\n",
    "    tree_method=\"hist\",\n",
    "    random_state=random_state,\n",
    ")\n",
    "\n",
    "param_dist = {\n",
    "    \"learning_rate\": np.linspace(0.02, 0.08, 13),\n",
    "    \"max_depth\": list(range(3, 11)),\n",
    "    \"subsample\": np.linspace(0.6, 1.0, 13),\n",
    "    \"colsample_bytree\": np.linspace(0.6, 1.0, 13),\n",
    "    \"min_child_weight\": [1, 2, 5, 10, 20, 30, 40],\n",
    "    \"gamma\": [0.0, 0.25, 0.5, 1.0, 2.0],\n",
    "    \"reg_alpha\": [0.0, 0.01, 0.1, 0.5, 1.0],\n",
    "    \"reg_lambda\": [0.5, 1.0, 2.0, 5.0],\n",
    "}\n",
    "\n",
    "sampler = ParameterSampler(param_dist, n_iter=n_iter, random_state=random_state)\n",
    "\n",
    "best = {\n",
    "    \"val_mae_clip\": np.inf,\n",
    "    \"params\": None,\n",
    "    \"model\": None,\n",
    "    \"pred_min_val_clip\": None,\n",
    "    \"pred_max_val_clip\": None,\n",
    "}\n",
    "\n",
    "for i, params in enumerate(sampler, start=1):\n",
    "    model = XGBRegressor(\n",
    "        **base_params,\n",
    "        **params,\n",
    "        early_stopping_rounds=50,   # <-- optimization that preserves intent\n",
    "        eval_metric=\"mae\",\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        Xtr, ytr,\n",
    "        sample_weight=wtr,\n",
    "        eval_set=[(Xva, yva)],\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    pred_val = model.predict(Xva)\n",
    "    mae_val_clip, pmin, pmax, _ = evaluate_with_clip_0_100(yva, pred_val)\n",
    "\n",
    "    if mae_val_clip < best[\"val_mae_clip\"]:\n",
    "        best.update(\n",
    "            val_mae_clip=float(mae_val_clip),\n",
    "            params=params,\n",
    "            model=model,\n",
    "            pred_min_val_clip=float(pmin),\n",
    "            pred_max_val_clip=float(pmax),\n",
    "        )\n",
    "\n",
    "print(\"Best (by Val 2020 MAE with clip):\")\n",
    "print(\"  val_mae_clip:\", f\"{best['val_mae_clip']:.4f}\")\n",
    "print(\"  params:\", best[\"params\"])\n",
    "print(\"  pred range val (clip):\", f\"[{best['pred_min_val_clip']:.4f}, {best['pred_max_val_clip']:.4f}]\")\n",
    "\n",
    "best_model = best[\"model\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d70ce8-c4f1-4822-9086-2cfeda9229a4",
   "metadata": {},
   "source": [
    "This was the lowest validation score observed across all searches. However, because this run did not include evaluation on Test 2021, it serves only as an exploratory signal rather than a candidate model. Subsequent refits using these parameters (reported below) confirmed that the improvement did not generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "94cdfdd7-a685-4e28-a55e-d50f4730cc63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T11:17:17.705095Z",
     "iopub.status.busy": "2026-02-15T11:17:17.704095Z",
     "iopub.status.idle": "2026-02-15T11:17:17.764427Z",
     "shell.execute_reply": "2026-02-15T11:17:17.761236Z",
     "shell.execute_reply.started": "2026-02-15T11:17:17.705021Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1138/1227314983.py:11: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_results = pd.concat([df_results, pd.DataFrame([new_row])], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "new_row = {\n",
    "    \"model_name\": \"XGBRegressor\",\n",
    "    \"objective\": \"reg:absoluteerror\",\n",
    "    \"tag\": \"xgb_manual_expanded_abs_n30_clip\",   \n",
    "    \"used_sample_weight\": True,\n",
    "    \"mae_val_2020\": 14.1195,\n",
    "    \"mae_test_2021\": None,         \n",
    "    \"pred_min_test\": None,       \n",
    "    \"pred_max_test\": None,          \n",
    "}\n",
    "df_results = pd.concat([df_results, pd.DataFrame([new_row])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b179f9e0-70cb-4abb-89ee-bd766b58cf88",
   "metadata": {},
   "source": [
    "#### **3. Refit of the validation‑only best model (val + test)** \n",
    "\n",
    "To verify whether the promising validation‑only configuration from the manual 100‑iteration search would generalize beyond the 2020 validation set, we refit the model using the full training data and evaluated it on both Val 2020 and Test 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "aed01095-1442-4e56-9e3d-b8f11e2994e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T10:13:35.163482Z",
     "iopub.status.busy": "2026-02-15T10:13:35.158258Z",
     "iopub.status.idle": "2026-02-15T10:13:51.417868Z",
     "shell.execute_reply": "2026-02-15T10:13:51.416270Z",
     "shell.execute_reply.started": "2026-02-15T10:13:35.163404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val 2020 MAE (clip):  14.1831\n",
      "Test 2021 MAE (clip): 15.4340\n",
      "Pred range test (clip): [0.0000, 53.1278]\n",
      "Baseline Huber-15 MAE (clip): 15.2000\n",
      "Beats baseline on Test (clip)? False\n",
      "Gap to baseline (Test, clip): 0.23396949768066477\n"
     ]
    }
   ],
   "source": [
    "best_params = {\n",
    "    \"subsample\": 0.8333333333333333,\n",
    "    \"reg_lambda\": 0.5,\n",
    "    \"reg_alpha\": 0.01,\n",
    "    \"min_child_weight\": 10,\n",
    "    \"max_depth\": 10,\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"gamma\": 0.25,\n",
    "    \"colsample_bytree\": 0.9333333333333333,\n",
    "}\n",
    "\n",
    "Xtr = X_train.to_numpy(dtype=np.float32, copy=False)\n",
    "Xva = X_val.to_numpy(dtype=np.float32, copy=False)\n",
    "Xte = X_test.to_numpy(dtype=np.float32, copy=False)\n",
    "ytr = y_train.to_numpy(dtype=np.float32, copy=False)\n",
    "yva = y_val.to_numpy(dtype=np.float32, copy=False)\n",
    "yte = y_test.to_numpy(dtype=np.float32, copy=False)\n",
    "wtr = sample_weight_train.astype(np.float32, copy=False)\n",
    "\n",
    "def mae_clip(y_true, y_pred):\n",
    "    return mean_absolute_error(y_true, np.clip(y_pred, 0.0, 100.0))\n",
    "\n",
    "model = XGBRegressor(\n",
    "    objective=\"reg:absoluteerror\",\n",
    "    n_estimators=300,     \n",
    "    tree_method=\"hist\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    **best_params,\n",
    ")\n",
    "\n",
    "model.fit(Xtr, ytr, sample_weight=wtr)\n",
    "\n",
    "pred_val = model.predict(Xva)\n",
    "pred_test = model.predict(Xte)\n",
    "\n",
    "mae_val_clip = mae_clip(yva, pred_val)\n",
    "mae_test_clip = mae_clip(yte, pred_test)\n",
    "\n",
    "print(f\"Val 2020 MAE (clip):  {mae_val_clip:.4f}\")\n",
    "print(f\"Test 2021 MAE (clip): {mae_test_clip:.4f}\")\n",
    "print(f\"Pred range test (clip): [{np.clip(pred_test,0,100).min():.4f}, {np.clip(pred_test,0,100).max():.4f}]\")\n",
    "\n",
    "baseline_huber_clip = 15.2000  # seu valor já medido\n",
    "print(f\"Baseline Huber-15 MAE (clip): {baseline_huber_clip:.4f}\")\n",
    "print(\"Beats baseline on Test (clip)?\", mae_test_clip < baseline_huber_clip)\n",
    "print(\"Gap to baseline (Test, clip):\", mae_test_clip - baseline_huber_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc31aba-071b-49a9-96de-1c4f06910480",
   "metadata": {},
   "source": [
    "Although the validation‑only search produced one of the lowest Val 2020 MAE values observed (14.1195), the refit using those hyperparameters did **not** translate into improved generalization. The Test 2021 MAE increased to **15.4340**, remaining above the baseline Huber‑15 model (15.2000). This confirms that the validation‑only improvement was not stable and did not carry over to the held‑out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "fec1dda2-10e9-4260-909e-86a1de366231",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T11:17:26.066132Z",
     "iopub.status.busy": "2026-02-15T11:17:26.065660Z",
     "iopub.status.idle": "2026-02-15T11:17:26.093980Z",
     "shell.execute_reply": "2026-02-15T11:17:26.087113Z",
     "shell.execute_reply.started": "2026-02-15T11:17:26.066093Z"
    }
   },
   "outputs": [],
   "source": [
    "new_row = {\n",
    "    \"model_name\": \"XGBRegressor\",\n",
    "    \"objective\": \"reg:absoluteerror\",\n",
    "    \"tag\": \"xgb_manual_expanded_abs_n30_refit_clip\",  # ajuste o nome se quiser\n",
    "    \"used_sample_weight\": True,\n",
    "    \"mae_val_2020\": float(mae_val_clip),\n",
    "    \"mae_test_2021\": float(mae_test_clip),\n",
    "    \"pred_min_test\": float(np.clip(pred_test, 0, 100).min()),\n",
    "    \"pred_max_test\": float(np.clip(pred_test, 0, 100).max()),\n",
    "}\n",
    "df_results = pd.concat([df_results, pd.DataFrame([new_row])], ignore_index=True)\n",
    "df_results = df_results.sort_values(by=\"mae_val_2020\", ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aa7a58-f654-4ab7-bbf2-8d40894a9c2d",
   "metadata": {},
   "source": [
    "#### **4. High‑capacity refit with early stopping disabled**  \n",
    "A refit with **n_estimators = 5000** (no early stopping):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "1baee3a3-a6f4-4b74-bb56-f2962142215d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T10:27:38.078999Z",
     "iopub.status.busy": "2026-02-15T10:27:38.078351Z",
     "iopub.status.idle": "2026-02-15T10:32:12.863599Z",
     "shell.execute_reply": "2026-02-15T10:32:12.862479Z",
     "shell.execute_reply.started": "2026-02-15T10:27:38.078955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_iteration: None\n",
      "Test 2021 MAE (clip): 15.4248685836792\n"
     ]
    }
   ],
   "source": [
    "model_es = XGBRegressor(\n",
    "    objective=\"reg:absoluteerror\",\n",
    "    n_estimators=5000,          # alto, mas para cedo\n",
    "    tree_method=\"hist\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    **best_params,\n",
    ")\n",
    "\n",
    "model_es.fit(\n",
    "    X_train.to_numpy(np.float32, copy=False),\n",
    "    y_train.to_numpy(np.float32, copy=False),\n",
    "    sample_weight=sample_weight_train.astype(np.float32, copy=False),\n",
    "    eval_set=[(X_val.to_numpy(np.float32, copy=False), y_val.to_numpy(np.float32, copy=False))],\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "pred_test = model_es.predict(X_test.to_numpy(np.float32, copy=False))\n",
    "print(\"best_iteration:\", getattr(model_es, \"best_iteration\", None))\n",
    "print(\"Test 2021 MAE (clip):\", mae_clip(y_test.to_numpy(np.float32, copy=False), pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd20fb04-276e-4596-89e6-90b9344f3897",
   "metadata": {},
   "source": [
    "A high‑capacity refit with 5000 trees (no early stopping) yielded a Test 2021 MAE of 15.4249, essentially identical to the standard refit. This confirms that increasing model capacity does not recover the validation‑only improvement and does not improve generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "8dd7bfd3-0586-4573-b145-4ce9bc2d7e6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T11:17:48.060042Z",
     "iopub.status.busy": "2026-02-15T11:17:48.059790Z",
     "iopub.status.idle": "2026-02-15T11:17:48.075324Z",
     "shell.execute_reply": "2026-02-15T11:17:48.074267Z",
     "shell.execute_reply.started": "2026-02-15T11:17:48.060023Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1138/1690258031.py:11: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_results = pd.concat([df_results, pd.DataFrame([new_row])], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "new_row = {\n",
    "    \"model_name\": \"XGBRegressor\",\n",
    "    \"objective\": \"reg:absoluteerror\",\n",
    "    \"tag\": \"xgb_manual_expanded_abs_n30_refit_no_es_clip\",\n",
    "    \"used_sample_weight\": True,\n",
    "    \"mae_val_2020\": None,  # este experimento é test-only\n",
    "    \"mae_test_2021\": float(mae_clip(y_test.to_numpy(np.float32, copy=False), pred_test)),\n",
    "    \"pred_min_test\": float(np.clip(pred_test, 0, 100).min()),\n",
    "    \"pred_max_test\": float(np.clip(pred_test, 0, 100).max()),\n",
    "}\n",
    "df_results = pd.concat([df_results, pd.DataFrame([new_row])], ignore_index=True)\n",
    "df_results = df_results.sort_values(by=\"mae_val_2020\", ascending=True, na_position=\"last\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61504353-6929-4301-b9ae-c410704c1b4d",
   "metadata": {},
   "source": [
    "#### **5. Simplified model (reduced depth and larger min_child_weight)**  \n",
    "A deliberately simpler variant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "cbf8a16c-8341-4aeb-a6ae-0f0634e686cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T10:35:11.416846Z",
     "iopub.status.busy": "2026-02-15T10:35:11.416059Z",
     "iopub.status.idle": "2026-02-15T10:35:39.294238Z",
     "shell.execute_reply": "2026-02-15T10:35:39.292635Z",
     "shell.execute_reply.started": "2026-02-15T10:35:11.416783Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 2021 MAE (clip): 15.527663230895996\n"
     ]
    }
   ],
   "source": [
    "params_simpler = dict(best_params)\n",
    "params_simpler.update({\"max_depth\": 4, \"min_child_weight\": 50})\n",
    "\n",
    "model_simple = XGBRegressor(\n",
    "    objective=\"reg:absoluteerror\",\n",
    "    n_estimators=800,\n",
    "    tree_method=\"hist\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    **params_simpler,\n",
    ")\n",
    "\n",
    "model_simple.fit(\n",
    "    X_train.to_numpy(np.float32, copy=False),\n",
    "    y_train.to_numpy(np.float32, copy=False),\n",
    "    sample_weight=sample_weight_train.astype(np.float32, copy=False),\n",
    ")\n",
    "\n",
    "pred_test = model_simple.predict(X_test.to_numpy(np.float32, copy=False))\n",
    "print(\"Test 2021 MAE (clip):\", mae_clip(y_test.to_numpy(np.float32, copy=False), pred_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9243c99b-220d-4f07-a78c-b90f10ad8eb8",
   "metadata": {},
   "source": [
    "This confirmed that reducing model complexity also failed to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "d89bf809-b8b7-4010-a4ad-d53bc6050013",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T11:17:54.709442Z",
     "iopub.status.busy": "2026-02-15T11:17:54.708698Z",
     "iopub.status.idle": "2026-02-15T11:17:54.740860Z",
     "shell.execute_reply": "2026-02-15T11:17:54.738363Z",
     "shell.execute_reply.started": "2026-02-15T11:17:54.709377Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1138/2507125951.py:11: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_results = pd.concat([df_results, pd.DataFrame([new_row])], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "new_row = {\n",
    "    \"model_name\": \"XGBRegressor\",\n",
    "    \"objective\": \"reg:absoluteerror\",\n",
    "    \"tag\": \"xgb_manual_expanded_abs_n30_refit_simplified_clip\",\n",
    "    \"used_sample_weight\": True,\n",
    "    \"mae_val_2020\": None,  # este experimento é test-only\n",
    "    \"mae_test_2021\": float(mae_clip(y_test.to_numpy(np.float32, copy=False), pred_test)),\n",
    "    \"pred_min_test\": float(np.clip(pred_test, 0, 100).min()),\n",
    "    \"pred_max_test\": float(np.clip(pred_test, 0, 100).max()),\n",
    "}\n",
    "df_results = pd.concat([df_results, pd.DataFrame([new_row])], ignore_index=True)\n",
    "df_results = df_results.sort_values(\n",
    "    by=\"mae_val_2020\",\n",
    "    ascending=True,\n",
    "    na_position=\"last\"\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fc5f6f-81c7-4de3-9255-39a21f68c8b5",
   "metadata": {},
   "source": [
    "#### **6. High‑capacity refit with proper early stopping**  \n",
    "Same best_params, but now with:\n",
    "\n",
    "- `n_estimators=5000`  \n",
    "- `early_stopping_rounds=50`  \n",
    "- `eval_metric=\"mae\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "c67919e7-93a4-4144-a77d-1cd01bf947e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T10:41:54.695037Z",
     "iopub.status.busy": "2026-02-15T10:41:54.693345Z",
     "iopub.status.idle": "2026-02-15T10:43:01.288877Z",
     "shell.execute_reply": "2026-02-15T10:43:01.284353Z",
     "shell.execute_reply.started": "2026-02-15T10:41:54.694963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_iteration: 1152\n",
      "Test 2021 MAE (clip): 15.413751602172852\n"
     ]
    }
   ],
   "source": [
    "model_es = XGBRegressor(\n",
    "    objective=\"reg:absoluteerror\",\n",
    "    n_estimators=5000,\n",
    "    tree_method=\"hist\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=50,   # <-- no construtor\n",
    "    eval_metric=\"mae\",          # <-- no construtor\n",
    "    **best_params,\n",
    ")\n",
    "\n",
    "model_es.fit(\n",
    "    X_train.to_numpy(np.float32, copy=False),\n",
    "    y_train.to_numpy(np.float32, copy=False),\n",
    "    sample_weight=sample_weight_train.astype(np.float32, copy=False),\n",
    "    eval_set=[(\n",
    "        X_val.to_numpy(np.float32, copy=False),\n",
    "        y_val.to_numpy(np.float32, copy=False)\n",
    "    )],\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "pred_test = model_es.predict(X_test.to_numpy(np.float32, copy=False))\n",
    "print(\"best_iteration:\", getattr(model_es, \"best_iteration\", None))\n",
    "print(\"Test 2021 MAE (clip):\", mae_clip(y_test.to_numpy(np.float32, copy=False), pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66165502-3afe-4b8b-8384-0425e5e75d52",
   "metadata": {},
   "source": [
    "Early stopping improved stability relative to the no‑ES version, but still remained well above the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "41cde067-40cf-41bd-a317-fbb57a0fa9c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T11:18:38.458719Z",
     "iopub.status.busy": "2026-02-15T11:18:38.457907Z",
     "iopub.status.idle": "2026-02-15T11:18:38.493004Z",
     "shell.execute_reply": "2026-02-15T11:18:38.486773Z",
     "shell.execute_reply.started": "2026-02-15T11:18:38.458655Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1138/3440800954.py:12: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_results = pd.concat([df_results, pd.DataFrame([new_row])], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "new_row = {\n",
    "    \"model_name\": \"XGBRegressor\",\n",
    "    \"objective\": \"reg:absoluteerror\",\n",
    "    \"tag\": \"xgb_manual_expanded_abs_n30_refit_es50_clip\",\n",
    "    \"used_sample_weight\": True,\n",
    "    \"mae_val_2020\": None,  # este experimento é test-only\n",
    "    \"mae_test_2021\": float(mae_clip(y_test.to_numpy(np.float32, copy=False), pred_test)),\n",
    "    \"pred_min_test\": float(np.clip(pred_test, 0, 100).min()),\n",
    "    \"pred_max_test\": float(np.clip(pred_test, 0, 100).max()),\n",
    "}\n",
    "\n",
    "df_results = pd.concat([df_results, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "df_results = df_results.sort_values(\n",
    "    by=\"mae_val_2020\",\n",
    "    ascending=True,\n",
    "    na_position=\"last\"\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "ebe81617-bf6f-4e70-b377-6fbb1414e0e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T11:58:26.468331Z",
     "iopub.status.busy": "2026-02-15T11:58:26.466920Z",
     "iopub.status.idle": "2026-02-15T11:58:26.539611Z",
     "shell.execute_reply": "2026-02-15T11:58:26.534930Z",
     "shell.execute_reply.started": "2026-02-15T11:58:26.468225Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>objective</th>\n",
       "      <th>tag</th>\n",
       "      <th>used_sample_weight</th>\n",
       "      <th>mae_val_2020</th>\n",
       "      <th>mae_test_2021</th>\n",
       "      <th>pred_min_test</th>\n",
       "      <th>pred_max_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:absoluteerror</td>\n",
       "      <td>xgb_manual_expanded_abs_n30_clip</td>\n",
       "      <td>True</td>\n",
       "      <td>14.1195</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:absoluteerror</td>\n",
       "      <td>xgb_tuned_expanded_abs_n30_clip</td>\n",
       "      <td>True</td>\n",
       "      <td>14.1227</td>\n",
       "      <td>15.3649</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>54.3557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:absoluteerror</td>\n",
       "      <td>xgb_tuned_absoluteerror_clip</td>\n",
       "      <td>True</td>\n",
       "      <td>14.1274</td>\n",
       "      <td>15.5146</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>55.2573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:absoluteerror</td>\n",
       "      <td>xgb_tuned_reg:absoluteerror</td>\n",
       "      <td>True</td>\n",
       "      <td>14.1279</td>\n",
       "      <td>15.5150</td>\n",
       "      <td>-3.2848</td>\n",
       "      <td>55.2573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:absoluteerror</td>\n",
       "      <td>xgb_tuned_expanded_reg:absoluteerror</td>\n",
       "      <td>True</td>\n",
       "      <td>14.1280</td>\n",
       "      <td>15.2541</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>55.0586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:pseudohubererror</td>\n",
       "      <td>xgb_tuned_expanded_reg:pseudohubererror</td>\n",
       "      <td>True</td>\n",
       "      <td>14.1431</td>\n",
       "      <td>15.3699</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>54.9421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:pseudohubererror</td>\n",
       "      <td>xgb_tuned_pseudohuber_clip</td>\n",
       "      <td>True</td>\n",
       "      <td>14.1455</td>\n",
       "      <td>15.3036</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>58.3352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:pseudohubererror</td>\n",
       "      <td>xgb_tuned_reg:pseudohubererror</td>\n",
       "      <td>True</td>\n",
       "      <td>14.1467</td>\n",
       "      <td>15.3044</td>\n",
       "      <td>-3.2895</td>\n",
       "      <td>58.3352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>xgb_tuned_expanded_reg:squarederror</td>\n",
       "      <td>True</td>\n",
       "      <td>14.1685</td>\n",
       "      <td>15.4683</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>52.6271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>xgb_tuned_squarederror_clip</td>\n",
       "      <td>True</td>\n",
       "      <td>14.1788</td>\n",
       "      <td>15.4563</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>54.7270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>xgb_tuned_reg:squarederror</td>\n",
       "      <td>True</td>\n",
       "      <td>14.1789</td>\n",
       "      <td>15.4564</td>\n",
       "      <td>-3.2927</td>\n",
       "      <td>54.7270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:absoluteerror</td>\n",
       "      <td>xgb_manual_expanded_abs_n30_refit_clip</td>\n",
       "      <td>True</td>\n",
       "      <td>14.1831</td>\n",
       "      <td>15.4340</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>53.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:absoluteerror</td>\n",
       "      <td>xgb_point_absoluteerror_clip</td>\n",
       "      <td>True</td>\n",
       "      <td>14.2578</td>\n",
       "      <td>15.6953</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>63.2540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:absoluteerror</td>\n",
       "      <td>xgb_point_reg:absoluteerror</td>\n",
       "      <td>True</td>\n",
       "      <td>14.2693</td>\n",
       "      <td>15.7063</td>\n",
       "      <td>-13.5263</td>\n",
       "      <td>63.2540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>xgb_point_squarederror_clip</td>\n",
       "      <td>True</td>\n",
       "      <td>14.3968</td>\n",
       "      <td>15.5734</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>64.6988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>xgb_point_reg:squarederror</td>\n",
       "      <td>True</td>\n",
       "      <td>14.4042</td>\n",
       "      <td>15.5807</td>\n",
       "      <td>-14.4951</td>\n",
       "      <td>64.6988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:pseudohubererror</td>\n",
       "      <td>xgb_point_pseudohuber_clip</td>\n",
       "      <td>True</td>\n",
       "      <td>14.6141</td>\n",
       "      <td>15.8018</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:pseudohubererror</td>\n",
       "      <td>xgb_point_reg:pseudohubererror</td>\n",
       "      <td>True</td>\n",
       "      <td>15.0095</td>\n",
       "      <td>15.8873</td>\n",
       "      <td>-295.9467</td>\n",
       "      <td>260.7536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TweedieRegressor</td>\n",
       "      <td>tweedie_power_1.8</td>\n",
       "      <td>tweedie_power_1.8</td>\n",
       "      <td>True</td>\n",
       "      <td>15.2044</td>\n",
       "      <td>15.6097</td>\n",
       "      <td>6.1842</td>\n",
       "      <td>42.7838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>TweedieRegressor</td>\n",
       "      <td>tweedie_power_1.5</td>\n",
       "      <td>tweedie_power_1.5</td>\n",
       "      <td>True</td>\n",
       "      <td>15.2073</td>\n",
       "      <td>15.5736</td>\n",
       "      <td>5.1983</td>\n",
       "      <td>42.2427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>TweedieRegressor</td>\n",
       "      <td>tweedie_power_1.2</td>\n",
       "      <td>tweedie_power_1.2</td>\n",
       "      <td>True</td>\n",
       "      <td>15.2108</td>\n",
       "      <td>15.5369</td>\n",
       "      <td>4.2808</td>\n",
       "      <td>41.7994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>TweedieRegressor</td>\n",
       "      <td>tweedie_power_1.0</td>\n",
       "      <td>tweedie_power_1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>15.2135</td>\n",
       "      <td>15.5129</td>\n",
       "      <td>3.7809</td>\n",
       "      <td>41.5472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>None</td>\n",
       "      <td>baseline_huber15</td>\n",
       "      <td>True</td>\n",
       "      <td>15.2613</td>\n",
       "      <td>15.2127</td>\n",
       "      <td>-15.6361</td>\n",
       "      <td>31.3571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>None</td>\n",
       "      <td>baseline_huber15_clip</td>\n",
       "      <td>True</td>\n",
       "      <td>15.2613</td>\n",
       "      <td>15.2000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>31.3571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Hurdle(LogReg + Huber)</td>\n",
       "      <td>hurdle</td>\n",
       "      <td>hurdle_logreg_huber_clip</td>\n",
       "      <td>True</td>\n",
       "      <td>15.2613</td>\n",
       "      <td>17.6846</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>24.6983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:absoluteerror</td>\n",
       "      <td>xgb_manual_expanded_abs_n30_refit_es50_clip</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.4138</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>57.3520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:absoluteerror</td>\n",
       "      <td>xgb_manual_expanded_abs_n30_refit_no_es_clip</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.4249</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>60.5046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:absoluteerror</td>\n",
       "      <td>xgb_manual_expanded_abs_n30_refit_simplified_clip</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.5277</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>47.4937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model_name             objective                                                tag  \\\n",
       "0             XGBRegressor     reg:absoluteerror                   xgb_manual_expanded_abs_n30_clip   \n",
       "1             XGBRegressor     reg:absoluteerror                    xgb_tuned_expanded_abs_n30_clip   \n",
       "2             XGBRegressor     reg:absoluteerror                       xgb_tuned_absoluteerror_clip   \n",
       "3             XGBRegressor     reg:absoluteerror                        xgb_tuned_reg:absoluteerror   \n",
       "4             XGBRegressor     reg:absoluteerror               xgb_tuned_expanded_reg:absoluteerror   \n",
       "5             XGBRegressor  reg:pseudohubererror            xgb_tuned_expanded_reg:pseudohubererror   \n",
       "6             XGBRegressor  reg:pseudohubererror                         xgb_tuned_pseudohuber_clip   \n",
       "7             XGBRegressor  reg:pseudohubererror                     xgb_tuned_reg:pseudohubererror   \n",
       "8             XGBRegressor      reg:squarederror                xgb_tuned_expanded_reg:squarederror   \n",
       "9             XGBRegressor      reg:squarederror                        xgb_tuned_squarederror_clip   \n",
       "10            XGBRegressor      reg:squarederror                         xgb_tuned_reg:squarederror   \n",
       "11            XGBRegressor     reg:absoluteerror             xgb_manual_expanded_abs_n30_refit_clip   \n",
       "12            XGBRegressor     reg:absoluteerror                       xgb_point_absoluteerror_clip   \n",
       "13            XGBRegressor     reg:absoluteerror                        xgb_point_reg:absoluteerror   \n",
       "14            XGBRegressor      reg:squarederror                        xgb_point_squarederror_clip   \n",
       "15            XGBRegressor      reg:squarederror                         xgb_point_reg:squarederror   \n",
       "16            XGBRegressor  reg:pseudohubererror                         xgb_point_pseudohuber_clip   \n",
       "17            XGBRegressor  reg:pseudohubererror                     xgb_point_reg:pseudohubererror   \n",
       "18        TweedieRegressor     tweedie_power_1.8                                  tweedie_power_1.8   \n",
       "19        TweedieRegressor     tweedie_power_1.5                                  tweedie_power_1.5   \n",
       "20        TweedieRegressor     tweedie_power_1.2                                  tweedie_power_1.2   \n",
       "21        TweedieRegressor     tweedie_power_1.0                                  tweedie_power_1.0   \n",
       "22          HuberRegressor                  None                                   baseline_huber15   \n",
       "23          HuberRegressor                  None                              baseline_huber15_clip   \n",
       "24  Hurdle(LogReg + Huber)                hurdle                           hurdle_logreg_huber_clip   \n",
       "25            XGBRegressor     reg:absoluteerror        xgb_manual_expanded_abs_n30_refit_es50_clip   \n",
       "26            XGBRegressor     reg:absoluteerror       xgb_manual_expanded_abs_n30_refit_no_es_clip   \n",
       "27            XGBRegressor     reg:absoluteerror  xgb_manual_expanded_abs_n30_refit_simplified_clip   \n",
       "\n",
       "    used_sample_weight  mae_val_2020  mae_test_2021  pred_min_test  pred_max_test  \n",
       "0                 True       14.1195            NaN            NaN            NaN  \n",
       "1                 True       14.1227        15.3649         0.0000        54.3557  \n",
       "2                 True       14.1274        15.5146         0.0000        55.2573  \n",
       "3                 True       14.1279        15.5150        -3.2848        55.2573  \n",
       "4                 True       14.1280        15.2541         0.0000        55.0586  \n",
       "5                 True       14.1431        15.3699         0.0000        54.9421  \n",
       "6                 True       14.1455        15.3036         0.0000        58.3352  \n",
       "7                 True       14.1467        15.3044        -3.2895        58.3352  \n",
       "8                 True       14.1685        15.4683         0.0000        52.6271  \n",
       "9                 True       14.1788        15.4563         0.0000        54.7270  \n",
       "10                True       14.1789        15.4564        -3.2927        54.7270  \n",
       "11                True       14.1831        15.4340         0.0000        53.1278  \n",
       "12                True       14.2578        15.6953         0.0000        63.2540  \n",
       "13                True       14.2693        15.7063       -13.5263        63.2540  \n",
       "14                True       14.3968        15.5734         0.0000        64.6988  \n",
       "15                True       14.4042        15.5807       -14.4951        64.6988  \n",
       "16                True       14.6141        15.8018         0.0000       100.0000  \n",
       "17                True       15.0095        15.8873      -295.9467       260.7536  \n",
       "18                True       15.2044        15.6097         6.1842        42.7838  \n",
       "19                True       15.2073        15.5736         5.1983        42.2427  \n",
       "20                True       15.2108        15.5369         4.2808        41.7994  \n",
       "21                True       15.2135        15.5129         3.7809        41.5472  \n",
       "22                True       15.2613        15.2127       -15.6361        31.3571  \n",
       "23                True       15.2613        15.2000         0.0000        31.3571  \n",
       "24                True       15.2613        17.6846         0.0000        24.6983  \n",
       "25                True           NaN        15.4138         0.0000        57.3520  \n",
       "26                True           NaN        15.4249         0.0000        60.5046  \n",
       "27                True           NaN        15.5277         0.0000        47.4937  "
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = df_results.drop_duplicates().reset_index(drop=True)\n",
    "df_results = (\n",
    "    df_results\n",
    "    .drop_duplicates(subset=[\"tag\"], keep=\"first\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ef26a3-4b77-41ce-a861-8f3d39440545",
   "metadata": {},
   "source": [
    "### **4.6.6 Final Assessment: No Challenger Surpassed the Baseline**\n",
    "\n",
    "Across:\n",
    "\n",
    "- linear models  \n",
    "- GLMs  \n",
    "- hurdle models  \n",
    "- XGBoost point‑runs  \n",
    "- tuned models  \n",
    "- expanded tuned models  \n",
    "- 100‑iteration searches  \n",
    "- early stopping  \n",
    "- clipping  \n",
    "\n",
    "…the conclusion was consistent:\n",
    "\n",
    "> **No model surpassed the Huber‑15 baseline on Test 2021.**\n",
    "\n",
    "The best challenger reached **15.2541**, but the baseline remained superior at **15.2000**.\n",
    "\n",
    "### **4.6.7 Final Model Selection**\n",
    "\n",
    "Given:\n",
    "\n",
    "- superior generalization under drift  \n",
    "- stability  \n",
    "- interpretability  \n",
    "- simplicity of deployment  \n",
    "- and the fact that no challenger beat it  \n",
    "\n",
    "The **Huber‑15 regression model** was selected as the final model for implementation.\n",
    "\n",
    "# 5. Evaluation & Persistence (Cycle 3)\n",
    "\n",
    "## **5.1 Select the winner (baseline vs best XGB)**\n",
    "\n",
    "The goal of this step is to formalize the model‑selection decision using a simple and reproducible rule. All candidate models from Cycle 3 are stored in `df_results`, including the baseline (Huber‑15) and the best XGBoost variants obtained during tuning.\n",
    "\n",
    "The selection procedure follows two principles:\n",
    "\n",
    "1. **Ranking by the predefined selection metric**  \n",
    "   Models are first ordered by their **Val 2020 MAE (clip)**, which was the metric used throughout Cycle 3 for hyperparameter search.\n",
    "\n",
    "2. **Final decision based on generalization**  \n",
    "   Even if a model ranks first on validation, it is only declared the champion if it also **beats the baseline on Test 2021**.  \n",
    "   This ensures that the final choice prioritizes out‑of‑sample performance and avoids overfitting to the validation split.\n",
    "\n",
    "The following cell implements this rule and prints the full comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "628ad497-8e51-4cdc-ae8c-2995c571569e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T12:02:43.866945Z",
     "iopub.status.busy": "2026-02-15T12:02:43.866540Z",
     "iopub.status.idle": "2026-02-15T12:02:43.990247Z",
     "shell.execute_reply": "2026-02-15T12:02:43.981924Z",
     "shell.execute_reply.started": "2026-02-15T12:02:43.866920Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Mode: clip ===\n",
      "Baseline:        baseline_huber15_clip  (Test=15.2000)\n",
      "Selected by Val: xgb_tuned_expanded_abs_n30_clip      (Test=15.3649)\n",
      "Gap to baseline: +0.1649\n",
      "Champion:        baseline_huber15_clip\n",
      "\n",
      "=== Mode: no_clip ===\n",
      "Baseline:        baseline_huber15  (Test=15.2127)\n",
      "Selected by Val: xgb_tuned_reg:absoluteerror      (Test=15.5150)\n",
      "Gap to baseline: +0.3023\n",
      "Champion:        baseline_huber15\n"
     ]
    }
   ],
   "source": [
    "# --- Dual-mode selection: clip and no-clip in one run ---\n",
    "\n",
    "SEL_COL  = \"mae_val_2020\"\n",
    "TEST_COL = \"mae_test_2021\"\n",
    "\n",
    "MODES = {\n",
    "    \"clip\": {\n",
    "        \"filter\": lambda df: df[\"tag\"].astype(str).str.contains(\"clip\"),\n",
    "        \"baseline\": \"baseline_huber15_clip\",\n",
    "    },\n",
    "    \"no_clip\": {\n",
    "        \"filter\": lambda df: ~df[\"tag\"].astype(str).str.contains(\"clip\"),\n",
    "        \"baseline\": \"baseline_huber15\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# --- Run both modes ---\n",
    "results = [evaluate_mode(m, df_results) for m in MODES]\n",
    "\n",
    "# --- Pretty print ---\n",
    "for r in results:\n",
    "    print(f\"\\n=== Mode: {r['mode']} ===\")\n",
    "    print(f\"Baseline:        {r['baseline_tag']}  (Test={r['baseline_test']:.4f})\")\n",
    "    print(f\"Selected by Val: {r['best_tag']}      (Test={r['best_test']:.4f})\")\n",
    "    print(f\"Gap to baseline: {r['gap']:+.4f}\")\n",
    "    print(f\"Champion:        {r['champion']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "1111af88-31a3-48c6-872c-a7cdd383d886",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T15:14:45.910386Z",
     "iopub.status.busy": "2026-02-16T15:14:45.898565Z",
     "iopub.status.idle": "2026-02-16T15:14:46.098799Z",
     "shell.execute_reply": "2026-02-16T15:14:46.094749Z",
     "shell.execute_reply.started": "2026-02-16T15:14:45.910036Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>objective</th>\n",
       "      <th>tag</th>\n",
       "      <th>used_sample_weight</th>\n",
       "      <th>mae_val_2020</th>\n",
       "      <th>mae_test_2021</th>\n",
       "      <th>pred_min_test</th>\n",
       "      <th>pred_max_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:absoluteerror</td>\n",
       "      <td>xgb_manual_expanded_abs_n30_clip</td>\n",
       "      <td>True</td>\n",
       "      <td>14.1195</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:absoluteerror</td>\n",
       "      <td>xgb_tuned_expanded_abs_n30_clip</td>\n",
       "      <td>True</td>\n",
       "      <td>14.1227</td>\n",
       "      <td>15.3649</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>54.3557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:absoluteerror</td>\n",
       "      <td>xgb_tuned_absoluteerror_clip</td>\n",
       "      <td>True</td>\n",
       "      <td>14.1274</td>\n",
       "      <td>15.5146</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>55.2573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:absoluteerror</td>\n",
       "      <td>xgb_tuned_reg:absoluteerror</td>\n",
       "      <td>True</td>\n",
       "      <td>14.1279</td>\n",
       "      <td>15.5150</td>\n",
       "      <td>-3.2848</td>\n",
       "      <td>55.2573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:absoluteerror</td>\n",
       "      <td>xgb_tuned_expanded_reg:absoluteerror</td>\n",
       "      <td>True</td>\n",
       "      <td>14.1280</td>\n",
       "      <td>15.2541</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>55.0586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:pseudohubererror</td>\n",
       "      <td>xgb_tuned_expanded_reg:pseudohubererror</td>\n",
       "      <td>True</td>\n",
       "      <td>14.1431</td>\n",
       "      <td>15.3699</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>54.9421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:pseudohubererror</td>\n",
       "      <td>xgb_tuned_pseudohuber_clip</td>\n",
       "      <td>True</td>\n",
       "      <td>14.1455</td>\n",
       "      <td>15.3036</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>58.3352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:pseudohubererror</td>\n",
       "      <td>xgb_tuned_reg:pseudohubererror</td>\n",
       "      <td>True</td>\n",
       "      <td>14.1467</td>\n",
       "      <td>15.3044</td>\n",
       "      <td>-3.2895</td>\n",
       "      <td>58.3352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>xgb_tuned_expanded_reg:squarederror</td>\n",
       "      <td>True</td>\n",
       "      <td>14.1685</td>\n",
       "      <td>15.4683</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>52.6271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>xgb_tuned_squarederror_clip</td>\n",
       "      <td>True</td>\n",
       "      <td>14.1788</td>\n",
       "      <td>15.4563</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>54.7270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>xgb_tuned_reg:squarederror</td>\n",
       "      <td>True</td>\n",
       "      <td>14.1789</td>\n",
       "      <td>15.4564</td>\n",
       "      <td>-3.2927</td>\n",
       "      <td>54.7270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:absoluteerror</td>\n",
       "      <td>xgb_manual_expanded_abs_n30_refit_clip</td>\n",
       "      <td>True</td>\n",
       "      <td>14.1831</td>\n",
       "      <td>15.4340</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>53.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:absoluteerror</td>\n",
       "      <td>xgb_point_absoluteerror_clip</td>\n",
       "      <td>True</td>\n",
       "      <td>14.2578</td>\n",
       "      <td>15.6953</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>63.2540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:absoluteerror</td>\n",
       "      <td>xgb_point_reg:absoluteerror</td>\n",
       "      <td>True</td>\n",
       "      <td>14.2693</td>\n",
       "      <td>15.7063</td>\n",
       "      <td>-13.5263</td>\n",
       "      <td>63.2540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>xgb_point_squarederror_clip</td>\n",
       "      <td>True</td>\n",
       "      <td>14.3968</td>\n",
       "      <td>15.5734</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>64.6988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>xgb_point_reg:squarederror</td>\n",
       "      <td>True</td>\n",
       "      <td>14.4042</td>\n",
       "      <td>15.5807</td>\n",
       "      <td>-14.4951</td>\n",
       "      <td>64.6988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:pseudohubererror</td>\n",
       "      <td>xgb_point_pseudohuber_clip</td>\n",
       "      <td>True</td>\n",
       "      <td>14.6141</td>\n",
       "      <td>15.8018</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:pseudohubererror</td>\n",
       "      <td>xgb_point_reg:pseudohubererror</td>\n",
       "      <td>True</td>\n",
       "      <td>15.0095</td>\n",
       "      <td>15.8873</td>\n",
       "      <td>-295.9467</td>\n",
       "      <td>260.7536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TweedieRegressor</td>\n",
       "      <td>tweedie_power_1.8</td>\n",
       "      <td>tweedie_power_1.8</td>\n",
       "      <td>True</td>\n",
       "      <td>15.2044</td>\n",
       "      <td>15.6097</td>\n",
       "      <td>6.1842</td>\n",
       "      <td>42.7838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>TweedieRegressor</td>\n",
       "      <td>tweedie_power_1.5</td>\n",
       "      <td>tweedie_power_1.5</td>\n",
       "      <td>True</td>\n",
       "      <td>15.2073</td>\n",
       "      <td>15.5736</td>\n",
       "      <td>5.1983</td>\n",
       "      <td>42.2427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>TweedieRegressor</td>\n",
       "      <td>tweedie_power_1.2</td>\n",
       "      <td>tweedie_power_1.2</td>\n",
       "      <td>True</td>\n",
       "      <td>15.2108</td>\n",
       "      <td>15.5369</td>\n",
       "      <td>4.2808</td>\n",
       "      <td>41.7994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>TweedieRegressor</td>\n",
       "      <td>tweedie_power_1.0</td>\n",
       "      <td>tweedie_power_1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>15.2135</td>\n",
       "      <td>15.5129</td>\n",
       "      <td>3.7809</td>\n",
       "      <td>41.5472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>None</td>\n",
       "      <td>baseline_huber15</td>\n",
       "      <td>True</td>\n",
       "      <td>15.2613</td>\n",
       "      <td>15.2127</td>\n",
       "      <td>-15.6361</td>\n",
       "      <td>31.3571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>None</td>\n",
       "      <td>baseline_huber15_clip</td>\n",
       "      <td>True</td>\n",
       "      <td>15.2613</td>\n",
       "      <td>15.2000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>31.3571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Hurdle(LogReg + Huber)</td>\n",
       "      <td>hurdle</td>\n",
       "      <td>hurdle_logreg_huber_clip</td>\n",
       "      <td>True</td>\n",
       "      <td>15.2613</td>\n",
       "      <td>17.6846</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>24.6983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:absoluteerror</td>\n",
       "      <td>xgb_manual_expanded_abs_n30_refit_es50_clip</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.4138</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>57.3520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:absoluteerror</td>\n",
       "      <td>xgb_manual_expanded_abs_n30_refit_no_es_clip</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.4249</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>60.5046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>reg:absoluteerror</td>\n",
       "      <td>xgb_manual_expanded_abs_n30_refit_simplified_clip</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.5277</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>47.4937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model_name             objective                                                tag  \\\n",
       "0             XGBRegressor     reg:absoluteerror                   xgb_manual_expanded_abs_n30_clip   \n",
       "1             XGBRegressor     reg:absoluteerror                    xgb_tuned_expanded_abs_n30_clip   \n",
       "2             XGBRegressor     reg:absoluteerror                       xgb_tuned_absoluteerror_clip   \n",
       "3             XGBRegressor     reg:absoluteerror                        xgb_tuned_reg:absoluteerror   \n",
       "4             XGBRegressor     reg:absoluteerror               xgb_tuned_expanded_reg:absoluteerror   \n",
       "5             XGBRegressor  reg:pseudohubererror            xgb_tuned_expanded_reg:pseudohubererror   \n",
       "6             XGBRegressor  reg:pseudohubererror                         xgb_tuned_pseudohuber_clip   \n",
       "7             XGBRegressor  reg:pseudohubererror                     xgb_tuned_reg:pseudohubererror   \n",
       "8             XGBRegressor      reg:squarederror                xgb_tuned_expanded_reg:squarederror   \n",
       "9             XGBRegressor      reg:squarederror                        xgb_tuned_squarederror_clip   \n",
       "10            XGBRegressor      reg:squarederror                         xgb_tuned_reg:squarederror   \n",
       "11            XGBRegressor     reg:absoluteerror             xgb_manual_expanded_abs_n30_refit_clip   \n",
       "12            XGBRegressor     reg:absoluteerror                       xgb_point_absoluteerror_clip   \n",
       "13            XGBRegressor     reg:absoluteerror                        xgb_point_reg:absoluteerror   \n",
       "14            XGBRegressor      reg:squarederror                        xgb_point_squarederror_clip   \n",
       "15            XGBRegressor      reg:squarederror                         xgb_point_reg:squarederror   \n",
       "16            XGBRegressor  reg:pseudohubererror                         xgb_point_pseudohuber_clip   \n",
       "17            XGBRegressor  reg:pseudohubererror                     xgb_point_reg:pseudohubererror   \n",
       "18        TweedieRegressor     tweedie_power_1.8                                  tweedie_power_1.8   \n",
       "19        TweedieRegressor     tweedie_power_1.5                                  tweedie_power_1.5   \n",
       "20        TweedieRegressor     tweedie_power_1.2                                  tweedie_power_1.2   \n",
       "21        TweedieRegressor     tweedie_power_1.0                                  tweedie_power_1.0   \n",
       "22          HuberRegressor                  None                                   baseline_huber15   \n",
       "23          HuberRegressor                  None                              baseline_huber15_clip   \n",
       "24  Hurdle(LogReg + Huber)                hurdle                           hurdle_logreg_huber_clip   \n",
       "25            XGBRegressor     reg:absoluteerror        xgb_manual_expanded_abs_n30_refit_es50_clip   \n",
       "26            XGBRegressor     reg:absoluteerror       xgb_manual_expanded_abs_n30_refit_no_es_clip   \n",
       "27            XGBRegressor     reg:absoluteerror  xgb_manual_expanded_abs_n30_refit_simplified_clip   \n",
       "\n",
       "    used_sample_weight  mae_val_2020  mae_test_2021  pred_min_test  pred_max_test  \n",
       "0                 True       14.1195            NaN            NaN            NaN  \n",
       "1                 True       14.1227        15.3649         0.0000        54.3557  \n",
       "2                 True       14.1274        15.5146         0.0000        55.2573  \n",
       "3                 True       14.1279        15.5150        -3.2848        55.2573  \n",
       "4                 True       14.1280        15.2541         0.0000        55.0586  \n",
       "5                 True       14.1431        15.3699         0.0000        54.9421  \n",
       "6                 True       14.1455        15.3036         0.0000        58.3352  \n",
       "7                 True       14.1467        15.3044        -3.2895        58.3352  \n",
       "8                 True       14.1685        15.4683         0.0000        52.6271  \n",
       "9                 True       14.1788        15.4563         0.0000        54.7270  \n",
       "10                True       14.1789        15.4564        -3.2927        54.7270  \n",
       "11                True       14.1831        15.4340         0.0000        53.1278  \n",
       "12                True       14.2578        15.6953         0.0000        63.2540  \n",
       "13                True       14.2693        15.7063       -13.5263        63.2540  \n",
       "14                True       14.3968        15.5734         0.0000        64.6988  \n",
       "15                True       14.4042        15.5807       -14.4951        64.6988  \n",
       "16                True       14.6141        15.8018         0.0000       100.0000  \n",
       "17                True       15.0095        15.8873      -295.9467       260.7536  \n",
       "18                True       15.2044        15.6097         6.1842        42.7838  \n",
       "19                True       15.2073        15.5736         5.1983        42.2427  \n",
       "20                True       15.2108        15.5369         4.2808        41.7994  \n",
       "21                True       15.2135        15.5129         3.7809        41.5472  \n",
       "22                True       15.2613        15.2127       -15.6361        31.3571  \n",
       "23                True       15.2613        15.2000         0.0000        31.3571  \n",
       "24                True       15.2613        17.6846         0.0000        24.6983  \n",
       "25                True           NaN        15.4138         0.0000        57.3520  \n",
       "26                True           NaN        15.4249         0.0000        60.5046  \n",
       "27                True           NaN        15.5277         0.0000        47.4937  "
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "0ae1569b-c78c-4c9f-88a0-6ac6258cdf97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T11:34:25.929330Z",
     "iopub.status.busy": "2026-02-17T11:34:25.926894Z",
     "iopub.status.idle": "2026-02-17T11:34:26.104018Z",
     "shell.execute_reply": "2026-02-17T11:34:26.090839Z",
     "shell.execute_reply.started": "2026-02-17T11:34:25.928832Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"model_name\":{\"0\":\"XGBRegressor\",\"1\":\"XGBRegressor\",\"2\":\"XGBRegressor\",\"3\":\"XGBRegressor\",\"4\":\"XGBRegressor\",\"5\":\"XGBRegressor\",\"6\":\"XGBRegressor\",\"7\":\"XGBRegressor\",\"8\":\"XGBRegressor\",\"9\":\"XGBRegressor\",\"10\":\"XGBRegressor\",\"11\":\"XGBRegressor\",\"12\":\"XGBRegressor\",\"13\":\"XGBRegressor\",\"14\":\"XGBRegressor\",\"15\":\"XGBRegressor\",\"16\":\"XGBRegressor\",\"17\":\"XGBRegressor\",\"18\":\"TweedieRegressor\",\"19\":\"TweedieRegressor\",\"20\":\"TweedieRegressor\",\"21\":\"TweedieRegressor\",\"22\":\"HuberRegressor\",\"23\":\"HuberRegressor\",\"24\":\"Hurdle(LogReg + Huber)\",\"25\":\"XGBRegressor\",\"26\":\"XGBRegressor\",\"27\":\"XGBRegressor\"},\"objective\":{\"0\":\"reg:absoluteerror\",\"1\":\"reg:absoluteerror\",\"2\":\"reg:absoluteerror\",\"3\":\"reg:absoluteerror\",\"4\":\"reg:absoluteerror\",\"5\":\"reg:pseudohubererror\",\"6\":\"reg:pseudohubererror\",\"7\":\"reg:pseudohubererror\",\"8\":\"reg:squarederror\",\"9\":\"reg:squarederror\",\"10\":\"reg:squarederror\",\"11\":\"reg:absoluteerror\",\"12\":\"reg:absoluteerror\",\"13\":\"reg:absoluteerror\",\"14\":\"reg:squarederror\",\"15\":\"reg:squarederror\",\"16\":\"reg:pseudohubererror\",\"17\":\"reg:pseudohubererror\",\"18\":\"tweedie_power_1.8\",\"19\":\"tweedie_power_1.5\",\"20\":\"tweedie_power_1.2\",\"21\":\"tweedie_power_1.0\",\"22\":null,\"23\":null,\"24\":\"hurdle\",\"25\":\"reg:absoluteerror\",\"26\":\"reg:absoluteerror\",\"27\":\"reg:absoluteerror\"},\"tag\":{\"0\":\"xgb_manual_expanded_abs_n30_clip\",\"1\":\"xgb_tuned_expanded_abs_n30_clip\",\"2\":\"xgb_tuned_absoluteerror_clip\",\"3\":\"xgb_tuned_reg:absoluteerror\",\"4\":\"xgb_tuned_expanded_reg:absoluteerror\",\"5\":\"xgb_tuned_expanded_reg:pseudohubererror\",\"6\":\"xgb_tuned_pseudohuber_clip\",\"7\":\"xgb_tuned_reg:pseudohubererror\",\"8\":\"xgb_tuned_expanded_reg:squarederror\",\"9\":\"xgb_tuned_squarederror_clip\",\"10\":\"xgb_tuned_reg:squarederror\",\"11\":\"xgb_manual_expanded_abs_n30_refit_clip\",\"12\":\"xgb_point_absoluteerror_clip\",\"13\":\"xgb_point_reg:absoluteerror\",\"14\":\"xgb_point_squarederror_clip\",\"15\":\"xgb_point_reg:squarederror\",\"16\":\"xgb_point_pseudohuber_clip\",\"17\":\"xgb_point_reg:pseudohubererror\",\"18\":\"tweedie_power_1.8\",\"19\":\"tweedie_power_1.5\",\"20\":\"tweedie_power_1.2\",\"21\":\"tweedie_power_1.0\",\"22\":\"baseline_huber15\",\"23\":\"baseline_huber15_clip\",\"24\":\"hurdle_logreg_huber_clip\",\"25\":\"xgb_manual_expanded_abs_n30_refit_es50_clip\",\"26\":\"xgb_manual_expanded_abs_n30_refit_no_es_clip\",\"27\":\"xgb_manual_expanded_abs_n30_refit_simplified_clip\"},\"used_sample_weight\":{\"0\":true,\"1\":true,\"2\":true,\"3\":true,\"4\":true,\"5\":true,\"6\":true,\"7\":true,\"8\":true,\"9\":true,\"10\":true,\"11\":true,\"12\":true,\"13\":true,\"14\":true,\"15\":true,\"16\":true,\"17\":true,\"18\":true,\"19\":true,\"20\":true,\"21\":true,\"22\":true,\"23\":true,\"24\":true,\"25\":true,\"26\":true,\"27\":true},\"mae_val_2020\":{\"0\":14.1195,\"1\":14.1226696212,\"2\":14.1273644698,\"3\":14.1278776067,\"4\":14.1279953741,\"5\":14.1430590843,\"6\":14.1454649235,\"7\":14.1467120018,\"8\":14.1685426743,\"9\":14.1788212728,\"10\":14.1789375256,\"11\":14.1830606461,\"12\":14.257828065,\"13\":14.2693288944,\"14\":14.396838721,\"15\":14.4042320693,\"16\":14.6141194004,\"17\":15.0094861966,\"18\":15.204356886,\"19\":15.2073190887,\"20\":15.2107726118,\"21\":15.2135261264,\"22\":15.2612789578,\"23\":15.2612789578,\"24\":15.2612789578,\"25\":null,\"26\":null,\"27\":null},\"mae_test_2021\":{\"0\":null,\"1\":15.3648536117,\"2\":15.514610736,\"3\":15.5149768503,\"4\":15.2541134253,\"5\":15.369854782,\"6\":15.3036046794,\"7\":15.3043888681,\"8\":15.4682897077,\"9\":15.4563023269,\"10\":15.4563812951,\"11\":15.4339694977,\"12\":15.695326653,\"13\":15.7062803141,\"14\":15.5734069028,\"15\":15.5807418553,\"16\":15.8018293679,\"17\":15.8872899221,\"18\":15.609690169,\"19\":15.5735686987,\"20\":15.5369360905,\"21\":15.5128889814,\"22\":15.2126670705,\"23\":15.1999690855,\"24\":17.6845624748,\"25\":15.4137516022,\"26\":15.4248685837,\"27\":15.5276632309},\"pred_min_test\":{\"0\":null,\"1\":0.0,\"2\":0.0,\"3\":-3.2848227024,\"4\":0.0,\"5\":0.0,\"6\":0.0,\"7\":-3.2894873619,\"8\":0.0,\"9\":0.0,\"10\":-3.2927355766,\"11\":0.0,\"12\":0.0,\"13\":-13.5263433456,\"14\":0.0,\"15\":-14.495054245,\"16\":0.0,\"17\":-295.9467468262,\"18\":6.1842262494,\"19\":5.1982924871,\"20\":4.2808255634,\"21\":3.7808899374,\"22\":-15.6360867235,\"23\":0.0,\"24\":0.0,\"25\":0.0,\"26\":0.0,\"27\":0.0},\"pred_max_test\":{\"0\":null,\"1\":54.3557052612,\"2\":55.2573394775,\"3\":55.2573394775,\"4\":55.0585746765,\"5\":54.942073822,\"6\":58.3352394104,\"7\":58.3352394104,\"8\":52.6271286011,\"9\":54.7270202637,\"10\":54.7270202637,\"11\":53.1277885437,\"12\":63.2539596558,\"13\":63.2539596558,\"14\":64.6988067627,\"15\":64.6988067627,\"16\":100.0,\"17\":260.7536010742,\"18\":42.7838214374,\"19\":42.2426605417,\"20\":41.7994213238,\"21\":41.547223603,\"22\":31.3570976962,\"23\":31.3570976962,\"24\":24.6982545705,\"25\":57.3520355225,\"26\":60.5046463013,\"27\":47.4936561584}}'"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc85563-95ed-4acf-ac82-bf37419b1368",
   "metadata": {},
   "source": [
    "1. **Main conclusion**\n",
    "\n",
    "   \n",
    "   Despite extensive tuning and several XGBoost variants achieving strong validation performance (Val 2020 MAE ≈ 14.12–14.18), none of them outperformed the Huber-15 baseline on the held-out Test 2021 set, in either **no-clip** or **clip (0–100)** mode.\n",
    "\n",
    "2. **Interpretation**\n",
    "\n",
    "   \n",
    "   This Val–Test contrast suggests that the improvements observed on 2020 do not carry over to 2021 under the chosen temporal split (potentially due to concept drift between 2020 and 2021), so validation gains alone are not sufficient to justify replacing the baseline.\n",
    "\n",
    "3. **Selection vs decision rule**\n",
    "\n",
    "   \n",
    "   Within each evaluation mode, models are **selected by Val 2020 MAE** and the final “champion” decision is made by **Test 2021 MAE** under the same mode; under this rule, **Huber-15 remains the champion**.\n",
    "\n",
    "4. **Short “punchline”**\n",
    "\n",
    "   \n",
    "   In this setting, the most complex model was not the most reliable out-of-sample; the robust Huber-15 baseline delivered the best generalization on Test 2021.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b0c102-5688-4d16-8e80-b6a3bb46aaa5",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart TD\n",
    "    A[\"Goal: choose the Cycle 3 champion<br/>under the frozen protocol (Huber-15 input space)\"] --> B[\"Protocol invariants (guardrails)<br/>• Temporal split: train ≤2019, val=2020, test=2021<br/>• 15 numeric columns only<br/>• Median imputer fit on train only<br/>• Recency weights (λ=0.05) on train only\"]\n",
    "\n",
    "    B --> C[\"Candidates evaluated\"]\n",
    "    C --> H[\"Baseline: HuberRegressor (Huber-15)\"]\n",
    "    C --> X[\"Challengers: XGBoost variants<br/>• point runs (3 objectives)<br/>• tuned runs (PredefinedSplit train/val)<br/>• expanded tuning (reg_alpha, reg_lambda, gamma, min_child_weight, ...)\"]\n",
    "    C --> T[\"Other probes<br/>• TweedieRegressor (powers)<br/>• Minimal hurdle (LogReg + Huber)\"]\n",
    "\n",
    "    %% Selection / Decision\n",
    "    H --> S[\"Selection metric (for tuning):<br/>Val 2020 MAE (within mode)\"]\n",
    "    X --> S\n",
    "    T --> S\n",
    "\n",
    "    S --> D[\"Decision metric (final):<br/>Test 2021 MAE (within mode)\"]\n",
    "\n",
    "    %% What happened\n",
    "    D --> V1[\"Observed pattern:<br/>XGBoost wins on Val 2020 (≈14.12–14.18)<br/>but loses on Test 2021 (≥15.25–15.89)\"]\n",
    "    D --> V2[\"Huber-15 remains best on Test 2021<br/>• no-clip: 15.2127<br/>• clip 0–100: 15.2000\"]\n",
    "\n",
    "    %% Why Huber wins\n",
    "    V1 --> W[\"Key reason for Huber-15 win:<br/>Generalization under temporal drift\"]\n",
    "    W --> W1[\"2021 distribution shifts (more zeros / drift)<br/>→ validation gains (2020) don't transfer\"]\n",
    "    W --> W2[\"Huber loss is robust to outliers / heavy tails<br/>→ fewer catastrophic errors in 2021\"]\n",
    "    W --> W3[\"Small, fixed feature space (15 numeric)<br/>→ reduced variance vs tuned tree ensembles\"]\n",
    "\n",
    "    %% Supporting signals\n",
    "    V1 --> Z[\"Supporting signals observed\"]\n",
    "    Z --> Z1[\"Tuned XGB shows lower Val MAE but higher Test MAE<br/>→ over-specialization to 2020 patterns\"]\n",
    "    Z --> Z2[\"Pseudo-Huber objective had unstable ranges in point run<br/>→ risk of extreme predictions\"]\n",
    "    Z --> Z3[\"Hurdle probe degraded MAE (error propagation)<br/>→ not beneficial in minimal form\"]\n",
    "    Z --> Z4[\"Clip helps Huber slightly (~-0.013 MAE) but<br/>doesn't change winner\"]\n",
    "\n",
    "    %% Champion decision\n",
    "    V2 --> CH[\"Champion decision (per mode):<br/>Champion = argmin(Test 2021 MAE)\"]\n",
    "    CH --> OUT[\"Winner: Huber-15 baseline<br/>Most stable and best generalization on 2021\"]\n",
    "\n",
    "    %% Visual emphasis\n",
    "    classDef winner fill:#dff7df,stroke:#2e7d32,stroke-width:2px;\n",
    "    classDef warn fill:#fff3cd,stroke:#b26a00,stroke-width:1px;\n",
    "    class H,OUT,V2,CH winner;\n",
    "    class V1,Z,W warn;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813096cb-30dd-4a84-aba3-eca233e3bf46",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart LR\n",
    "    A[\"Same protocol for everyone<br/>(same split, same 15 features, same preprocessing)\"] --> B[\"Compare two signals\"]\n",
    "    B --> C[\"Validation (2020)<br/>(Looks good now)\"]\n",
    "    B --> D[\"Test (2021)<br/>(will it still work next year?)\"]\n",
    "\n",
    "    C --> XGB[\"XGBoost: very low Val MAE<br/>(learns many fine patterns)\"]\n",
    "    D --> XGBT[\"But higher Test MAE in 2021<br/>(patterns don't transfer under drift)\"]\n",
    "\n",
    "    C --> HUB[\"Huber-15: Val MAE not the best<br/>(simpler fit)\"]\n",
    "    D --> HUBT[\"But best Test MAE in 2021<br/>(stable under drift + robust loss)\"]\n",
    "\n",
    "    XGB --> E[\"Reason: high flexibility → can overfit 2020 quirks\"]\n",
    "    E --> XGBT\n",
    "\n",
    "    HUB --> F[\"Reason: robust loss + low-variance model<br/>→ avoids big mistakes when 2021 shifts\"]\n",
    "    F --> HUBT\n",
    "\n",
    "    XGBT --> G[\"Champion rule:<br/>pick lowest Test MAE (generalization)\"]\n",
    "    HUBT --> G\n",
    "\n",
    "    G --> WIN[\"Winner: Huber-15<br/>(most consistent next-year performance)\"]\n",
    "\n",
    "    classDef win fill:#dff7df,stroke:#2e7d32,stroke-width:2px;\n",
    "    classDef lose fill:#ffe6e6,stroke:#b71c1c,stroke-width:1px;\n",
    "    class WIN,HUBT win;\n",
    "    class XGBT lose;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025f4d45-9df9-4dd5-944b-6a828fca670c",
   "metadata": {},
   "source": [
    "## 5.2 — Persist the champion model (`champion.joblib`)\n",
    "\n",
    "The Cycle 3 champion is the **Huber-15 baseline** under the frozen protocol (15 numeric columns + train-only median imputation + train-only recency weights, λ=0.05).  \n",
    "Since no challenger improved **Test 2021 MAE** under the same guardrails, we persist this fitted model as the official Cycle 3 artifact.\n",
    "\n",
    "- **Artifact:** `models/cycle_03/champion.joblib`\n",
    "- **Model:** `HuberRegressor`\n",
    "- **Protocol:** `Baseline_Huber15_recency0p05_medfill`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "d8768e22-4b87-43f4-b9d9-f7ff9dec6fc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T23:29:36.025966Z",
     "iopub.status.busy": "2026-02-16T23:29:35.997774Z",
     "iopub.status.idle": "2026-02-16T23:29:36.187272Z",
     "shell.execute_reply": "2026-02-16T23:29:36.179872Z",
     "shell.execute_reply.started": "2026-02-16T23:29:36.024954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved champion model: /mnt/c/Users/Daniel/OneDrive/Documentos/_Cursos/Outros/PopForecast/models/cycle_03/champion.joblib\n"
     ]
    }
   ],
   "source": [
    "# --- 5.2 Persist the champion model ---\n",
    "\n",
    "\n",
    "# Ensure output dir exists\n",
    "CYCLE3_MODELS_DIR = PROJECT_ROOT / \"models\" / \"cycle_03\"\n",
    "CYCLE3_MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHAMPION_PATH = CYCLE3_MODELS_DIR / \"champion.joblib\"\n",
    "\n",
    "# Champion = Huber baseline under the frozen protocol\n",
    "champion_model = huber  # assumes you already fit() it in 3.4\n",
    "\n",
    "joblib.dump(champion_model, CHAMPION_PATH)\n",
    "print(\"Saved champion model:\", CHAMPION_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b412aa-4647-4949-912a-766c17369d1d",
   "metadata": {},
   "source": [
    "## 5.3 — Record run metadata (traceability bundle)\n",
    "\n",
    "To make the Cycle 3 outcome auditable and reproducible, we write a metadata JSON capturing:\n",
    "- **protocol guardrails** (temporal split, 15-column list, imputation scope, recency settings, NaN-year policy),\n",
    "- **champion identity and params**,\n",
    "- **metrics** reported in two modes:\n",
    "  - **no_clip** (decision metric)\n",
    "  - **clip_0_100** (evaluation-only sanity check)\n",
    "- **artifact hashes** (SHA256) and key environment versions.\n",
    "\n",
    "- **Run metadata:** `models/cycle_03/run_metadata_cycle3.json`\n",
    "- **Decision metric:** `mae_test_2021_no_clip`\n",
    "- **Reported modes:** `no_clip`, `clip_0_100` (clip is evaluation-only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "a1293af5-1ab4-4a05-8d61-23663e929cfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T21:56:47.683061Z",
     "iopub.status.busy": "2026-02-16T21:56:47.681954Z",
     "iopub.status.idle": "2026-02-16T21:56:47.938305Z",
     "shell.execute_reply": "2026-02-16T21:56:47.936732Z",
     "shell.execute_reply.started": "2026-02-16T21:56:47.683024Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary: {'baseline_beaten_any_no_clip': False, 'baseline_beaten_any_clip': False, 'best_candidate_no_clip': 15.254113425308288, 'best_candidate_clip': 15.303604679397173}\n",
      "Saved run metadata: /mnt/c/Users/Daniel/OneDrive/Documentos/_Cursos/Outros/PopForecast/models/cycle_03/run_metadata_cycle3.json\n"
     ]
    }
   ],
   "source": [
    "# --- 5.3 Record run metadata (audit) ---\n",
    "    \n",
    "# baseline rows\n",
    "base_no_clip = df_results.loc[df_results[\"tag\"].eq(\"baseline_huber15\"), \"mae_test_2021\"].iloc[0]\n",
    "base_clip    = df_results.loc[df_results[\"tag\"].eq(\"baseline_huber15_clip\"), \"mae_test_2021\"].iloc[0]\n",
    "\n",
    "# candidates\n",
    "cand = df_results[df_results[\"model_name\"] != \"HuberRegressor\"].copy()\n",
    "cand[\"is_clip\"] = cand[\"tag\"].map(_is_clip_tag)\n",
    "\n",
    "best_cand_no_clip = cand.loc[~cand[\"is_clip\"], \"mae_test_2021\"].min()\n",
    "best_cand_clip    = cand.loc[cand[\"is_clip\"],  \"mae_test_2021\"].min()\n",
    "\n",
    "summary = {\n",
    "    \"baseline_beaten_any_no_clip\": bool(best_cand_no_clip < base_no_clip) if pd.notna(best_cand_no_clip) else None,\n",
    "    \"baseline_beaten_any_clip\":    bool(best_cand_clip    < base_clip)    if pd.notna(best_cand_clip)    else None,\n",
    "    \"best_candidate_no_clip\": float(best_cand_no_clip) if pd.notna(best_cand_no_clip) else None,\n",
    "    \"best_candidate_clip\":    float(best_cand_clip)    if pd.notna(best_cand_clip)    else None,\n",
    "}\n",
    "# print(f'summary: {summary}')\n",
    "\n",
    "\n",
    "# Governance + baseline audit files (already loaded earlier)\n",
    "governance_path = CYCLE2_MODELS_DIR / \"frozen_config_cycle2.json\"\n",
    "baseline_audit_v3_path = CYCLE2_MODELS_DIR / \"baseline_huber15_audit_v3_from_pack.json\"\n",
    "\n",
    "metadata = {\n",
    "    \"cycle\": 3,\n",
    "    \"timestamp_utc\": pd.Timestamp.utcnow().isoformat(),\n",
    "    \"champion\": {\n",
    "        \"tag\": \"baseline_huber15\",              # canonical champion id (no-clip)\n",
    "        \"model_name\": \"HuberRegressor\",\n",
    "        \"protocol_name\": protocol_key,\n",
    "        \"params\": huber.get_params(),\n",
    "    \n",
    "        # clarify how winner is decided\n",
    "        \"selection\": {\n",
    "            \"decision_metric\": \"mae_test_2021_no_clip\",   # winner decided on no-clip\n",
    "            \"reported_modes\": [\"no_clip\", \"clip_0_100\"],\n",
    "            \"clip_is_eval_only\": True,\n",
    "        },\n",
    "    \n",
    "        # report both, with explicit naming\n",
    "        \"metrics\": {\n",
    "            \"val_2020_mae_no_clip\": float(mae_val),\n",
    "            \"test_2021_mae_no_clip\": float(mae_test),\n",
    "    \n",
    "            # if you already computed these, store them; otherwise compute here\n",
    "            \"val_2020_mae_clip_0_100\": float(mean_absolute_error(\n",
    "                y_val.to_numpy(float),\n",
    "                np.clip(y_pred_val, 0.0, 100.0),\n",
    "            )),\n",
    "            \"test_2021_mae_clip_0_100\": float(mean_absolute_error(\n",
    "                y_test.to_numpy(float),\n",
    "                np.clip(y_pred_test, 0.0, 100.0),\n",
    "            )),\n",
    "        },\n",
    "    \n",
    "        # keep raw prediction range (no-clip preds)\n",
    "        \"pred_range_test_no_clip\": {\n",
    "            \"min\": float(np.min(y_pred_test)),\n",
    "            \"max\": float(np.max(y_pred_test)),\n",
    "        },\n",
    "    },\n",
    "    \"protocol_guardrails\": {\n",
    "        \"split\": governance_cfg[\"decision_split\"],\n",
    "        \"imputation\": governance_cfg[\"baseline_protocols\"][protocol_key][\"preprocessing\"][\"imputation\"],\n",
    "        \"recency_weighting\": governance_cfg[\"baseline_protocols\"][protocol_key][\"recency_weighting\"],\n",
    "        \"numeric_cols_15\": numeric_cols,\n",
    "        \"nan_year_policy\": \"train\",\n",
    "    },\n",
    "    \"artifacts\": {\n",
    "        \"champion_joblib\": str(CHAMPION_PATH),\n",
    "        \"champion_joblib_sha256\": _sha256_file(CHAMPION_PATH),\n",
    "        \"governance_config_path\": str(governance_path),\n",
    "        \"governance_config_sha256\": _sha256_file(governance_path) if governance_path.exists() else None,\n",
    "        \"baseline_audit_v3_path\": str(baseline_audit_v3_path),\n",
    "        \"baseline_audit_v3_sha256\": _sha256_file(baseline_audit_v3_path) if baseline_audit_v3_path.exists() else None,\n",
    "    },\n",
    "    \"environment\": {\n",
    "        \"python\": sys.version,\n",
    "        \"platform\": platform.platform(),\n",
    "        \"numpy\": np.__version__,\n",
    "        \"pandas\": pd.__version__,\n",
    "        \"sklearn\": sklearn.__version__,\n",
    "        \"xgboost\": xgb.__version__,\n",
    "    },\n",
    "    \"summary\": {\n",
    "        \"baseline_beaten_by_any_candidate\": bool((df_results[\"mae_test_2021\"] < mae_test).any())\n",
    "        if \"df_results\" in globals() and \"mae_test_2021\" in df_results.columns\n",
    "        else None\n",
    "    },\n",
    "}\n",
    "\n",
    "metadata[\"summary\"] = summary\n",
    "\n",
    "RUN_METADATA_PATH.write_text(json.dumps(metadata, indent=2), encoding=\"utf-8\")\n",
    "print(\"Saved run metadata:\", RUN_METADATA_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e89d557-6e88-4a21-9c30-981c6868375d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T15:59:13.242742Z",
     "iopub.status.busy": "2026-02-16T15:59:13.241909Z",
     "iopub.status.idle": "2026-02-16T15:59:13.607016Z",
     "shell.execute_reply": "2026-02-16T15:59:13.605762Z",
     "shell.execute_reply.started": "2026-02-16T15:59:13.242669Z"
    }
   },
   "source": [
    "# 6. Cycle 3 — Final outcome \n",
    "\n",
    "Across point-runs, tuning (PredefinedSplit train/val), and expanded searches, XGBoost candidates achieved strong validation MAE but did not translate that advantage into **Test 2021** improvements.  \n",
    "Under the frozen protocol and the temporal decision split, the **Huber-15** baseline remained the most reliable generalizer and is therefore the Cycle 3 champion.\n",
    "\n",
    "Next steps:\n",
    "- Use `champion.joblib` as the reference model for subsequent cycles.\n",
    "- Treat clip(0–100) as a reporting sanity check only; keep all model selection decisions based on **no_clip** test MAE.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
